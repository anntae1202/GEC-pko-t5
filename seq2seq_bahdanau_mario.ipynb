{
 "cells": [
  {
   "cell_type": "code",
   "execution_count": 1,
   "id": "d2c89f7e-a7a1-4004-91a1-dd45c14b481f",
   "metadata": {},
   "outputs": [],
   "source": [
    "import argparse\n",
    "import os\n",
    "import re\n",
    "import random\n",
    "from glob import glob\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 2,
   "id": "39b2a30c-1c9d-4f9f-8792-5e236343ba2f",
   "metadata": {},
   "outputs": [],
   "source": [
    "import numpy as np\n",
    "import pandas as pd\n",
    "from tqdm import tqdm, trange"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 3,
   "id": "449c0443-8678-4454-8282-6c08a26a5abd",
   "metadata": {},
   "outputs": [],
   "source": [
    "import torch\n",
    "import torch.nn as nn\n",
    "import torch.nn.functional as F\n",
    "from torch import optim"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 4,
   "id": "8693d471-25ab-486b-bc26-c5fa1208b6f2",
   "metadata": {},
   "outputs": [],
   "source": [
    "import json\n",
    "from setproctitle import setproctitle\n",
    "from hanspell import spell_checker\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 5,
   "id": "9da61617-f1eb-4863-a727-5c965c3380a9",
   "metadata": {},
   "outputs": [],
   "source": [
    "parser = argparse.ArgumentParser()\n",
    "parser.add_argument('--proccess_name', default='seq2seq_bahdanau_mario')\n",
    "parser.add_argument('--data_path', default='s-kr/fine-tune/dataset/kspon_gec_dataset')\n",
    "parser.add_argument('--klec_path', default='./save')\n",
    "parser.add_argument('--devices', default='1')\n",
    "parser.add_argument('--max_len', default=1000)\n",
    "args = parser.parse_args([])"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 6,
   "id": "b36af593-8928-4b6b-9e75-067e96ed4e30",
   "metadata": {},
   "outputs": [],
   "source": [
    "# process 이름 설정\n",
    "setproctitle(args.proccess_name)\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 7,
   "id": "e23617c9-a73f-468d-ab7e-43d1197fc06e",
   "metadata": {},
   "outputs": [],
   "source": [
    "# cuda device 설정\n",
    "os.environ[\"CUDA_VISIBLE_DEVICES\"] = args.devices\n",
    "device = torch.device('cuda' if torch.cuda.is_available() else 'cpu')"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 8,
   "id": "287e84c2-254b-417e-bda2-d0dcba596be3",
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Kind of Device: cuda\n",
      "Count of available GPUs: 1\n",
      "Current cuda Device: 0\n"
     ]
    }
   ],
   "source": [
    "print('Kind of Device:', device)\n",
    "print('Count of available GPUs:', torch.cuda.device_count())\n",
    "print('Current cuda Device:', torch.cuda.current_device())"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 9,
   "id": "437f74c0-2d69-468e-8fcd-addaac0584b5",
   "metadata": {},
   "outputs": [],
   "source": [
    "import matplotlib.pyplot as plt\n",
    "# plt.switch_backend('agg')\n",
    "import matplotlib.ticker as ticker\n",
    "import numpy as np\n",
    "\n",
    "def showPlot(points):\n",
    "    plt.figure()\n",
    "    fig, ax = plt.subplots()\n",
    "    # 주기적인 간격에 이 locator가 tick을 설정\n",
    "    loc = ticker.MultipleLocator(base=0.2)\n",
    "    ax.yaxis.set_major_locator(loc)\n",
    "    plt.plot(points)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 10,
   "id": "0636c283-c83c-469e-a219-9c70687e26c1",
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "안\n",
      "태\n",
      "형\n",
      " \n",
      "최\n",
      "고\n",
      "최\n",
      "고\n"
     ]
    }
   ],
   "source": [
    "a=\"안태형 최고최고!!\"\n",
    "for i in list(a):\n",
    "    if i not in r'[^~?!\\s+]':\n",
    "        print(i)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "7d52d8ef-e9f9-466b-856e-9d01f6138752",
   "metadata": {},
   "outputs": [],
   "source": []
  },
  {
   "cell_type": "code",
   "execution_count": 11,
   "id": "0a59d95c-9df8-4095-83c2-feaa60b7c92c",
   "metadata": {},
   "outputs": [],
   "source": [
    "SOS_token = 0\n",
    "EOS_token = 1\n",
    "\n",
    "class Lang:\n",
    "    def __init__(self, name):\n",
    "        self.name = name\n",
    "        self.word2index = {}\n",
    "        self.word2count = {}\n",
    "        self.index2word = {0: 'SOS', 1: 'EOS'}\n",
    "        self.n_words = 2\n",
    "        \n",
    "    def addSentence(self, sentence):\n",
    "        # for word in sentence.split(' '):\n",
    "        #     self.addWord(word)\n",
    "        for word in list(sentence):\n",
    "            if word not in r'[^~?!\\s+]':\n",
    "                # print(word)\n",
    "                self.addWord(word)\n",
    "    \n",
    "    def addWord(self, word):\n",
    "        if word not in self.word2index:\n",
    "            self.word2index[word] = self.n_words\n",
    "            self.word2count[word] = 1\n",
    "            self.index2word[self.n_words] = word\n",
    "            self.n_words += 1\n",
    "        else:\n",
    "            self.word2count[word] += 1\n",
    "            \n",
    "            \n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 12,
   "id": "3550e343-5bf2-4fd5-bc43-3dee61879b07",
   "metadata": {},
   "outputs": [],
   "source": [
    "def normalizeString(s):\n",
    "    s = re.sub(r\"[^가-힣,.\\s]+\",'', s) # ,.?! 호함시킬걸\n",
    "    return s"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 13,
   "id": "374da174-6d7d-4bda-9089-bffbce8c440b",
   "metadata": {},
   "outputs": [],
   "source": [
    "def readLangs(lang1, lang2, reverse=False):\n",
    "    print('Reading lines...')\n",
    "    # tsv read\n",
    "    pairs = []\n",
    "    data_list=glob(os.path.join(args.data_path, '*.tsv'))\n",
    "    for data in tqdm(data_list):\n",
    "        df = pd.read_csv(data, sep='\\t', encoding='utf-8')\n",
    "        # 'form' 칼럼에 일부 데이터(3개) NaN(Not a Number: 표현 불가능한 수치형) 결측치 존재하여 삭제\n",
    "        df = df.dropna()\n",
    "        # df = df[:1000] # 1000 개만?\n",
    "        # 모든 줄을 분리하고 정규화\n",
    "\n",
    "        for _, row in df.iterrows():\n",
    "            # print(row)\n",
    "            form = row['form']\n",
    "            corrected_form = row['corrected_form']\n",
    "            pair = [normalizeString(form), normalizeString(corrected_form)]\n",
    "            pairs.append(pair) # pairs는 form과 cor을 한 리스트에 넣어둔 리스트임. 2차원\n",
    "\n",
    "          #  if len(pairs) >=500000: # 전체 학습 데이터수량\n",
    "              #  break\n",
    "    print('Dataset length :',len(pairs)) \n",
    "    input_lang = Lang(lang1)\n",
    "    output_lang = Lang(lang2)\n",
    "\n",
    "\n",
    "    return input_lang, output_lang, pairs"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 14,
   "id": "e9faab1d-96a3-4d76-81b7-28fe5489beb0",
   "metadata": {
    "scrolled": true,
    "tags": []
   },
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Reading lines...\n"
     ]
    },
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "100%|████████████████████████████████████████████████████████████████████████████████████████████████████████████████| 62/62 [01:34<00:00,  1.52s/it]\n"
     ]
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Dataset length : 612411\n",
      "Counted words:\n",
      "form 1862\n",
      "corrected_form 2269\n",
      "['음 그거래 그럼 심화해야지 뭐 일단 애들 돈만 좀 어울리 주고 어 그렇게 해서 아무튼 칠 월 지월 다에 가자고', '음 그래 그럼 시마이 해야지 뭐, 일단 애들 돈만 좀 올려주고 어 그렇게 해서 아무튼 월 전에는 내려가자고.']\n",
      "max_len : 322\n"
     ]
    }
   ],
   "source": [
    "def prepareData(lang1, lang2, reverse=False):\n",
    "    max_len = 0\n",
    "\n",
    "    input_lang, output_lang, pairs = readLangs(lang1, lang2, reverse)\n",
    "    for pair in pairs:\n",
    "        input_lang.addSentence(pair[0])\n",
    "        output_lang.addSentence(pair[1])\n",
    "        \n",
    "        if len(pair[0]) > max_len:\n",
    "            max_len = len(pair[0])\n",
    "        if len(pair[1]) > max_len:\n",
    "            max_len = len(pair[1])\n",
    "    print(\"Counted words:\")\n",
    "    print(input_lang.name, input_lang.n_words)\n",
    "    print(output_lang.name, output_lang.n_words)\n",
    "    return input_lang, output_lang, pairs, max_len\n",
    "\n",
    "\n",
    "\n",
    "input_lang, output_lang, pairs, max_len = prepareData('form', 'corrected_form', False)\n",
    "print(random.choice(pairs))\n",
    "print('max_len :', max_len)\n",
    "\n",
    "# pair"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 15,
   "id": "71fc4ec8-5276-459c-9ccd-ac9eb4f54668",
   "metadata": {
    "scrolled": true,
    "tags": []
   },
   "outputs": [],
   "source": [
    "import json\n",
    "import os\n",
    "\n",
    "form_vocab_path = os.path.join(args.klec_path, 'form_vocab.json')\n",
    "with open(form_vocab_path, 'w') as vocab_file1:\n",
    "    json.dump(input_lang.index2word, vocab_file1)\n",
    "\n",
    "cform_vocab_path = os.path.join(args.klec_path, 'cform_vocab.json')\n",
    "with open(cform_vocab_path, 'w') as vocab_file2:\n",
    "    json.dump(output_lang.index2word, vocab_file2)\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "5c776fd6-31ca-40ce-9cd9-7c682e101f20",
   "metadata": {},
   "outputs": [],
   "source": []
  },
  {
   "cell_type": "code",
   "execution_count": 16,
   "id": "ad83d18a-852c-469e-9d26-a38cbc56ce89",
   "metadata": {},
   "outputs": [],
   "source": [
    "def indexesFromSentence(lang, sentence):\n",
    "    # return [lang.word2index[word] for word in sentence.split(' ')]\n",
    "    return [lang.word2index[word] for word in list(sentence) if word not in r'[^~?!\\s+]']\n",
    "\n",
    "\n",
    "def tensorFromSentence(lang, sentence):\n",
    "    indexes = indexesFromSentence(lang, sentence)\n",
    "    indexes.append(EOS_token)\n",
    "    return torch.tensor(indexes, dtype=torch.long, device=device).view(-1, 1)\n",
    "\n",
    "def tensorFromPair(pair):\n",
    "    input_tensor = tensorFromSentence(input_lang, pair[0]) \n",
    "    target_tensor = tensorFromSentence(output_lang, pair[1])\n",
    "    return (input_tensor, target_tensor)\n",
    "\n",
    "\n",
    "# def targetToSentence(targets_list , id2char):\n",
    "    \n",
    "#     sentence=\"\"\n",
    "    \n",
    "#     for i in targets_list:\n",
    "#         sentence += id2char[int(n)]\n",
    "        \n",
    "#     return sentence\n",
    "        \n"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "2fc9ab84-2721-4cb2-b2f0-6c9ce64cd955",
   "metadata": {},
   "source": [
    "### https://wikidocs.net/159523 참고한 encoder, decoder"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 17,
   "id": "fb069120-f528-4ea1-b881-a97ad485285c",
   "metadata": {},
   "outputs": [],
   "source": [
    "class EncoderLSTM(nn.Module):\n",
    "    def __init__(self, input_size, hidden_size,n_layers=1):\n",
    "        super(EncoderLSTM, self).__init__()\n",
    "        self.hidden_size = hidden_size\n",
    "\n",
    "        self.embedding = nn.Embedding(input_size, hidden_size)\n",
    "        self.lstm = nn.LSTM(hidden_size, hidden_size, n_layers)\n",
    "\n",
    "    def forward(self, inputs, hidden):\n",
    "        # Embed input words\n",
    "        embedded = self.embedding(inputs).view([-1, 1, 256])\n",
    "        # Pass the embedded word vectors into LSTM and return all outputs\n",
    "        \n",
    "        output, hidden = self.lstm(embedded, hidden)\n",
    "\n",
    "        # print('output.size : ', output.size)\n",
    "        # print('hidden.size : ', hidden.size) \n",
    "        \n",
    "        return output, hidden\n",
    "\n",
    "    def initHidden(self):\n",
    "        return (torch.zeros(1, 1, self.hidden_size, device=device),\n",
    "                torch.zeros(1, 1, self.hidden_size, device=device))"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 18,
   "id": "644e390c-af10-4f54-9b7d-b9f9b72b69ab",
   "metadata": {},
   "outputs": [],
   "source": [
    "class BahdanauDecoder(nn.Module):\n",
    "    def __init__(self, hidden_size, output_size, n_layers=1, drop_prob=0.1):\n",
    "        super(BahdanauDecoder, self).__init__()\n",
    "        self.hidden_size = hidden_size\n",
    "        self.output_size = output_size\n",
    "        self.n_layers = n_layers\n",
    "        self.drop_prob = drop_prob\n",
    "\n",
    "        self.embedding = nn.Embedding(self.output_size, self.hidden_size)\n",
    "\n",
    "        self.fc_hidden = nn.Linear(self.hidden_size, self.hidden_size, bias=False)\n",
    "        self.fc_encoder = nn.Linear(self.hidden_size, self.hidden_size, bias=False)\n",
    "        self.weight = nn.Parameter(torch.FloatTensor(1, hidden_size))\n",
    "        self.attn_combine = nn.Linear(self.hidden_size * 2, self.hidden_size)\n",
    "        self.dropout = nn.Dropout(self.drop_prob)\n",
    "        self.lstm = nn.LSTM(self.hidden_size*2, self.hidden_size, batch_first=True)\n",
    "        self.classifier = nn.Linear(self.hidden_size, self.output_size)\n",
    "\n",
    "        \n",
    "    def forward(self, inputs, hidden, encoder_outputs):\n",
    "        encoder_outputs = encoder_outputs.squeeze()\n",
    "        # Embed input words\n",
    "        embedded = self.embedding(inputs).view(1, -1)\n",
    "        embedded = self.dropout(embedded)\n",
    "\n",
    "        # Calculating Alignment Scores\n",
    "        x = torch.tanh(self.fc_hidden(hidden[0])+self.fc_encoder(encoder_outputs))\n",
    "        alignment_scores = x.bmm(self.weight.unsqueeze(2))  \n",
    "\n",
    "        # Softmaxing alignment scores to get Attention weights\n",
    "        attn_weights = F.softmax(alignment_scores.view(1,-1), dim=1)\n",
    "\n",
    "        # Multiplying the Attention weights with encoder outputs to get the context vector\n",
    "        context_vector = torch.bmm(attn_weights.unsqueeze(0),\n",
    "                                 encoder_outputs.unsqueeze(0))\n",
    "\n",
    "        # Concatenating context vector with embedded input word\n",
    "        output = torch.cat((embedded, context_vector[0]), 1).unsqueeze(0)\n",
    "        # Passing the concatenated vector as input to the LSTM cell\n",
    "        output, hidden = self.lstm(output, hidden)\n",
    "        # Passing the LSTM output through a Linear layer acting as a classifier\n",
    "        output = F.log_softmax(self.classifier(output[0]), dim=1)\n",
    "        return output, hidden, attn_weights\n",
    "    \n",
    "    \n",
    "    def initHidden(self):\n",
    "        return (torch.zeros(1, 1, self.hidden_size, device=device),\n",
    "                torch.zeros(1, 1, self.hidden_size, device=device))"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 19,
   "id": "b38ba654-0bf6-4d3d-a207-088b7646ee1e",
   "metadata": {},
   "outputs": [],
   "source": [
    "teacher_forcing_ratio = 0.5\n",
    "\n",
    "def train(input_tensor, target_tensor,\n",
    "          encoder, decoder,\n",
    "          encoder_optimizer, decoder_optimizer,\n",
    "          criterion, max_length=max_len):\n",
    "    \n",
    "    encoder_hidden = encoder.initHidden()\n",
    "    # print('encoder_hidden.shape', encoder_hidden)\n",
    "    \n",
    "    encoder_optimizer.zero_grad()\n",
    "    decoder_optimizer.zero_grad()\n",
    "    \n",
    "    input_length = input_tensor.size(0)\n",
    "    target_length = target_tensor.size(0)\n",
    "    \n",
    "    encoder_outputs = torch.zeros(max_length, encoder.hidden_size, device=device)\n",
    "    # print('encoder_outputs.shape', encoder_outputs.shape)\n",
    "    \n",
    "    loss = 0\n",
    "    for fr in range(input_length):\n",
    "        # print('input_tensor[fr].shape', input_tensor[fr].shape)\n",
    "        # print('encoder(input_tensor[fr], encoder_hidden)', encoder(input_tensor[fr], encoder_hidden))\n",
    "        encoder_output, encoder_hidden = encoder(input_tensor[fr], encoder_hidden)\n",
    "        # print('encoder_output.shape', encoder_output.shape)\n",
    "        encoder_outputs[fr] = encoder_output[0, 0]\n",
    "    \n",
    "    decoder_input = torch.tensor([[SOS_token]], device=device)\n",
    "    \n",
    "    decoder_hidden = encoder_hidden\n",
    "    \n",
    "    use_teacher_forcing = True if random.random() < teacher_forcing_ratio else False\n",
    "    \n",
    "    if use_teacher_forcing:\n",
    "        # Teacher forcing 포함: 목표를 다음 입력으로 전달\n",
    "        for cfr in range(target_length):\n",
    "            decoder_output, decoder_hidden, decoder_attention = decoder(decoder_input, decoder_hidden, encoder_outputs)\n",
    "            \n",
    "            \n",
    "            loss += criterion(decoder_output, target_tensor[cfr]) # decoder_output = pred\n",
    "            decoder_input = target_tensor[cfr]\n",
    " \n",
    "    else:\n",
    "        # Teacer forcing 미포함: 자신의 예측을 다음 입력을 사용\n",
    "        for cfr in range(target_length):\n",
    "            decoder_output, decoder_hidden, decoder_attention = decoder(\n",
    "                decoder_input, decoder_hidden, encoder_outputs)\n",
    "            \n",
    "            topv, topi = decoder_output.topk(1)\n",
    "            decoder_input = topi.squeeze().detach() # 입력으로 사용할 부분을 히스토리에서 분리\n",
    "            \n",
    "            loss += criterion(decoder_output, target_tensor[cfr])\n",
    "            if decoder_input.item() == EOS_token:\n",
    "                break\n",
    "                \n",
    "    loss.backward()\n",
    "    \n",
    "    encoder_optimizer.step()\n",
    "    decoder_optimizer.step()\n",
    "    \n",
    "    return (decoder_output, loss.item() / target_length)"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "f952f304-79e6-40c4-bce9-b2dc160d5a96",
   "metadata": {},
   "source": [
    "### 현재 시간과 진행률%을 고려해 경과된 시간과 남은 예상 시간을 출력하는 헬퍼 함수"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 20,
   "id": "dcaa785a-8e67-4f1d-94e4-799b8e94ae4a",
   "metadata": {},
   "outputs": [],
   "source": [
    "import time\n",
    "import math\n",
    "\n",
    "def asMinutes(s):\n",
    "    m = math.floor(s / 60)\n",
    "    s -= m * 60\n",
    "    return '%dm %ds' % (m, s)\n",
    "\n",
    "def timeSince(since, percent):\n",
    "    now = time.time()\n",
    "    s = now - since\n",
    "    es = s / (percent)\n",
    "    rs = es - s\n",
    "    return '%s (- %s)' % (asMinutes(s), asMinutes(rs))"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "27fa668d-9573-45d1-be79-7975a957eea3",
   "metadata": {},
   "source": [
    "### 여러 번 train 을 호출하며 때로는 진행률 (예제의 %, 현재까지의 예상 시간)과 평균 손실을 출력"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 21,
   "id": "e5061704-6275-4b67-91ab-efb56c741c42",
   "metadata": {},
   "outputs": [],
   "source": [
    "from multiprocessing import Pool\n",
    "import time\n",
    "from tqdm.auto import tqdm\n",
    "from concurrent.futures import ThreadPoolExecutor\n",
    "\n",
    "\n",
    "\n",
    "def trainIters(encoder, decoder, n_iters, print_every=1000, plot_every=100, learning_rate=0.001):\n",
    "    start = time.time() # batch size * iteration = 전체 데이터수\n",
    "    plot_losses = []\n",
    "    y_pred=[]\n",
    "    print_loss_total = 0 # print_every 마다 초기화\n",
    "    plot_loss_total = 0 # plot_every 마다 초기화\n",
    "    \n",
    "    \n",
    "    target_list=[]\n",
    "    \n",
    "    encoder_optimizer = optim.Adam(encoder.parameters(), lr=learning_rate)\n",
    "    decoder_optimizer = optim.Adam(decoder.parameters(), lr=learning_rate)\n",
    "    training_pairs = [tensorFromPair(random.choice(pairs)) for i in range(n_iters)]\n",
    "    \n",
    "    # criterion = nn.CrossEntropyLoss()\n",
    "    criterion = nn.NLLLoss(size_average=False) \n",
    "    \n",
    "    # 여기에 에폭 기능 추가 필요\n",
    "    \n",
    "    for iter in trange(1, n_iters + 1):\n",
    "        training_pair = training_pairs[iter - 1]\n",
    "        input_tensor = training_pair[0] \n",
    "        target_tensor = training_pair[1]\n",
    "\n",
    "        pred, loss = train(input_tensor, target_tensor, encoder, decoder, encoder_optimizer, decoder_optimizer, criterion)\n",
    "        \n",
    "        # print('loss : ',loss)\n",
    "        \n",
    "        y_pred.append(pred)\n",
    "        print_loss_total += loss\n",
    "        plot_loss_total += loss\n",
    "        \n",
    "        if iter % print_every == 0:\n",
    "            print_loss_avg = print_loss_total / print_every\n",
    "            \n",
    "            plot_loss_total = 0\n",
    "            print('Average loss : %s (%d %d%%) %.4f' % (timeSince(start, iter / n_iters), iter, iter / n_iters * 100, print_loss_avg))\n",
    "        \n",
    "        if iter % plot_every == 0:\n",
    "            plot_loss_avg = plot_loss_total / plot_every\n",
    "            plot_losses.append(plot_loss_avg)\n",
    "            plot_loss_total = 0\n",
    "            \n",
    "    return (plot_losses, y_pred)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 22,
   "id": "21e02a45-e6e7-4680-9d35-1c739f4a0282",
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "EncoderLSTM(\n",
      "  (embedding): Embedding(1862, 256)\n",
      "  (lstm): LSTM(256, 256)\n",
      ")\n",
      "BahdanauDecoder(\n",
      "  (embedding): Embedding(2269, 256)\n",
      "  (fc_hidden): Linear(in_features=256, out_features=256, bias=False)\n",
      "  (fc_encoder): Linear(in_features=256, out_features=256, bias=False)\n",
      "  (attn_combine): Linear(in_features=512, out_features=256, bias=True)\n",
      "  (dropout): Dropout(p=0.1, inplace=False)\n",
      "  (lstm): LSTM(512, 256, batch_first=True)\n",
      "  (classifier): Linear(in_features=256, out_features=2269, bias=True)\n",
      ")\n"
     ]
    }
   ],
   "source": [
    "hidden_size = 256 \n",
    "encoder1 = EncoderLSTM(input_lang.n_words, hidden_size).to(device)\n",
    "attn_decoder1 = BahdanauDecoder(hidden_size, output_lang.n_words).to(device)\n",
    "\n",
    "# trainIters(encoder1, attn_decoder1, 75000, print_every=5000)\n",
    "\n",
    "\n",
    "print(encoder1)\n",
    "print(attn_decoder1)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 23,
   "id": "a04096e7-129a-4088-a5b3-e8b3f4008e72",
   "metadata": {
    "scrolled": true,
    "tags": []
   },
   "outputs": [
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "/opt/conda/lib/python3.8/site-packages/torch/nn/_reduction.py:44: UserWarning: size_average and reduce args will be deprecated, please use reduction='sum' instead.\n",
      "  warnings.warn(warning.format(ret))\n",
      "  7%|███████                                                                                                  | 5001/75000 [11:23<2:06:33,  9.22it/s]"
     ]
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Average loss : 11m 33s (- 161m 44s) (5000 6%) 3.7233\n"
     ]
    },
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      " 13%|█████████████▊                                                                                          | 10000/75000 [23:08<2:17:16,  7.89it/s]"
     ]
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Average loss : 23m 18s (- 151m 29s) (10000 13%) 7.0323\n"
     ]
    },
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      " 20%|████████████████████▊                                                                                   | 15001/75000 [34:56<2:43:10,  6.13it/s]"
     ]
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Average loss : 35m 6s (- 140m 26s) (15000 20%) 10.0381\n"
     ]
    },
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      " 27%|███████████████████████████▋                                                                            | 20000/75000 [46:49<1:38:58,  9.26it/s]"
     ]
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Average loss : 46m 59s (- 129m 14s) (20000 26%) 12.6721\n"
     ]
    },
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      " 33%|██████████████████████████████████▋                                                                     | 25001/75000 [58:57<2:22:05,  5.86it/s]"
     ]
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Average loss : 59m 7s (- 118m 15s) (25000 33%) 14.9868\n"
     ]
    },
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      " 40%|████████████████████████████████████████▊                                                             | 30000/75000 [1:11:04<2:12:55,  5.64it/s]"
     ]
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Average loss : 71m 14s (- 106m 51s) (30000 40%) 17.1328\n"
     ]
    },
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      " 47%|███████████████████████████████████████████████▌                                                      | 35001/75000 [1:22:55<1:43:06,  6.47it/s]"
     ]
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Average loss : 83m 5s (- 94m 57s) (35000 46%) 19.1337\n"
     ]
    },
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      " 53%|███████████████████████████████████████████████████████▍                                                | 40001/75000 [1:35:05<42:42, 13.66it/s]"
     ]
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Average loss : 95m 15s (- 83m 20s) (40000 53%) 21.0748\n"
     ]
    },
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      " 60%|█████████████████████████████████████████████████████████████▏                                        | 45001/75000 [1:47:15<1:33:36,  5.34it/s]"
     ]
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Average loss : 107m 25s (- 71m 37s) (45000 60%) 22.9413\n"
     ]
    },
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      " 67%|█████████████████████████████████████████████████████████████████████▎                                  | 50001/75000 [1:59:13<33:07, 12.58it/s]"
     ]
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Average loss : 119m 23s (- 59m 41s) (50000 66%) 24.7717\n"
     ]
    },
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      " 73%|████████████████████████████████████████████████████████████████████████████▎                           | 55000/75000 [2:11:05<41:10,  8.09it/s]"
     ]
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Average loss : 131m 15s (- 47m 43s) (55000 73%) 26.5634\n"
     ]
    },
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      " 80%|███████████████████████████████████████████████████████████████████████████████████▏                    | 60001/75000 [2:23:01<34:50,  7.18it/s]"
     ]
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Average loss : 143m 11s (- 35m 47s) (60000 80%) 28.3123\n"
     ]
    },
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      " 87%|██████████████████████████████████████████████████████████████████████████████████████████▏             | 65000/75000 [2:34:45<32:04,  5.20it/s]"
     ]
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Average loss : 154m 55s (- 23m 50s) (65000 86%) 30.0677\n"
     ]
    },
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      " 93%|█████████████████████████████████████████████████████████████████████████████████████████████████       | 70000/75000 [2:47:11<14:42,  5.66it/s]"
     ]
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Average loss : 167m 21s (- 11m 57s) (70000 93%) 31.8112\n"
     ]
    },
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "100%|████████████████████████████████████████████████████████████████████████████████████████████████████████| 75000/75000 [2:59:09<00:00,  6.98it/s]"
     ]
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Average loss : 179m 19s (- 0m 0s) (75000 100%) 33.4757\n"
     ]
    },
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "\n"
     ]
    }
   ],
   "source": [
    "plot_losses, y_pred = trainIters(encoder1, attn_decoder1, 75000, print_every=5000)\n",
    "# # loss에서 nan 뜨면 kernel restart\n",
    "\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 24,
   "id": "328394a3-8a4e-4c0a-9bf2-d79a0ebc6c2f",
   "metadata": {
    "scrolled": true,
    "tags": []
   },
   "outputs": [
    {
     "data": {
      "text/plain": [
       "[5.115086273414005,\n",
       " 4.476462042599552,\n",
       " 4.2527270813619005,\n",
       " 4.2126880365815955,\n",
       " 4.1915090741122825,\n",
       " 3.9406931896781066,\n",
       " 3.965471253684188,\n",
       " 3.9850195701437614,\n",
       " 3.840318416613247,\n",
       " 3.839075392331164,\n",
       " 3.92948565566157,\n",
       " 3.9722173804566374,\n",
       " 3.8283695501571433,\n",
       " 3.831113484466767,\n",
       " 3.8079111226524156,\n",
       " 3.7130149005770807,\n",
       " 3.6748745157821032,\n",
       " 3.7208597004224733,\n",
       " 3.747474725326317,\n",
       " 3.7776195026191584,\n",
       " 3.7261658579425485,\n",
       " 3.6929747556095434,\n",
       " 3.4864625405960656,\n",
       " 3.6203019345076175,\n",
       " 3.7974081723674917,\n",
       " 3.645011926728793,\n",
       " 3.5181272516509186,\n",
       " 3.5677319075501934,\n",
       " 3.376069788046914,\n",
       " 3.5405125654850393,\n",
       " 3.726844262608811,\n",
       " 3.6592520087077367,\n",
       " 3.5440972696357598,\n",
       " 3.537883748358725,\n",
       " 3.4293206876092217,\n",
       " 3.702299413338111,\n",
       " 3.479685008105445,\n",
       " 3.4718878134741034,\n",
       " 3.35792137681505,\n",
       " 3.535056184205913,\n",
       " 3.416538091651948,\n",
       " 3.5657452282519246,\n",
       " 3.5215475409696317,\n",
       " 3.597511433395317,\n",
       " 3.644921205125438,\n",
       " 3.566617498615278,\n",
       " 3.432433652629123,\n",
       " 3.337148406923707,\n",
       " 3.43225013792286,\n",
       " 0.0,\n",
       " 3.5374158369804145,\n",
       " 3.4017118718787476,\n",
       " 3.367271513098125,\n",
       " 3.417721594897159,\n",
       " 3.4297115573729235,\n",
       " 3.3859144231680833,\n",
       " 3.383681268197321,\n",
       " 3.5158218726082304,\n",
       " 3.4505639182225,\n",
       " 3.3945044302636926,\n",
       " 3.30335705109531,\n",
       " 3.5029565390213464,\n",
       " 3.4960076021536843,\n",
       " 3.3607856203500988,\n",
       " 3.375949732500088,\n",
       " 3.256090812388658,\n",
       " 3.3354215180250866,\n",
       " 3.427081880841919,\n",
       " 3.3697717958126336,\n",
       " 3.3032231862798414,\n",
       " 3.4582326431796546,\n",
       " 3.4196125824697527,\n",
       " 3.2268071621348633,\n",
       " 3.2532479774104184,\n",
       " 3.4211298966579893,\n",
       " 3.3024402088348377,\n",
       " 3.2283022611788383,\n",
       " 3.341728536866828,\n",
       " 3.2849995309834754,\n",
       " 3.314266642331239,\n",
       " 3.202702934206126,\n",
       " 3.357023872512642,\n",
       " 3.197543654696751,\n",
       " 3.1365109597617997,\n",
       " 3.334486857850783,\n",
       " 3.008159352045868,\n",
       " 3.2438652017183336,\n",
       " 3.26985215792968,\n",
       " 3.2675609101218455,\n",
       " 3.1455398164977897,\n",
       " 3.2569589630695885,\n",
       " 3.2120065976860297,\n",
       " 3.1665372745823586,\n",
       " 3.2193585882411457,\n",
       " 3.266372666648503,\n",
       " 3.156831868297653,\n",
       " 3.2022916550451366,\n",
       " 3.194066416414125,\n",
       " 3.211923063022892,\n",
       " 0.0,\n",
       " 3.0960876408584426,\n",
       " 3.0710648552042317,\n",
       " 3.2563650408170135,\n",
       " 2.9664040965777883,\n",
       " 3.224244038199714,\n",
       " 3.0899919953980906,\n",
       " 2.8745642940373854,\n",
       " 3.032966147668477,\n",
       " 2.957669362538283,\n",
       " 3.1018088496343603,\n",
       " 3.210836839220381,\n",
       " 3.011578279979077,\n",
       " 3.115985748598147,\n",
       " 3.066573444552523,\n",
       " 3.1591595866075357,\n",
       " 3.0762341960336586,\n",
       " 3.032585931678812,\n",
       " 3.04841258757831,\n",
       " 2.9434465581048714,\n",
       " 3.1899015047224064,\n",
       " 2.932342472154864,\n",
       " 3.153729636182506,\n",
       " 3.162069690825063,\n",
       " 3.0497493758692764,\n",
       " 3.232825667850679,\n",
       " 3.0861722723628158,\n",
       " 2.8068147939196693,\n",
       " 3.0051989769988063,\n",
       " 2.9719622842934763,\n",
       " 3.0315169276644323,\n",
       " 2.8117799111052433,\n",
       " 2.7572171506283927,\n",
       " 3.089855437936773,\n",
       " 2.9925262322908246,\n",
       " 2.795770746801548,\n",
       " 2.8168497420815015,\n",
       " 3.0239931119170036,\n",
       " 3.0406121895098583,\n",
       " 2.806412755603745,\n",
       " 2.8937482361212608,\n",
       " 2.870009477240311,\n",
       " 3.0719269334772252,\n",
       " 2.911212938933528,\n",
       " 3.0385534770257525,\n",
       " 2.9530359966255775,\n",
       " 2.7789224623661384,\n",
       " 2.86624769006404,\n",
       " 2.8966520887067766,\n",
       " 3.284264390315036,\n",
       " 0.0,\n",
       " 2.73781903281996,\n",
       " 2.822272299591231,\n",
       " 2.7559341664671577,\n",
       " 2.8002246530088604,\n",
       " 2.669188082987615,\n",
       " 2.71613856448957,\n",
       " 2.847933047571351,\n",
       " 2.6903787623899262,\n",
       " 2.6351271158415375,\n",
       " 2.7811975658616537,\n",
       " 2.7693993817288085,\n",
       " 2.725790977870306,\n",
       " 2.5500329649069373,\n",
       " 2.637403386176353,\n",
       " 2.62748148246553,\n",
       " 2.8007722358037483,\n",
       " 2.8471327487686473,\n",
       " 2.5872436442912288,\n",
       " 2.572024388093539,\n",
       " 2.714783050509535,\n",
       " 2.69227590852088,\n",
       " 2.502573505268534,\n",
       " 2.6069558825147165,\n",
       " 2.5586259934588536,\n",
       " 2.627834086554125,\n",
       " 2.6803857728799896,\n",
       " 2.5748711846805117,\n",
       " 2.5397125563280927,\n",
       " 2.661404693213819,\n",
       " 2.557535144232917,\n",
       " 2.5055525908578704,\n",
       " 2.68790533309782,\n",
       " 2.61537577245472,\n",
       " 2.610800418673466,\n",
       " 2.437353491741796,\n",
       " 2.492859389131466,\n",
       " 2.7679355414357123,\n",
       " 2.613343067575091,\n",
       " 2.476343054236837,\n",
       " 2.7271481754864535,\n",
       " 2.4414401822553717,\n",
       " 2.5269043557865047,\n",
       " 2.454768834194206,\n",
       " 2.5802774765576326,\n",
       " 2.486206327455172,\n",
       " 2.726823170393721,\n",
       " 2.6639518891711247,\n",
       " 2.5277905783697974,\n",
       " 2.5140438227348856,\n",
       " 0.0,\n",
       " 2.5065769232393356,\n",
       " 2.4260769662410153,\n",
       " 2.3526120211725656,\n",
       " 2.4706311807466252,\n",
       " 2.372888502583204,\n",
       " 2.3149817630480443,\n",
       " 2.4409656851695907,\n",
       " 2.361886095067768,\n",
       " 2.164335915896208,\n",
       " 2.264610841506946,\n",
       " 2.353624725842728,\n",
       " 2.2781897222178404,\n",
       " 2.449782325538177,\n",
       " 2.299606093983122,\n",
       " 2.275100443621663,\n",
       " 2.426503655801075,\n",
       " 2.3140777478628767,\n",
       " 2.5298140908589057,\n",
       " 2.274895198058067,\n",
       " 2.3319825184421528,\n",
       " 2.329906827199602,\n",
       " 2.537166680181339,\n",
       " 2.0842502006627854,\n",
       " 2.3254488757281435,\n",
       " 2.132743532645685,\n",
       " 2.2542956428351193,\n",
       " 2.325410317628479,\n",
       " 2.5208408876326245,\n",
       " 2.1760125026241073,\n",
       " 2.3312652937663394,\n",
       " 2.392372835719493,\n",
       " 2.2552085079175663,\n",
       " 2.1561561987422873,\n",
       " 2.401343287913395,\n",
       " 2.3183574375151506,\n",
       " 2.219191085820097,\n",
       " 2.0918821936690177,\n",
       " 2.425811229207936,\n",
       " 2.041533930120203,\n",
       " 2.2963038318110636,\n",
       " 2.4831780723665533,\n",
       " 2.1904959289370742,\n",
       " 2.2819207921706877,\n",
       " 2.2329036764059147,\n",
       " 2.251860340804282,\n",
       " 2.1145867626179973,\n",
       " 2.1163351858708865,\n",
       " 2.500033359587862,\n",
       " 2.2862023690661832,\n",
       " 0.0,\n",
       " 2.2982832897061654,\n",
       " 1.9700581892660594,\n",
       " 2.085157996936595,\n",
       " 2.228258670598561,\n",
       " 2.0844282142627617,\n",
       " 2.1734785529278926,\n",
       " 2.2388396169626406,\n",
       " 2.158816670184172,\n",
       " 2.0373799793857357,\n",
       " 2.133990221857107,\n",
       " 2.280038777901509,\n",
       " 2.2368653090837216,\n",
       " 2.123811672094274,\n",
       " 2.290405978973675,\n",
       " 2.057972025948811,\n",
       " 2.4306681119959355,\n",
       " 2.3123477278819182,\n",
       " 2.270075549236999,\n",
       " 2.154258357573372,\n",
       " 2.226993975434211,\n",
       " 2.326116753806348,\n",
       " 2.075866686229999,\n",
       " 2.2695847946062746,\n",
       " 2.010306153752292,\n",
       " 1.9605130138705875,\n",
       " 2.2145940751666187,\n",
       " 2.4724178161901977,\n",
       " 2.230674142449013,\n",
       " 1.9059099906052555,\n",
       " 2.267287738592548,\n",
       " 1.9787113783471701,\n",
       " 1.9314056144540148,\n",
       " 2.0705681299272833,\n",
       " 2.189417446411452,\n",
       " 2.1390281026062916,\n",
       " 2.3093452594001476,\n",
       " 1.9279171527656482,\n",
       " 2.1135459118581936,\n",
       " 2.133802553321662,\n",
       " 2.2062664156530603,\n",
       " 2.0429671168455723,\n",
       " 2.0530252788683825,\n",
       " 2.199601343705761,\n",
       " 2.092763471672945,\n",
       " 2.1004239364196597,\n",
       " 1.9887908377676653,\n",
       " 2.0469090148120728,\n",
       " 2.1596645669225754,\n",
       " 1.9387699653022827,\n",
       " 0.0,\n",
       " 2.0046895173718893,\n",
       " 1.8891919631366159,\n",
       " 1.9353734610155322,\n",
       " 2.072848792614059,\n",
       " 2.0344514285835515,\n",
       " 2.015103422079004,\n",
       " 2.043045001146714,\n",
       " 2.3112734719147756,\n",
       " 1.9115826275734324,\n",
       " 2.170544264483703,\n",
       " 2.061058001816867,\n",
       " 2.1617766491603985,\n",
       " 2.0152247613206677,\n",
       " 2.044462355536082,\n",
       " 2.1371987197599407,\n",
       " 1.957092048272881,\n",
       " 1.924356582225314,\n",
       " 2.031009682690354,\n",
       " 1.8747494321236584,\n",
       " 1.9763381832657745,\n",
       " 1.8899978267235895,\n",
       " 2.1683235284431492,\n",
       " 1.880813851068259,\n",
       " 1.9634822703256223,\n",
       " 2.1865176797070665,\n",
       " 2.0373635383433033,\n",
       " 2.0134274010676267,\n",
       " 1.9894679622970168,\n",
       " 1.9607971959302923,\n",
       " 2.0459983820338756,\n",
       " 2.0023053173733265,\n",
       " 1.7826437482630617,\n",
       " 2.0800917880524343,\n",
       " 1.9659615614514572,\n",
       " 1.9873976740339907,\n",
       " 1.844966284984147,\n",
       " 2.022964009551155,\n",
       " 1.8636226229061077,\n",
       " 2.212394682158698,\n",
       " 1.9082892561141518,\n",
       " 1.8668275961094938,\n",
       " 2.105972621732811,\n",
       " 1.921484295356314,\n",
       " 1.9430640868048805,\n",
       " 1.7529407545028066,\n",
       " 1.9992934732443475,\n",
       " 2.1032268118876503,\n",
       " 2.2021682339607254,\n",
       " 1.8612208083879462,\n",
       " 0.0,\n",
       " 2.1085334614118127,\n",
       " 1.9408253108484166,\n",
       " 2.0357730171490056,\n",
       " 1.7250263653542919,\n",
       " 1.7187443793230726,\n",
       " 2.187729993781645,\n",
       " 2.0068894849722474,\n",
       " 1.928017027106043,\n",
       " 1.9736421714269048,\n",
       " 1.8796865191438497,\n",
       " 1.9975877554568586,\n",
       " 1.8536715695349404,\n",
       " 2.075227559339892,\n",
       " 2.071777507976809,\n",
       " 2.1311357246556204,\n",
       " 1.8433095615059027,\n",
       " 1.916292747042751,\n",
       " 1.8803838043703076,\n",
       " 2.1135005848824817,\n",
       " 1.822334900276447,\n",
       " 2.2001322008080773,\n",
       " 2.005483293757462,\n",
       " 1.8579835569208507,\n",
       " 2.036572638530753,\n",
       " 2.0830442004494505,\n",
       " 1.91111686882299,\n",
       " 1.980876770655795,\n",
       " 2.1845858776065517,\n",
       " 1.9177176137705771,\n",
       " 1.8743801468641814,\n",
       " 1.9561663565819274,\n",
       " 2.065199760887663,\n",
       " 1.960184804785911,\n",
       " 1.993472078623767,\n",
       " 1.7752047946950589,\n",
       " 1.8021020368499254,\n",
       " 1.8997019605371017,\n",
       " 1.8175198414577787,\n",
       " 2.1177574284131797,\n",
       " 1.9055199261194213,\n",
       " 1.6993155056825409,\n",
       " 1.9757741831774327,\n",
       " 1.7412240925883347,\n",
       " 2.0145232298549263,\n",
       " 1.825234438668449,\n",
       " 1.8479335074199381,\n",
       " 1.757232896136343,\n",
       " 1.980239238274799,\n",
       " 2.002542356598377,\n",
       " 0.0,\n",
       " 1.885111467895125,\n",
       " 1.8023286327588126,\n",
       " 1.6853412437115376,\n",
       " 1.9211171519355563,\n",
       " 1.9813068952604476,\n",
       " 1.6946381985195387,\n",
       " 2.0260642359580534,\n",
       " 1.8136210069012262,\n",
       " 1.7148675030900953,\n",
       " 1.8404388670246137,\n",
       " 1.924655124955247,\n",
       " 1.8649720494940076,\n",
       " 1.9774149353515142,\n",
       " 1.857282499079011,\n",
       " 1.741477451224843,\n",
       " 1.5332027121470748,\n",
       " 1.9305460553866294,\n",
       " 1.9068518223857667,\n",
       " 1.8624824554585002,\n",
       " 1.9376471939104056,\n",
       " 1.6321525536962087,\n",
       " 2.074134453622325,\n",
       " 1.9812550675212097,\n",
       " 2.090324615416864,\n",
       " 1.6182322756353826,\n",
       " 1.8683609783922632,\n",
       " 2.064113388406759,\n",
       " 1.8925794470994832,\n",
       " 2.0324389004310044,\n",
       " 1.7059347026884362,\n",
       " 1.8820572311621937,\n",
       " 1.9716116563635604,\n",
       " 1.9824597073295258,\n",
       " 1.9435143537972874,\n",
       " 1.8630858305481843,\n",
       " 1.8778446741993289,\n",
       " 1.955665034450863,\n",
       " 1.6908098857676221,\n",
       " 1.7136870999140592,\n",
       " 2.005783699590116,\n",
       " 1.9090455102932171,\n",
       " 1.9280103058540936,\n",
       " 1.8547626484764839,\n",
       " 2.104117309891278,\n",
       " 1.558104728353397,\n",
       " 1.74829041423426,\n",
       " 1.8333538050903602,\n",
       " 1.7969824112987083,\n",
       " 1.9170364056677585,\n",
       " 0.0,\n",
       " 2.0552081955251795,\n",
       " 2.0475624187270016,\n",
       " 1.926883385186456,\n",
       " 1.7870284379801609,\n",
       " 1.8266622682424447,\n",
       " 2.039086166128071,\n",
       " 1.7863765810299455,\n",
       " 1.6706670558640653,\n",
       " 2.0702885909545503,\n",
       " 2.1380420288038544,\n",
       " 1.5658175726452541,\n",
       " 1.5710427206538287,\n",
       " 1.8163441416704316,\n",
       " 1.6930584097978387,\n",
       " 1.795778563092065,\n",
       " 1.6159095511398354,\n",
       " 1.6835478810149251,\n",
       " 1.7397554070422345,\n",
       " 1.9467548889586264,\n",
       " 2.009497196787629,\n",
       " 1.737087122526675,\n",
       " 1.7628909594824085,\n",
       " 1.9317503839709147,\n",
       " 1.8025604392766896,\n",
       " 1.7726518036034546,\n",
       " 2.0073130863217425,\n",
       " 1.5669821589770543,\n",
       " 1.7685752639164567,\n",
       " 1.5899883066601512,\n",
       " 1.8316473502263917,\n",
       " 1.8294452716538598,\n",
       " 1.9262781346440414,\n",
       " 1.7175206579042632,\n",
       " 1.952582374722411,\n",
       " 1.7515524987992084,\n",
       " 1.8941928785112074,\n",
       " 2.0877363887770963,\n",
       " 1.6439558241447911,\n",
       " 1.7270024142138394,\n",
       " 1.9027086713683412,\n",
       " 1.875295049062702,\n",
       " 1.573706895020039,\n",
       " 1.7807211906613056,\n",
       " 1.9445344885186646,\n",
       " 1.8267414853895036,\n",
       " 1.6476291896555686,\n",
       " 1.9278164215302116,\n",
       " 2.0638891881767503,\n",
       " 2.063214210128873,\n",
       " 0.0,\n",
       " 1.5947671128571221,\n",
       " 1.6872023457457614,\n",
       " 1.731602390383311,\n",
       " 1.884932586607948,\n",
       " 1.7895232553559506,\n",
       " 1.7126233291010236,\n",
       " 1.6970074232838668,\n",
       " 1.9772239535426737,\n",
       " 1.6438886443512646,\n",
       " 1.8563079803215288,\n",
       " 1.5376711071004343,\n",
       " 1.6889214774087133,\n",
       " 1.8705440833291664,\n",
       " 1.9063410141175516,\n",
       " 1.8917903261243398,\n",
       " 1.6229720686762703,\n",
       " 1.8260796850035865,\n",
       " 1.75101414009948,\n",
       " 1.6887664621290543,\n",
       " 1.6917519741251734,\n",
       " 1.7367341479957372,\n",
       " 1.8798461118100362,\n",
       " 1.8415716609980017,\n",
       " 1.805177909969558,\n",
       " 1.6980634072591834,\n",
       " 1.7739284051213846,\n",
       " 2.0011459432032344,\n",
       " 1.9199296945112667,\n",
       " 1.6893465570818806,\n",
       " 1.8390446730032568,\n",
       " 1.8302336631434657,\n",
       " 1.899239839780908,\n",
       " 1.8437432973454992,\n",
       " 1.7922989224881887,\n",
       " 1.6989708784949036,\n",
       " 1.6869383120328907,\n",
       " 1.780826230550879,\n",
       " 1.731440817505653,\n",
       " 1.8191978424114899,\n",
       " 1.85042043927442,\n",
       " 2.0819302346620905,\n",
       " 1.878125594251883,\n",
       " 1.670527270095529,\n",
       " 1.6493001335486661,\n",
       " 1.9780340574066426,\n",
       " 2.033294823976475,\n",
       " 1.8393580903424758,\n",
       " 1.7898659485745603,\n",
       " 1.772994709296732,\n",
       " 0.0,\n",
       " 1.7634776061763617,\n",
       " 1.7735064295050698,\n",
       " 1.645305989732405,\n",
       " 1.812965191500075,\n",
       " 1.443437095228085,\n",
       " 1.7696423770892977,\n",
       " 1.6227972558015156,\n",
       " 1.6300655159282698,\n",
       " 1.6688492400196742,\n",
       " 1.5831933252926456,\n",
       " 2.072388966144439,\n",
       " 1.8809802550395722,\n",
       " 2.1104310494377456,\n",
       " 1.8481391410501384,\n",
       " 1.635233466838065,\n",
       " 1.4213000670641918,\n",
       " 1.9182140898526878,\n",
       " 1.889748733739699,\n",
       " 1.7108278078213457,\n",
       " 1.8979602584450956,\n",
       " 1.4842718030998696,\n",
       " 1.4924274679217462,\n",
       " 1.754719816578996,\n",
       " 1.7464754466018593,\n",
       " 1.9100483897013714,\n",
       " 1.876065960009707,\n",
       " 1.6346134372942396,\n",
       " 1.7182201597072002,\n",
       " 1.6353953979888245,\n",
       " 1.828166928537285,\n",
       " 1.7566241267481906,\n",
       " 1.7760971750708077,\n",
       " 1.5077155211761224,\n",
       " 1.547558025946255,\n",
       " 1.7821579905839513,\n",
       " 1.703865762395772,\n",
       " 1.8226368825986499,\n",
       " 1.8555813805414243,\n",
       " 1.8259319296375842,\n",
       " 1.926027796271308,\n",
       " 1.823281671599577,\n",
       " 1.8080361160253688,\n",
       " 1.8155737612909615,\n",
       " 1.7612454132138182,\n",
       " 1.8591499682565282,\n",
       " 1.776033889271956,\n",
       " 1.7860529855070228,\n",
       " 1.5574211286944828,\n",
       " 1.6615461157786569,\n",
       " 0.0,\n",
       " 1.7029200581402526,\n",
       " 1.6683047957324002,\n",
       " 1.8136894871323692,\n",
       " 1.8530002847833094,\n",
       " 1.691696006212835,\n",
       " 1.7230644419650887,\n",
       " 2.030389903918064,\n",
       " 1.5486014873335545,\n",
       " 1.822497565685251,\n",
       " 1.921451050118167,\n",
       " 1.900731637028989,\n",
       " 1.7835545024959762,\n",
       " 1.8600492369885646,\n",
       " 1.947578586339918,\n",
       " 1.8913619508583333,\n",
       " 1.8667674964225953,\n",
       " 2.0276553642921007,\n",
       " 1.6542077027251552,\n",
       " 1.7355186667107358,\n",
       " 1.882833566395587,\n",
       " 1.6009057788243006,\n",
       " 1.8810383374137962,\n",
       " 1.570661882746049,\n",
       " 2.042559822601139,\n",
       " 1.6908948255981386,\n",
       " 1.6647500680488216,\n",
       " 1.7097957379438258,\n",
       " 1.863677078294592,\n",
       " 1.5862879930037637,\n",
       " 1.683879297127692,\n",
       " 1.641377535493164,\n",
       " 1.6812509476000685,\n",
       " 1.9515812431217123,\n",
       " 1.440671724465748,\n",
       " 1.627980701280732,\n",
       " 1.7126661475491445,\n",
       " 1.6592372980321912,\n",
       " 1.6704147617282845,\n",
       " 1.900076073435722,\n",
       " 1.7425442370132924,\n",
       " 1.7780590908593303,\n",
       " 1.6829233146426947,\n",
       " 1.7002734428689021,\n",
       " 1.7462581105709865,\n",
       " 1.8952133931384163,\n",
       " 1.6886566970274441,\n",
       " 1.6473797723933041,\n",
       " 1.6492698714107519,\n",
       " 1.7637198821115474,\n",
       " 0.0,\n",
       " 1.4459570245028377,\n",
       " 1.8625404124768625,\n",
       " 1.7995174393990578,\n",
       " 1.9729004778255725,\n",
       " 1.4683409510927152,\n",
       " 1.522553128873267,\n",
       " 1.8442880130775974,\n",
       " 1.6642831823984237,\n",
       " 1.7533471481573892,\n",
       " 1.856600003321172,\n",
       " 1.6131796676994734,\n",
       " 1.8843205918860735,\n",
       " 1.8406964835389903,\n",
       " 1.6935574260454862,\n",
       " 1.743107121772479,\n",
       " 2.0502610435056803,\n",
       " 1.8571380624706089,\n",
       " 1.8155736333305412,\n",
       " 1.6340432800436338,\n",
       " 1.7748551368403829,\n",
       " 1.6485073708041418,\n",
       " 1.8121725345377389,\n",
       " 1.7031683076596706,\n",
       " 1.8433322190770756,\n",
       " 1.6736841091905046,\n",
       " 1.4578016931368398,\n",
       " 1.8543946135207232,\n",
       " 1.7111885007012742,\n",
       " 1.7308227726751084,\n",
       " 1.8234120276034267,\n",
       " 1.6770934115541187,\n",
       " 1.9276925884351916,\n",
       " 1.7216020448264933,\n",
       " 1.69365438152437,\n",
       " 1.639747562973738,\n",
       " 1.5593035426971589,\n",
       " 1.8166609596523897,\n",
       " 1.7221519635045401,\n",
       " 1.8474025704432435,\n",
       " 1.7649475522930516,\n",
       " 1.904945047837345,\n",
       " 1.7364716893327183,\n",
       " 1.5885320615823293,\n",
       " 1.8037872689574241,\n",
       " 1.7202316640254807,\n",
       " 1.7946099505695248,\n",
       " 1.752399021064667,\n",
       " 1.8172983834951517,\n",
       " 1.616564463768876,\n",
       " 0.0,\n",
       " 1.6295002470007884,\n",
       " 1.6467191149753235,\n",
       " 1.6599038192160536,\n",
       " 1.7623028361068853,\n",
       " 1.8267771048749122,\n",
       " 1.8194585052907681,\n",
       " 1.5687616612686366,\n",
       " 1.6361627542863402,\n",
       " 1.6896067142574236,\n",
       " 1.6395567626549892,\n",
       " 1.6533171613313908,\n",
       " 1.7252125163521494,\n",
       " 1.698812059377845,\n",
       " 1.632409610349076,\n",
       " 1.4842187182465942,\n",
       " 1.584132972938889,\n",
       " 1.4998476654685047,\n",
       " 1.6739603562834098,\n",
       " 1.6877993637227546,\n",
       " 1.387903427692421,\n",
       " 1.6705114862843933,\n",
       " 1.6171381296142489,\n",
       " 1.7437978005847716,\n",
       " 1.4835450511086525,\n",
       " 1.5094512254290207,\n",
       " 1.8691027710270862,\n",
       " 1.7107342914554278,\n",
       " 1.501405404438531,\n",
       " 1.7555397332361933,\n",
       " 1.6560970490573714,\n",
       " 1.403131609339289,\n",
       " 1.6429605731550554,\n",
       " 1.6391482476956396,\n",
       " 1.8707028906128955,\n",
       " 1.5386736513290429,\n",
       " 1.877808651630941,\n",
       " 1.7317486553630994,\n",
       " 1.7705049336783296,\n",
       " 1.8068571934591997,\n",
       " 1.789490784180379,\n",
       " 1.437056563724921,\n",
       " 1.747158702955544,\n",
       " 1.738829372436044,\n",
       " 1.7329272998996528,\n",
       " 1.7052378915088895,\n",
       " 1.7582982320735223,\n",
       " 1.590357352546795,\n",
       " 1.8011701472525234,\n",
       " 1.6237558817186741,\n",
       " 0.0]"
      ]
     },
     "execution_count": 24,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "plot_losses"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 25,
   "id": "3519f71b-abc7-4123-8eba-22781b115f80",
   "metadata": {},
   "outputs": [
    {
     "data": {
      "text/plain": [
       "<Figure size 432x288 with 0 Axes>"
      ]
     },
     "metadata": {},
     "output_type": "display_data"
    },
    {
     "data": {
      "image/png": "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\n",
      "text/plain": [
       "<Figure size 432x288 with 1 Axes>"
      ]
     },
     "metadata": {
      "needs_background": "light"
     },
     "output_type": "display_data"
    }
   ],
   "source": [
    "showPlot(plot_losses)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 26,
   "id": "ce9cd371-571a-4f8f-9a28-2756eaca9547",
   "metadata": {
    "scrolled": true,
    "tags": []
   },
   "outputs": [
    {
     "data": {
      "text/plain": [
       "[tensor([[-7.7984, -7.6460, -7.7156,  ..., -7.6000, -7.8471, -7.7791]],\n",
       "        device='cuda:0', grad_fn=<LogSoftmaxBackward>),\n",
       " tensor([[-7.8067, -7.6391, -7.6607,  ..., -7.7664, -7.7153, -7.8503]],\n",
       "        device='cuda:0', grad_fn=<LogSoftmaxBackward>),\n",
       " tensor([[-7.8450, -7.5354, -7.7701,  ..., -7.7028, -7.7339, -7.6587]],\n",
       "        device='cuda:0', grad_fn=<LogSoftmaxBackward>),\n",
       " tensor([[-7.6479, -7.5344, -7.7424,  ..., -7.7626, -7.7189, -7.7393]],\n",
       "        device='cuda:0', grad_fn=<LogSoftmaxBackward>),\n",
       " tensor([[-7.8073, -7.3289, -7.7700,  ..., -7.5796, -7.8297, -7.7760]],\n",
       "        device='cuda:0', grad_fn=<LogSoftmaxBackward>),\n",
       " tensor([[-7.8633, -7.1437, -7.3841,  ..., -7.4963, -7.5744, -7.6237]],\n",
       "        device='cuda:0', grad_fn=<LogSoftmaxBackward>),\n",
       " tensor([[-7.8941, -6.9599, -7.2116,  ..., -7.5162, -7.5526, -7.6667]],\n",
       "        device='cuda:0', grad_fn=<LogSoftmaxBackward>),\n",
       " tensor([[-7.9771, -6.8427, -7.0592,  ..., -7.4464, -7.6306, -7.6273]],\n",
       "        device='cuda:0', grad_fn=<LogSoftmaxBackward>),\n",
       " tensor([[-8.1040, -6.5790, -6.7898,  ..., -7.4911, -7.6105, -7.7426]],\n",
       "        device='cuda:0', grad_fn=<LogSoftmaxBackward>),\n",
       " tensor([[-8.2203, -6.3177, -6.6072,  ..., -7.5367, -7.7191, -7.8075]],\n",
       "        device='cuda:0', grad_fn=<LogSoftmaxBackward>),\n",
       " tensor([[-7.9845, -6.4572, -6.8224,  ..., -7.6658, -7.7440, -7.8898]],\n",
       "        device='cuda:0', grad_fn=<LogSoftmaxBackward>),\n",
       " tensor([[-8.0581, -6.0907, -6.6132,  ..., -7.7707, -7.7896, -7.9759]],\n",
       "        device='cuda:0', grad_fn=<LogSoftmaxBackward>),\n",
       " tensor([[-8.6629, -5.6026, -6.0581,  ..., -7.9695, -8.0522, -8.3011]],\n",
       "        device='cuda:0', grad_fn=<LogSoftmaxBackward>),\n",
       " tensor([[-8.8463, -5.3880, -5.9047,  ..., -8.1196, -8.2356, -8.4619]],\n",
       "        device='cuda:0', grad_fn=<LogSoftmaxBackward>),\n",
       " tensor([[-8.0305, -5.8803, -6.5205,  ..., -7.6265, -7.6575, -7.7975]],\n",
       "        device='cuda:0', grad_fn=<LogSoftmaxBackward>),\n",
       " tensor([[-9.0887, -4.9463, -5.6421,  ..., -8.3732, -8.3903, -8.6675]],\n",
       "        device='cuda:0', grad_fn=<LogSoftmaxBackward>),\n",
       " tensor([[-9.1749, -4.7001, -5.3909,  ..., -8.4927, -8.5031, -8.7228]],\n",
       "        device='cuda:0', grad_fn=<LogSoftmaxBackward>),\n",
       " tensor([[-9.3088, -4.4902, -5.2253,  ..., -8.4988, -8.5709, -8.7781]],\n",
       "        device='cuda:0', grad_fn=<LogSoftmaxBackward>),\n",
       " tensor([[-9.2857, -4.2928, -5.0561,  ..., -8.4445, -8.5478, -8.7306]],\n",
       "        device='cuda:0', grad_fn=<LogSoftmaxBackward>),\n",
       " tensor([[-9.3419, -4.0710, -4.8424,  ..., -8.4789, -8.6340, -8.7797]],\n",
       "        device='cuda:0', grad_fn=<LogSoftmaxBackward>),\n",
       " tensor([[-9.6277, -3.8238, -4.7209,  ..., -8.6808, -8.8443, -8.9542]],\n",
       "        device='cuda:0', grad_fn=<LogSoftmaxBackward>),\n",
       " tensor([[-9.6891, -3.5846, -4.6216,  ..., -8.7795, -8.8406, -9.0090]],\n",
       "        device='cuda:0', grad_fn=<LogSoftmaxBackward>),\n",
       " tensor([[-9.7702, -3.4120, -4.4314,  ..., -8.8094, -8.9792, -9.0595]],\n",
       "        device='cuda:0', grad_fn=<LogSoftmaxBackward>),\n",
       " tensor([[-9.8713, -3.1942, -4.3320,  ..., -8.9474, -9.0127, -9.1385]],\n",
       "        device='cuda:0', grad_fn=<LogSoftmaxBackward>),\n",
       " tensor([[-9.9117, -3.1255, -4.1451,  ..., -8.9740, -9.1160, -9.2231]],\n",
       "        device='cuda:0', grad_fn=<LogSoftmaxBackward>),\n",
       " tensor([[-10.0457,  -2.9580,  -4.0706,  ...,  -9.1078,  -9.1761,  -9.3145]],\n",
       "        device='cuda:0', grad_fn=<LogSoftmaxBackward>),\n",
       " tensor([[-9.9502, -2.9734, -3.9325,  ..., -9.1278, -9.1969, -9.2821]],\n",
       "        device='cuda:0', grad_fn=<LogSoftmaxBackward>),\n",
       " tensor([[-10.1405,  -2.9715,  -3.8503,  ...,  -9.2109,  -9.3408,  -9.4698]],\n",
       "        device='cuda:0', grad_fn=<LogSoftmaxBackward>),\n",
       " tensor([[-10.2200,  -2.9474,  -3.7561,  ...,  -9.2969,  -9.4221,  -9.5477]],\n",
       "        device='cuda:0', grad_fn=<LogSoftmaxBackward>),\n",
       " tensor([[-10.2446,  -2.9129,  -3.6983,  ...,  -9.3455,  -9.4539,  -9.6070]],\n",
       "        device='cuda:0', grad_fn=<LogSoftmaxBackward>),\n",
       " tensor([[-10.4033,  -2.9619,  -3.6480,  ...,  -9.4710,  -9.5885,  -9.7087]],\n",
       "        device='cuda:0', grad_fn=<LogSoftmaxBackward>),\n",
       " tensor([[-10.4196,  -3.0021,  -3.5604,  ...,  -9.5011,  -9.6640,  -9.7827]],\n",
       "        device='cuda:0', grad_fn=<LogSoftmaxBackward>),\n",
       " tensor([[-10.4855,  -3.0342,  -3.5134,  ...,  -9.5763,  -9.7436,  -9.8683]],\n",
       "        device='cuda:0', grad_fn=<LogSoftmaxBackward>),\n",
       " tensor([[-10.6412,  -3.0163,  -3.4507,  ...,  -9.7185,  -9.7886,  -9.9405]],\n",
       "        device='cuda:0', grad_fn=<LogSoftmaxBackward>),\n",
       " tensor([[-10.6665,  -3.0063,  -3.3965,  ...,  -9.7718,  -9.8758, -10.0487]],\n",
       "        device='cuda:0', grad_fn=<LogSoftmaxBackward>),\n",
       " tensor([[-10.8229,  -2.9183,  -3.3444,  ...,  -9.8936, -10.0449, -10.1353]],\n",
       "        device='cuda:0', grad_fn=<LogSoftmaxBackward>),\n",
       " tensor([[-10.7083,  -3.0678,  -3.3314,  ...,  -9.8110, -10.0141, -10.1310]],\n",
       "        device='cuda:0', grad_fn=<LogSoftmaxBackward>),\n",
       " tensor([[-10.3788,  -3.1177,  -3.4037,  ...,  -9.6556,  -9.7808,  -9.9192]],\n",
       "        device='cuda:0', grad_fn=<LogSoftmaxBackward>),\n",
       " tensor([[-10.8017,  -3.1120,  -3.3609,  ...,  -9.9371, -10.1111, -10.2373]],\n",
       "        device='cuda:0', grad_fn=<LogSoftmaxBackward>),\n",
       " tensor([[-10.8487,  -3.1229,  -3.3839,  ...,  -9.9882, -10.1718, -10.2858]],\n",
       "        device='cuda:0', grad_fn=<LogSoftmaxBackward>),\n",
       " tensor([[-11.1215,  -3.1743,  -3.4322,  ..., -10.2234, -10.3236, -10.3680]],\n",
       "        device='cuda:0', grad_fn=<LogSoftmaxBackward>),\n",
       " tensor([[-11.1652,  -2.9827,  -3.4066,  ..., -10.3062, -10.3535, -10.4533]],\n",
       "        device='cuda:0', grad_fn=<LogSoftmaxBackward>),\n",
       " tensor([[-10.9775,  -3.0636,  -3.4162,  ..., -10.1546, -10.3042, -10.4188]],\n",
       "        device='cuda:0', grad_fn=<LogSoftmaxBackward>),\n",
       " tensor([[-10.9742,  -3.0605,  -3.4336,  ..., -10.1535, -10.3278, -10.4404]],\n",
       "        device='cuda:0', grad_fn=<LogSoftmaxBackward>),\n",
       " tensor([[-10.9595,  -3.0449,  -3.4730,  ..., -10.1351, -10.3496, -10.4434]],\n",
       "        device='cuda:0', grad_fn=<LogSoftmaxBackward>),\n",
       " tensor([[-10.2424,  -3.2133,  -3.6454,  ...,  -9.7020,  -9.8030,  -9.8728]],\n",
       "        device='cuda:0', grad_fn=<LogSoftmaxBackward>),\n",
       " tensor([[-11.3427,  -2.8648,  -3.5310,  ..., -10.4863, -10.5603, -10.6567]],\n",
       "        device='cuda:0', grad_fn=<LogSoftmaxBackward>),\n",
       " tensor([[-11.1288,  -2.9115,  -3.5321,  ..., -10.2988, -10.4619, -10.5630]],\n",
       "        device='cuda:0', grad_fn=<LogSoftmaxBackward>),\n",
       " tensor([[-11.1857,  -2.8830,  -3.5176,  ..., -10.3610, -10.5201, -10.6152]],\n",
       "        device='cuda:0', grad_fn=<LogSoftmaxBackward>),\n",
       " tensor([[-11.4267,  -2.9446,  -3.5624,  ..., -10.6225, -10.6280, -10.7004]],\n",
       "        device='cuda:0', grad_fn=<LogSoftmaxBackward>),\n",
       " tensor([[-11.4259,  -2.7807,  -3.5037,  ..., -10.6481, -10.6689, -10.7178]],\n",
       "        device='cuda:0', grad_fn=<LogSoftmaxBackward>),\n",
       " tensor([[-11.2194,  -2.9398,  -3.4490,  ..., -10.4255, -10.5381, -10.6295]],\n",
       "        device='cuda:0', grad_fn=<LogSoftmaxBackward>),\n",
       " tensor([[-11.4392,  -3.0633,  -3.4924,  ..., -10.7129, -10.6394, -10.7376]],\n",
       "        device='cuda:0', grad_fn=<LogSoftmaxBackward>),\n",
       " tensor([[-11.4151,  -3.0233,  -3.5122,  ..., -10.6956, -10.6562, -10.7269]],\n",
       "        device='cuda:0', grad_fn=<LogSoftmaxBackward>),\n",
       " tensor([[-11.1393,  -3.2131,  -3.4686,  ..., -10.4350, -10.4914, -10.6228]],\n",
       "        device='cuda:0', grad_fn=<LogSoftmaxBackward>),\n",
       " tensor([[-11.2260,  -3.4064,  -3.6250,  ..., -10.6423, -10.5087, -10.6389]],\n",
       "        device='cuda:0', grad_fn=<LogSoftmaxBackward>),\n",
       " tensor([[-11.3442,  -3.3246,  -3.6402,  ..., -10.7489, -10.5315, -10.7609]],\n",
       "        device='cuda:0', grad_fn=<LogSoftmaxBackward>),\n",
       " tensor([[-11.3720,  -3.3457,  -3.7308,  ..., -10.7838, -10.6086, -10.7406]],\n",
       "        device='cuda:0', grad_fn=<LogSoftmaxBackward>),\n",
       " tensor([[-11.3411,  -3.5622,  -3.7688,  ..., -10.7706, -10.6403, -10.7704]],\n",
       "        device='cuda:0', grad_fn=<LogSoftmaxBackward>),\n",
       " tensor([[-10.9831,  -3.5453,  -3.8241,  ..., -10.4252, -10.4105, -10.5873]],\n",
       "        device='cuda:0', grad_fn=<LogSoftmaxBackward>),\n",
       " tensor([[-11.3003,  -3.4788,  -4.0409,  ..., -10.7811, -10.5584, -10.7306]],\n",
       "        device='cuda:0', grad_fn=<LogSoftmaxBackward>),\n",
       " tensor([[-11.2112,  -3.6738,  -4.1753,  ..., -10.7282, -10.4693, -10.6742]],\n",
       "        device='cuda:0', grad_fn=<LogSoftmaxBackward>),\n",
       " tensor([[-11.2607,  -3.5696,  -4.2219,  ..., -10.7310, -10.5359, -10.7234]],\n",
       "        device='cuda:0', grad_fn=<LogSoftmaxBackward>),\n",
       " tensor([[-10.9182,  -3.7017,  -4.1804,  ..., -10.3917, -10.3439, -10.5710]],\n",
       "        device='cuda:0', grad_fn=<LogSoftmaxBackward>),\n",
       " tensor([[-10.9292,  -3.6959,  -4.2348,  ..., -10.3847, -10.3676, -10.5740]],\n",
       "        device='cuda:0', grad_fn=<LogSoftmaxBackward>),\n",
       " tensor([[-10.3240,  -3.6809,  -4.2633,  ...,  -9.9200,  -9.9190, -10.1300]],\n",
       "        device='cuda:0', grad_fn=<LogSoftmaxBackward>),\n",
       " tensor([[-10.9239,  -3.7061,  -4.3606,  ..., -10.4006, -10.3601, -10.5851]],\n",
       "        device='cuda:0', grad_fn=<LogSoftmaxBackward>),\n",
       " tensor([[-10.6080,  -3.1456,  -4.3808,  ..., -10.2803, -10.1607, -10.3190]],\n",
       "        device='cuda:0', grad_fn=<LogSoftmaxBackward>),\n",
       " tensor([[-11.2588,  -3.4826,  -4.5554,  ..., -10.8275, -10.5881, -10.7824]],\n",
       "        device='cuda:0', grad_fn=<LogSoftmaxBackward>),\n",
       " tensor([[-11.2531,  -3.4566,  -4.4910,  ..., -10.9080, -10.6375, -10.8024]],\n",
       "        device='cuda:0', grad_fn=<LogSoftmaxBackward>),\n",
       " tensor([[-11.2806,  -3.6536,  -4.5063,  ..., -10.8993, -10.6489, -10.8289]],\n",
       "        device='cuda:0', grad_fn=<LogSoftmaxBackward>),\n",
       " tensor([[-10.9088,  -3.6001,  -4.3933,  ..., -10.4433, -10.3753, -10.6157]],\n",
       "        device='cuda:0', grad_fn=<LogSoftmaxBackward>),\n",
       " tensor([[-11.0581,  -3.6369,  -4.5387,  ..., -10.7019, -10.5663, -10.7393]],\n",
       "        device='cuda:0', grad_fn=<LogSoftmaxBackward>),\n",
       " tensor([[-11.2096,  -3.1554,  -4.5079,  ..., -10.8916, -10.7037, -10.8414]],\n",
       "        device='cuda:0', grad_fn=<LogSoftmaxBackward>),\n",
       " tensor([[-10.8850,  -3.4270,  -4.3906,  ..., -10.4259, -10.3998, -10.6204]],\n",
       "        device='cuda:0', grad_fn=<LogSoftmaxBackward>),\n",
       " tensor([[-11.1202,  -3.5753,  -4.6604,  ..., -10.7338, -10.6702, -10.8520]],\n",
       "        device='cuda:0', grad_fn=<LogSoftmaxBackward>),\n",
       " tensor([[-11.1916,  -2.8479,  -4.5555,  ..., -10.8593, -10.7584, -10.8954]],\n",
       "        device='cuda:0', grad_fn=<LogSoftmaxBackward>),\n",
       " tensor([[-11.1521,  -2.8523,  -4.6494,  ..., -10.7799, -10.6999, -10.8687]],\n",
       "        device='cuda:0', grad_fn=<LogSoftmaxBackward>),\n",
       " tensor([[-10.8969,  -3.0106,  -4.4318,  ..., -10.4467, -10.4606, -10.6944]],\n",
       "        device='cuda:0', grad_fn=<LogSoftmaxBackward>),\n",
       " tensor([[-10.9089,  -2.9062,  -4.4085,  ..., -10.4515, -10.4743, -10.7159]],\n",
       "        device='cuda:0', grad_fn=<LogSoftmaxBackward>),\n",
       " tensor([[-11.2841,  -2.2942,  -4.3334,  ..., -10.9738, -10.8860, -11.0039]],\n",
       "        device='cuda:0', grad_fn=<LogSoftmaxBackward>),\n",
       " tensor([[-10.8823,  -2.8492,  -4.2863,  ..., -10.4763, -10.4896, -10.7023]],\n",
       "        device='cuda:0', grad_fn=<LogSoftmaxBackward>),\n",
       " tensor([[-10.8879,  -2.7721,  -4.2447,  ..., -10.5037, -10.5091, -10.7022]],\n",
       "        device='cuda:0', grad_fn=<LogSoftmaxBackward>),\n",
       " tensor([[-10.9009,  -2.7539,  -4.2163,  ..., -10.5172, -10.5021, -10.7016]],\n",
       "        device='cuda:0', grad_fn=<LogSoftmaxBackward>),\n",
       " tensor([[-11.3933,  -2.0480,  -4.1802,  ..., -11.0699, -10.9862, -11.0925]],\n",
       "        device='cuda:0', grad_fn=<LogSoftmaxBackward>),\n",
       " tensor([[-10.9197,  -2.7381,  -4.1518,  ..., -10.5241, -10.4943, -10.7101]],\n",
       "        device='cuda:0', grad_fn=<LogSoftmaxBackward>),\n",
       " tensor([[-10.0243,  -2.7513,  -4.3082,  ...,  -9.6040,  -9.5651,  -9.6948]],\n",
       "        device='cuda:0', grad_fn=<LogSoftmaxBackward>),\n",
       " tensor([[-11.4215,  -1.9667,  -4.0221,  ..., -11.0863, -10.9754, -11.1160]],\n",
       "        device='cuda:0', grad_fn=<LogSoftmaxBackward>),\n",
       " tensor([[-11.4140,  -2.6341,  -3.9668,  ..., -11.0345, -10.9697, -10.9723]],\n",
       "        device='cuda:0', grad_fn=<LogSoftmaxBackward>),\n",
       " tensor([[-10.9993,  -2.6952,  -4.0025,  ..., -10.5965, -10.5156, -10.7451]],\n",
       "        device='cuda:0', grad_fn=<LogSoftmaxBackward>),\n",
       " tensor([[-8.8268, -4.1342, -4.6506,  ..., -8.5983, -8.6027, -8.6471]],\n",
       "        device='cuda:0', grad_fn=<LogSoftmaxBackward>),\n",
       " tensor([[-10.9942,  -2.9794,  -3.8214,  ..., -10.5870, -10.5002, -10.6273]],\n",
       "        device='cuda:0', grad_fn=<LogSoftmaxBackward>),\n",
       " tensor([[-11.6578,  -2.2466,  -3.8683,  ..., -11.2621, -11.1240, -11.1579]],\n",
       "        device='cuda:0', grad_fn=<LogSoftmaxBackward>),\n",
       " tensor([[-11.1384,  -2.8803,  -3.6729,  ..., -10.7125, -10.6215, -10.6795]],\n",
       "        device='cuda:0', grad_fn=<LogSoftmaxBackward>),\n",
       " tensor([[-11.6062,  -2.3907,  -3.7303,  ..., -11.3040, -11.1245, -11.2916]],\n",
       "        device='cuda:0', grad_fn=<LogSoftmaxBackward>),\n",
       " tensor([[-10.9037,  -3.2061,  -3.5785,  ..., -10.5143, -10.4333, -10.6668]],\n",
       "        device='cuda:0', grad_fn=<LogSoftmaxBackward>),\n",
       " tensor([[-10.9445,  -3.2518,  -3.5204,  ..., -10.5358, -10.4921, -10.6654]],\n",
       "        device='cuda:0', grad_fn=<LogSoftmaxBackward>),\n",
       " tensor([[-11.2004,  -3.6685,  -3.5959,  ..., -10.8115, -10.7464, -10.8922]],\n",
       "        device='cuda:0', grad_fn=<LogSoftmaxBackward>),\n",
       " tensor([[-11.6129,  -2.6338,  -3.5067,  ..., -11.2443, -11.0939, -11.2090]],\n",
       "        device='cuda:0', grad_fn=<LogSoftmaxBackward>),\n",
       " tensor([[-11.3407,  -3.2054,  -3.4386,  ..., -11.0623, -10.9481, -10.9617]],\n",
       "        device='cuda:0', grad_fn=<LogSoftmaxBackward>),\n",
       " tensor([[-11.3717,  -3.2395,  -3.4033,  ..., -11.0839, -10.9635, -10.9908]],\n",
       "        device='cuda:0', grad_fn=<LogSoftmaxBackward>),\n",
       " tensor([[-11.5316,  -3.2504,  -3.4233,  ..., -11.0952, -11.0201, -11.0611]],\n",
       "        device='cuda:0', grad_fn=<LogSoftmaxBackward>),\n",
       " tensor([[-10.8404,  -3.2512,  -3.1828,  ..., -10.4576, -10.4304, -10.6154]],\n",
       "        device='cuda:0', grad_fn=<LogSoftmaxBackward>),\n",
       " tensor([[-10.7866,  -3.4095,  -3.1922,  ..., -10.3127, -10.4127, -10.4548]],\n",
       "        device='cuda:0', grad_fn=<LogSoftmaxBackward>),\n",
       " tensor([[-11.5234,  -2.1928,  -3.3502,  ..., -11.2213, -10.9467, -11.1569]],\n",
       "        device='cuda:0', grad_fn=<LogSoftmaxBackward>),\n",
       " tensor([[-10.7337,  -3.5672,  -3.3585,  ..., -10.3057, -10.3775, -10.4424]],\n",
       "        device='cuda:0', grad_fn=<LogSoftmaxBackward>),\n",
       " tensor([[-11.3456,  -3.3418,  -3.5341,  ..., -10.9609, -10.8641, -10.9739]],\n",
       "        device='cuda:0', grad_fn=<LogSoftmaxBackward>),\n",
       " tensor([[-11.5913,  -2.3566,  -3.5672,  ..., -11.2429, -11.0487, -11.1464]],\n",
       "        device='cuda:0', grad_fn=<LogSoftmaxBackward>),\n",
       " tensor([[-11.6077,  -2.2436,  -3.5680,  ..., -11.2192, -11.0429, -11.1657]],\n",
       "        device='cuda:0', grad_fn=<LogSoftmaxBackward>),\n",
       " tensor([[-11.1529,  -2.7773,  -3.5049,  ..., -10.6183, -10.6922, -10.8603]],\n",
       "        device='cuda:0', grad_fn=<LogSoftmaxBackward>),\n",
       " tensor([[-11.2034,  -2.9265,  -3.6472,  ..., -10.9389, -10.6938, -10.8759]],\n",
       "        device='cuda:0', grad_fn=<LogSoftmaxBackward>),\n",
       " tensor([[-11.4056,  -2.2505,  -3.8538,  ..., -11.2309, -10.9017, -11.0460]],\n",
       "        device='cuda:0', grad_fn=<LogSoftmaxBackward>),\n",
       " tensor([[-11.2206,  -3.0298,  -3.9879,  ..., -11.1189, -10.8649, -11.0430]],\n",
       "        device='cuda:0', grad_fn=<LogSoftmaxBackward>),\n",
       " tensor([[-11.3914,  -2.1643,  -3.9976,  ..., -11.2303, -10.8715, -11.0844]],\n",
       "        device='cuda:0', grad_fn=<LogSoftmaxBackward>),\n",
       " tensor([[-11.4002,  -2.9231,  -3.9535,  ..., -11.1775, -11.0878, -11.1919]],\n",
       "        device='cuda:0', grad_fn=<LogSoftmaxBackward>),\n",
       " tensor([[-11.6715,  -1.8907,  -4.0512,  ..., -11.4250, -11.1611, -11.3516]],\n",
       "        device='cuda:0', grad_fn=<LogSoftmaxBackward>),\n",
       " tensor([[-10.7046,  -3.2605,  -4.0137,  ..., -10.4180, -10.4074, -10.4776]],\n",
       "        device='cuda:0', grad_fn=<LogSoftmaxBackward>),\n",
       " tensor([[-11.7652,  -2.8288,  -4.2185,  ..., -11.4571, -11.3593, -11.4371]],\n",
       "        device='cuda:0', grad_fn=<LogSoftmaxBackward>),\n",
       " tensor([[-10.7976,  -3.2063,  -3.9950,  ..., -10.4466, -10.5039, -10.5648]],\n",
       "        device='cuda:0', grad_fn=<LogSoftmaxBackward>),\n",
       " tensor([[-11.9178,  -1.8341,  -4.2074,  ..., -11.6438, -11.3333, -11.6023]],\n",
       "        device='cuda:0', grad_fn=<LogSoftmaxBackward>),\n",
       " tensor([[-12.0092,  -1.8847,  -4.3150,  ..., -11.7214, -11.4464, -11.6638]],\n",
       "        device='cuda:0', grad_fn=<LogSoftmaxBackward>),\n",
       " tensor([[-11.9401,  -2.0885,  -4.5160,  ..., -11.7185, -11.3732, -11.6283]],\n",
       "        device='cuda:0', grad_fn=<LogSoftmaxBackward>),\n",
       " tensor([[-12.0608,  -3.2160,  -4.6224,  ..., -11.7675, -11.5440, -11.6990]],\n",
       "        device='cuda:0', grad_fn=<LogSoftmaxBackward>),\n",
       " tensor([[-10.8847,  -3.5766,  -4.1836,  ..., -10.4532, -10.5732, -10.7077]],\n",
       "        device='cuda:0', grad_fn=<LogSoftmaxBackward>),\n",
       " tensor([[-10.9602,  -3.2219,  -4.1111,  ..., -10.4951, -10.6315, -10.7224]],\n",
       "        device='cuda:0', grad_fn=<LogSoftmaxBackward>),\n",
       " tensor([[-12.0189,  -1.7415,  -4.3999,  ..., -11.7030, -11.4949, -11.6973]],\n",
       "        device='cuda:0', grad_fn=<LogSoftmaxBackward>),\n",
       " tensor([[-11.0559,  -3.0313,  -4.1630,  ..., -10.6798, -10.6635, -10.8991]],\n",
       "        device='cuda:0', grad_fn=<LogSoftmaxBackward>),\n",
       " tensor([[-12.3568,  -3.0456,  -4.5953,  ..., -11.9571, -11.8993, -11.9621]],\n",
       "        device='cuda:0', grad_fn=<LogSoftmaxBackward>),\n",
       " tensor([[-12.0831,  -1.6838,  -4.5389,  ..., -11.7303, -11.5911, -11.8024]],\n",
       "        device='cuda:0', grad_fn=<LogSoftmaxBackward>),\n",
       " tensor([[-11.9104,  -2.9913,  -4.6266,  ..., -11.5536, -11.3568, -11.4999]],\n",
       "        device='cuda:0', grad_fn=<LogSoftmaxBackward>),\n",
       " tensor([[-11.1064,  -3.1117,  -4.3554,  ..., -10.6976, -10.7092, -10.9407]],\n",
       "        device='cuda:0', grad_fn=<LogSoftmaxBackward>),\n",
       " tensor([[-11.9903,  -1.5825,  -4.6030,  ..., -11.5778, -11.4681, -11.6731]],\n",
       "        device='cuda:0', grad_fn=<LogSoftmaxBackward>),\n",
       " tensor([[-11.0782,  -3.0938,  -4.3853,  ..., -10.6606, -10.7095, -10.8998]],\n",
       "        device='cuda:0', grad_fn=<LogSoftmaxBackward>),\n",
       " tensor([[-12.1461,  -1.4273,  -4.7116,  ..., -11.8280, -11.6237, -11.9440]],\n",
       "        device='cuda:0', grad_fn=<LogSoftmaxBackward>),\n",
       " tensor([[-12.0571,  -1.5140,  -4.6920,  ..., -11.6214, -11.5732, -11.7209]],\n",
       "        device='cuda:0', grad_fn=<LogSoftmaxBackward>),\n",
       " tensor([[-11.1567,  -3.1595,  -4.4110,  ..., -10.6773, -10.7101, -10.9008]],\n",
       "        device='cuda:0', grad_fn=<LogSoftmaxBackward>),\n",
       " tensor([[-11.6769,  -2.9078,  -4.6137,  ..., -11.1958, -11.3071, -11.3617]],\n",
       "        device='cuda:0', grad_fn=<LogSoftmaxBackward>),\n",
       " tensor([[-11.6889,  -1.1880,  -4.7106,  ..., -11.2392, -11.0409, -11.2728]],\n",
       "        device='cuda:0', grad_fn=<LogSoftmaxBackward>),\n",
       " tensor([[-11.1714,  -3.3003,  -4.4079,  ..., -10.6438, -10.7683, -10.8655]],\n",
       "        device='cuda:0', grad_fn=<LogSoftmaxBackward>),\n",
       " tensor([[-11.1816,  -3.2506,  -4.3809,  ..., -10.6905, -10.7771, -10.9121]],\n",
       "        device='cuda:0', grad_fn=<LogSoftmaxBackward>),\n",
       " tensor([[-11.1919,  -3.1829,  -4.3074,  ..., -10.6989, -10.8108, -10.9494]],\n",
       "        device='cuda:0', grad_fn=<LogSoftmaxBackward>),\n",
       " tensor([[-12.4243,  -1.5721,  -4.7330,  ..., -12.0985, -11.9609, -12.0764]],\n",
       "        device='cuda:0', grad_fn=<LogSoftmaxBackward>),\n",
       " tensor([[-11.0894,  -3.1847,  -4.3783,  ..., -10.5553, -10.6704, -10.7465]],\n",
       "        device='cuda:0', grad_fn=<LogSoftmaxBackward>),\n",
       " tensor([[-12.4348,  -1.6748,  -4.7444,  ..., -11.9704, -11.9924, -12.0359]],\n",
       "        device='cuda:0', grad_fn=<LogSoftmaxBackward>),\n",
       " tensor([[-11.1024,  -3.0743,  -4.3201,  ..., -10.5590, -10.7145, -10.7856]],\n",
       "        device='cuda:0', grad_fn=<LogSoftmaxBackward>),\n",
       " tensor([[-11.5998,  -2.7732,  -4.2751,  ..., -10.8910, -11.0741, -11.2282]],\n",
       "        device='cuda:0', grad_fn=<LogSoftmaxBackward>),\n",
       " tensor([[-11.1457,  -3.2157,  -4.2766,  ..., -10.4583, -10.7498, -10.7007]],\n",
       "        device='cuda:0', grad_fn=<LogSoftmaxBackward>),\n",
       " tensor([[-12.6290,  -1.5367,  -4.7110,  ..., -12.1487, -12.1707, -12.1684]],\n",
       "        device='cuda:0', grad_fn=<LogSoftmaxBackward>),\n",
       " tensor([[-12.5433,  -1.3022,  -4.7133,  ..., -12.1629, -12.0124, -12.2203]],\n",
       "        device='cuda:0', grad_fn=<LogSoftmaxBackward>),\n",
       " tensor([[-12.6712,  -1.2840,  -4.7318,  ..., -12.2612, -12.1465, -12.3234]],\n",
       "        device='cuda:0', grad_fn=<LogSoftmaxBackward>),\n",
       " tensor([[-11.0776,  -3.2872,  -4.2821,  ..., -10.4108, -10.6631, -10.6705]],\n",
       "        device='cuda:0', grad_fn=<LogSoftmaxBackward>),\n",
       " tensor([[-11.1521,  -2.8877,  -4.1627,  ..., -10.5032, -10.7075, -10.8334]],\n",
       "        device='cuda:0', grad_fn=<LogSoftmaxBackward>),\n",
       " tensor([[-11.1745,  -2.9435,  -4.1144,  ..., -10.5283, -10.7325, -10.8349]],\n",
       "        device='cuda:0', grad_fn=<LogSoftmaxBackward>),\n",
       " tensor([[-12.4739,  -1.3259,  -4.5233,  ..., -12.1134, -11.9748, -12.1727]],\n",
       "        device='cuda:0', grad_fn=<LogSoftmaxBackward>),\n",
       " tensor([[-11.9804,  -2.9718,  -4.1922,  ..., -11.5521, -11.6242, -11.5864]],\n",
       "        device='cuda:0', grad_fn=<LogSoftmaxBackward>),\n",
       " tensor([[-12.4334,  -1.4240,  -4.4192,  ..., -12.0264, -11.9305, -12.1442]],\n",
       "        device='cuda:0', grad_fn=<LogSoftmaxBackward>),\n",
       " tensor([[-11.2386,  -3.6810,  -3.9994,  ..., -10.5136, -10.7678, -10.8020]],\n",
       "        device='cuda:0', grad_fn=<LogSoftmaxBackward>),\n",
       " tensor([[-12.5533,  -1.4089,  -4.3831,  ..., -12.1803, -12.0251, -12.3008]],\n",
       "        device='cuda:0', grad_fn=<LogSoftmaxBackward>),\n",
       " tensor([[-12.4048,  -1.4774,  -4.1848,  ..., -11.9310, -11.7761, -12.0422]],\n",
       "        device='cuda:0', grad_fn=<LogSoftmaxBackward>),\n",
       " tensor([[-12.5134,  -1.4326,  -4.2812,  ..., -12.0912, -11.8993, -12.2437]],\n",
       "        device='cuda:0', grad_fn=<LogSoftmaxBackward>),\n",
       " tensor([[-11.3207,  -3.3148,  -3.8364,  ..., -10.6772, -10.9189, -11.0362]],\n",
       "        device='cuda:0', grad_fn=<LogSoftmaxBackward>),\n",
       " tensor([[-11.3359,  -3.2909,  -3.8051,  ..., -10.6876, -10.9067, -11.0320]],\n",
       "        device='cuda:0', grad_fn=<LogSoftmaxBackward>),\n",
       " tensor([[-12.3494,  -1.2650,  -4.3566,  ..., -11.9985, -11.7478, -12.1242]],\n",
       "        device='cuda:0', grad_fn=<LogSoftmaxBackward>),\n",
       " tensor([[-11.3551,  -3.2929,  -3.8736,  ..., -10.7315, -10.9718, -11.1000]],\n",
       "        device='cuda:0', grad_fn=<LogSoftmaxBackward>),\n",
       " tensor([[-11.4101,  -3.2592,  -3.8829,  ..., -10.7810, -11.0117, -11.1418]],\n",
       "        device='cuda:0', grad_fn=<LogSoftmaxBackward>),\n",
       " tensor([[-11.4207,  -3.2528,  -3.9385,  ..., -10.8196, -11.0368, -11.1686]],\n",
       "        device='cuda:0', grad_fn=<LogSoftmaxBackward>),\n",
       " tensor([[-12.7166,  -1.1577,  -4.6134,  ..., -12.3970, -12.1262, -12.6279]],\n",
       "        device='cuda:0', grad_fn=<LogSoftmaxBackward>),\n",
       " tensor([[-12.9674,  -1.1945,  -4.7658,  ..., -12.7083, -12.4172, -12.8918]],\n",
       "        device='cuda:0', grad_fn=<LogSoftmaxBackward>),\n",
       " tensor([[-11.5311,  -3.2404,  -4.0490,  ..., -10.9590, -11.1671, -11.2973]],\n",
       "        device='cuda:0', grad_fn=<LogSoftmaxBackward>),\n",
       " tensor([[-11.5341,  -3.2289,  -4.1063,  ..., -10.9943, -11.1696, -11.3125]],\n",
       "        device='cuda:0', grad_fn=<LogSoftmaxBackward>),\n",
       " tensor([[-13.2152,  -1.3294,  -4.7291,  ..., -12.7316, -12.3683, -13.1306]],\n",
       "        device='cuda:0', grad_fn=<LogSoftmaxBackward>),\n",
       " tensor([[-11.6705,  -3.2235,  -4.0978,  ..., -11.0825, -11.2672, -11.4308]],\n",
       "        device='cuda:0', grad_fn=<LogSoftmaxBackward>),\n",
       " tensor([[-12.8111,  -3.1176,  -4.5657,  ..., -12.3131, -12.2545, -12.6244]],\n",
       "        device='cuda:0', grad_fn=<LogSoftmaxBackward>),\n",
       " tensor([[-11.6838,  -3.3456,  -4.0320,  ..., -11.0455, -11.2159, -11.4332]],\n",
       "        device='cuda:0', grad_fn=<LogSoftmaxBackward>),\n",
       " tensor([[-11.7222,  -3.0721,  -4.0816,  ..., -11.1306, -11.3195, -11.4385]],\n",
       "        device='cuda:0', grad_fn=<LogSoftmaxBackward>),\n",
       " tensor([[-12.7536,  -3.1590,  -4.3980,  ..., -12.1152, -12.1430, -12.4558]],\n",
       "        device='cuda:0', grad_fn=<LogSoftmaxBackward>),\n",
       " tensor([[-11.6976,  -2.9952,  -3.9977,  ..., -11.1142, -11.2716, -11.4279]],\n",
       "        device='cuda:0', grad_fn=<LogSoftmaxBackward>),\n",
       " tensor([[-11.7371,  -2.8905,  -4.0034,  ..., -11.1497, -11.3193, -11.4529]],\n",
       "        device='cuda:0', grad_fn=<LogSoftmaxBackward>),\n",
       " tensor([[-11.7000,  -2.8402,  -4.0123,  ..., -11.1264, -11.2985, -11.4342]],\n",
       "        device='cuda:0', grad_fn=<LogSoftmaxBackward>),\n",
       " tensor([[-11.7519,  -2.7690,  -4.0174,  ..., -11.1735, -11.3404, -11.4862]],\n",
       "        device='cuda:0', grad_fn=<LogSoftmaxBackward>),\n",
       " tensor([[-13.8115,  -0.8104,  -4.8992,  ..., -13.2321, -12.9625, -13.6840]],\n",
       "        device='cuda:0', grad_fn=<LogSoftmaxBackward>),\n",
       " tensor([[-11.9642,  -2.6924,  -4.1253,  ..., -11.2358, -11.5052, -11.6601]],\n",
       "        device='cuda:0', grad_fn=<LogSoftmaxBackward>),\n",
       " tensor([[-12.8273,  -2.5403,  -4.5858,  ..., -12.1354, -12.1340, -12.5508]],\n",
       "        device='cuda:0', grad_fn=<LogSoftmaxBackward>),\n",
       " tensor([[-11.9465,  -2.5469,  -4.1437,  ..., -11.2389, -11.5002, -11.6613]],\n",
       "        device='cuda:0', grad_fn=<LogSoftmaxBackward>),\n",
       " tensor([[-11.7051,  -2.6622,  -4.0838,  ..., -11.0647, -11.3357, -11.4997]],\n",
       "        device='cuda:0', grad_fn=<LogSoftmaxBackward>),\n",
       " tensor([[-11.8863,  -2.4584,  -4.1781,  ..., -11.1814, -11.4804, -11.6296]],\n",
       "        device='cuda:0', grad_fn=<LogSoftmaxBackward>),\n",
       " tensor([[-11.9541,  -2.3849,  -4.1764,  ..., -11.2142, -11.5179, -11.6729]],\n",
       "        device='cuda:0', grad_fn=<LogSoftmaxBackward>),\n",
       " tensor([[-11.7639,  -2.3570,  -4.0780,  ..., -11.1238, -11.4233, -11.5117]],\n",
       "        device='cuda:0', grad_fn=<LogSoftmaxBackward>),\n",
       " tensor([[-11.7997,  -2.3574,  -4.0749,  ..., -11.1568, -11.4722, -11.5490]],\n",
       "        device='cuda:0', grad_fn=<LogSoftmaxBackward>),\n",
       " tensor([[-13.6159,  -0.9536,  -5.1288,  ..., -13.0386, -12.8633, -13.4434]],\n",
       "        device='cuda:0', grad_fn=<LogSoftmaxBackward>),\n",
       " tensor([[-11.6657,  -2.9877,  -4.0260,  ..., -10.9092, -11.3635, -11.4489]],\n",
       "        device='cuda:0', grad_fn=<LogSoftmaxBackward>),\n",
       " tensor([[-11.8858,  -2.3564,  -4.1042,  ..., -11.2411, -11.5859, -11.6342]],\n",
       "        device='cuda:0', grad_fn=<LogSoftmaxBackward>),\n",
       " tensor([[-11.9612,  -2.4327,  -4.0987,  ..., -11.2993, -11.6062, -11.6867]],\n",
       "        device='cuda:0', grad_fn=<LogSoftmaxBackward>),\n",
       " tensor([[-11.9476,  -2.2658,  -4.1101,  ..., -11.3067, -11.6268, -11.6804]],\n",
       "        device='cuda:0', grad_fn=<LogSoftmaxBackward>),\n",
       " tensor([[-12.1589,  -1.9862,  -4.4111,  ..., -11.4680, -11.7132, -11.9621]],\n",
       "        device='cuda:0', grad_fn=<LogSoftmaxBackward>),\n",
       " tensor([[-12.5369,  -1.9934,  -4.4786,  ..., -11.8086, -12.1650, -12.0765]],\n",
       "        device='cuda:0', grad_fn=<LogSoftmaxBackward>),\n",
       " tensor([[-11.9555,  -2.2394,  -4.1030,  ..., -11.2817, -11.6083, -11.6722]],\n",
       "        device='cuda:0', grad_fn=<LogSoftmaxBackward>),\n",
       " tensor([[-11.9045,  -2.2300,  -4.1120,  ..., -11.2545, -11.5884, -11.6501]],\n",
       "        device='cuda:0', grad_fn=<LogSoftmaxBackward>),\n",
       " tensor([[-13.8590,  -0.6355,  -5.3592,  ..., -13.3289, -13.2963, -13.6005]],\n",
       "        device='cuda:0', grad_fn=<LogSoftmaxBackward>),\n",
       " tensor([[-12.0262,  -2.1956,  -4.1939,  ..., -11.2680, -11.6180, -11.7222]],\n",
       "        device='cuda:0', grad_fn=<LogSoftmaxBackward>),\n",
       " tensor([[-12.1080,  -2.1351,  -4.1771,  ..., -11.3029, -11.6524, -11.7336]],\n",
       "        device='cuda:0', grad_fn=<LogSoftmaxBackward>),\n",
       " tensor([[-12.0518,  -2.1696,  -4.1746,  ..., -11.2491, -11.6085, -11.6946]],\n",
       "        device='cuda:0', grad_fn=<LogSoftmaxBackward>),\n",
       " tensor([[-12.0382,  -2.1602,  -4.1900,  ..., -11.2650, -11.6032, -11.6814]],\n",
       "        device='cuda:0', grad_fn=<LogSoftmaxBackward>),\n",
       " tensor([[-12.0637,  -2.1345,  -4.1955,  ..., -11.2794, -11.6125, -11.6970]],\n",
       "        device='cuda:0', grad_fn=<LogSoftmaxBackward>),\n",
       " tensor([[-13.8739,  -0.4493,  -5.4477,  ..., -13.3803, -13.3021, -13.6469]],\n",
       "        device='cuda:0', grad_fn=<LogSoftmaxBackward>),\n",
       " tensor([[-12.0235,  -2.0593,  -4.2186,  ..., -11.2358, -11.5384, -11.6528]],\n",
       "        device='cuda:0', grad_fn=<LogSoftmaxBackward>),\n",
       " tensor([[-12.3085,  -2.0949,  -4.3414,  ..., -11.3835, -11.7517, -11.9185]],\n",
       "        device='cuda:0', grad_fn=<LogSoftmaxBackward>),\n",
       " tensor([[-13.9205,  -0.4220,  -5.5795,  ..., -13.4397, -13.2808, -13.6993]],\n",
       "        device='cuda:0', grad_fn=<LogSoftmaxBackward>),\n",
       " tensor([[-11.9264,  -2.1186,  -4.2651,  ..., -11.1690, -11.4258, -11.5873]],\n",
       "        device='cuda:0', grad_fn=<LogSoftmaxBackward>),\n",
       " tensor([[-11.9090,  -2.1832,  -4.2915,  ..., -11.1592, -11.3961, -11.5997]],\n",
       "        device='cuda:0', grad_fn=<LogSoftmaxBackward>),\n",
       " tensor([[-14.1602,  -0.3142,  -5.5680,  ..., -13.6679, -13.3509, -13.8532]],\n",
       "        device='cuda:0', grad_fn=<LogSoftmaxBackward>),\n",
       " tensor([[-13.8703,  -2.4516,  -5.0999,  ..., -13.3776, -13.4155, -13.5025]],\n",
       "        device='cuda:0', grad_fn=<LogSoftmaxBackward>),\n",
       " tensor([[-11.7903,  -2.2497,  -4.3770,  ..., -11.0664, -11.3164, -11.5414]],\n",
       "        device='cuda:0', grad_fn=<LogSoftmaxBackward>),\n",
       " tensor([[-11.5639,  -3.2631,  -4.4146,  ..., -10.8414, -11.0813, -11.4426]],\n",
       "        device='cuda:0', grad_fn=<LogSoftmaxBackward>),\n",
       " tensor([[-11.6939,  -2.2264,  -4.4176,  ..., -10.9906, -11.1850, -11.4808]],\n",
       "        device='cuda:0', grad_fn=<LogSoftmaxBackward>),\n",
       " tensor([[-11.5350,  -2.2196,  -4.5033,  ..., -10.8962, -11.0346, -11.4664]],\n",
       "        device='cuda:0', grad_fn=<LogSoftmaxBackward>),\n",
       " tensor([[-12.5001,  -1.7510,  -4.8576,  ..., -11.8996, -11.8584, -12.0509]],\n",
       "        device='cuda:0', grad_fn=<LogSoftmaxBackward>),\n",
       " tensor([[-11.4800,  -2.4882,  -4.6019,  ..., -10.8358, -10.9992, -11.4370]],\n",
       "        device='cuda:0', grad_fn=<LogSoftmaxBackward>),\n",
       " tensor([[-11.1634,  -4.0494,  -4.6588,  ..., -10.3776, -10.7276, -11.2943]],\n",
       "        device='cuda:0', grad_fn=<LogSoftmaxBackward>),\n",
       " tensor([[-14.3835,  -0.4412,  -6.1839,  ..., -14.0568, -13.6249, -14.2793]],\n",
       "        device='cuda:0', grad_fn=<LogSoftmaxBackward>),\n",
       " tensor([[-14.3933,  -0.3736,  -6.3476,  ..., -14.0595, -13.6271, -14.2493]],\n",
       "        device='cuda:0', grad_fn=<LogSoftmaxBackward>),\n",
       " tensor([[-11.5087,  -1.9597,  -4.7379,  ..., -10.8862, -10.9796, -11.4223]],\n",
       "        device='cuda:0', grad_fn=<LogSoftmaxBackward>),\n",
       " tensor([[-14.3713,  -0.3602,  -6.3585,  ..., -14.0603, -13.6377, -14.2742]],\n",
       "        device='cuda:0', grad_fn=<LogSoftmaxBackward>),\n",
       " tensor([[-11.4216,  -2.0295,  -4.8246,  ..., -10.7944, -10.8849, -11.2968]],\n",
       "        device='cuda:0', grad_fn=<LogSoftmaxBackward>),\n",
       " tensor([[-14.0328,  -0.3696,  -6.2554,  ..., -13.6957, -13.3076, -13.8724]],\n",
       "        device='cuda:0', grad_fn=<LogSoftmaxBackward>),\n",
       " tensor([[-11.5438,  -1.9119,  -4.7707,  ..., -10.9351, -10.9564, -11.3302]],\n",
       "        device='cuda:0', grad_fn=<LogSoftmaxBackward>),\n",
       " tensor([[-11.3550,  -1.9825,  -4.8567,  ..., -10.8020, -10.8112, -11.2065]],\n",
       "        device='cuda:0', grad_fn=<LogSoftmaxBackward>),\n",
       " tensor([[-14.1651,  -0.4005,  -6.2539,  ..., -13.8769, -13.3962, -13.9665]],\n",
       "        device='cuda:0', grad_fn=<LogSoftmaxBackward>),\n",
       " tensor([[-12.8194,  -2.2396,  -5.2410,  ..., -12.3807, -12.1852, -12.3040]],\n",
       "        device='cuda:0', grad_fn=<LogSoftmaxBackward>),\n",
       " tensor([[-12.3883,  -2.4479,  -5.1179,  ..., -12.2226, -11.6838, -11.9067]],\n",
       "        device='cuda:0', grad_fn=<LogSoftmaxBackward>),\n",
       " tensor([[-11.8076,  -2.6733,  -5.0426,  ..., -11.7403, -11.2005, -11.3475]],\n",
       "        device='cuda:0', grad_fn=<LogSoftmaxBackward>),\n",
       " tensor([[-14.1529,  -0.4210,  -6.1688,  ..., -13.8127, -13.3588, -13.9304]],\n",
       "        device='cuda:0', grad_fn=<LogSoftmaxBackward>),\n",
       " tensor([[-13.8365,  -0.4712,  -6.1846,  ..., -13.5364, -13.0113, -13.6121]],\n",
       "        device='cuda:0', grad_fn=<LogSoftmaxBackward>),\n",
       " tensor([[-13.9992,  -0.3984,  -6.1500,  ..., -13.6333, -13.2270, -13.7263]],\n",
       "        device='cuda:0', grad_fn=<LogSoftmaxBackward>),\n",
       " tensor([[-14.0197,  -0.3692,  -6.1727,  ..., -13.6396, -13.2365, -13.7744]],\n",
       "        device='cuda:0', grad_fn=<LogSoftmaxBackward>),\n",
       " tensor([[-13.9585,  -0.3526,  -6.1666,  ..., -13.5842, -13.1532, -13.6709]],\n",
       "        device='cuda:0', grad_fn=<LogSoftmaxBackward>),\n",
       " tensor([[-13.9915,  -0.3621,  -6.0801,  ..., -13.6485, -13.2397, -13.6926]],\n",
       "        device='cuda:0', grad_fn=<LogSoftmaxBackward>),\n",
       " tensor([[-10.7028,  -4.3287,  -4.8701,  ..., -10.3033, -10.3387, -10.7723]],\n",
       "        device='cuda:0', grad_fn=<LogSoftmaxBackward>),\n",
       " tensor([[-10.7581,  -3.6285,  -4.8059,  ..., -10.5295, -10.3796, -10.8891]],\n",
       "        device='cuda:0', grad_fn=<LogSoftmaxBackward>),\n",
       " tensor([[-12.1769,  -2.3471,  -4.6881,  ..., -11.8429, -11.7550, -11.8475]],\n",
       "        device='cuda:0', grad_fn=<LogSoftmaxBackward>),\n",
       " tensor([[-11.2252,  -2.3655,  -4.6981,  ..., -10.8879, -10.7117, -11.1823]],\n",
       "        device='cuda:0', grad_fn=<LogSoftmaxBackward>),\n",
       " tensor([[-12.5584,  -2.6523,  -4.6727,  ..., -12.0907, -11.9982, -12.1679]],\n",
       "        device='cuda:0', grad_fn=<LogSoftmaxBackward>),\n",
       " tensor([[-11.5038,  -2.4264,  -4.6220,  ..., -11.1415, -11.0082, -11.4747]],\n",
       "        device='cuda:0', grad_fn=<LogSoftmaxBackward>),\n",
       " tensor([[-12.6454,  -2.6095,  -4.6934,  ..., -12.1730, -12.1348, -12.1429]],\n",
       "        device='cuda:0', grad_fn=<LogSoftmaxBackward>),\n",
       " tensor([[-11.6654,  -2.3688,  -4.6169,  ..., -11.2862, -11.1846, -11.6690]],\n",
       "        device='cuda:0', grad_fn=<LogSoftmaxBackward>),\n",
       " tensor([[-11.7011,  -2.8284,  -4.5866,  ..., -11.3664, -11.2051, -11.7213]],\n",
       "        device='cuda:0', grad_fn=<LogSoftmaxBackward>),\n",
       " tensor([[-14.8915,  -0.5229,  -5.8849,  ..., -14.4104, -14.2457, -14.5732]],\n",
       "        device='cuda:0', grad_fn=<LogSoftmaxBackward>),\n",
       " tensor([[-11.8599,  -2.4287,  -4.4968,  ..., -11.4858, -11.3290, -11.8279]],\n",
       "        device='cuda:0', grad_fn=<LogSoftmaxBackward>),\n",
       " tensor([[-11.8902,  -2.3944,  -4.4346,  ..., -11.5180, -11.3575, -11.8554]],\n",
       "        device='cuda:0', grad_fn=<LogSoftmaxBackward>),\n",
       " tensor([[-11.8580,  -2.2934,  -4.4946,  ..., -11.4838, -11.4093, -11.8777]],\n",
       "        device='cuda:0', grad_fn=<LogSoftmaxBackward>),\n",
       " tensor([[-14.4181,  -0.3907,  -5.5920,  ..., -13.9634, -13.7829, -14.0480]],\n",
       "        device='cuda:0', grad_fn=<LogSoftmaxBackward>),\n",
       " tensor([[-11.9361,  -2.3298,  -4.2946,  ..., -11.5376, -11.3711, -11.8533]],\n",
       "        device='cuda:0', grad_fn=<LogSoftmaxBackward>),\n",
       " tensor([[-15.1237,  -0.2286,  -5.8150,  ..., -14.5979, -14.2977, -14.5951]],\n",
       "        device='cuda:0', grad_fn=<LogSoftmaxBackward>),\n",
       " tensor([[-11.8743,  -2.3886,  -4.1740,  ..., -11.4935, -11.3899, -11.7614]],\n",
       "        device='cuda:0', grad_fn=<LogSoftmaxBackward>),\n",
       " tensor([[-11.7701,  -3.1753,  -4.2186,  ..., -11.4732, -11.4009, -11.7408]],\n",
       "        device='cuda:0', grad_fn=<LogSoftmaxBackward>),\n",
       " tensor([[-13.9302,  -0.4001,  -5.0475,  ..., -13.4479, -13.2074, -13.4322]],\n",
       "        device='cuda:0', grad_fn=<LogSoftmaxBackward>),\n",
       " tensor([[-11.7741,  -4.2277,  -4.3803,  ..., -11.4216, -11.4949, -11.6631]],\n",
       "        device='cuda:0', grad_fn=<LogSoftmaxBackward>),\n",
       " tensor([[-12.3932,  -2.5469,  -3.8940,  ..., -11.7487, -11.6908, -11.7115]],\n",
       "        device='cuda:0', grad_fn=<LogSoftmaxBackward>),\n",
       " tensor([[-12.1212,  -2.5644,  -3.9184,  ..., -11.6405, -11.6658, -11.6449]],\n",
       "        device='cuda:0', grad_fn=<LogSoftmaxBackward>),\n",
       " tensor([[-14.9996,  -0.1867,  -5.6409,  ..., -14.4394, -14.1109, -14.4284]],\n",
       "        device='cuda:0', grad_fn=<LogSoftmaxBackward>),\n",
       " tensor([[-12.4115,  -2.3450,  -3.9701,  ..., -11.8035, -11.7810, -11.7769]],\n",
       "        device='cuda:0', grad_fn=<LogSoftmaxBackward>),\n",
       " tensor([[-11.9071,  -2.1809,  -4.0135,  ..., -11.5082, -11.4600, -11.7742]],\n",
       "        device='cuda:0', grad_fn=<LogSoftmaxBackward>),\n",
       " tensor([[-11.9038,  -2.1771,  -3.9754,  ..., -11.4971, -11.4625, -11.7614]],\n",
       "        device='cuda:0', grad_fn=<LogSoftmaxBackward>),\n",
       " tensor([[-14.3741,  -0.3805,  -5.0991,  ..., -13.9854, -13.7478, -13.9874]],\n",
       "        device='cuda:0', grad_fn=<LogSoftmaxBackward>),\n",
       " tensor([[-14.6332,  -0.4602,  -5.1227,  ..., -14.3681, -14.0425, -14.3061]],\n",
       "        device='cuda:0', grad_fn=<LogSoftmaxBackward>),\n",
       " tensor([[-11.8477,  -2.3054,  -3.9617,  ..., -11.4642, -11.4195, -11.7384]],\n",
       "        device='cuda:0', grad_fn=<LogSoftmaxBackward>),\n",
       " tensor([[-14.1529,  -0.6170,  -5.0166,  ..., -13.9097, -13.5888, -13.8892]],\n",
       "        device='cuda:0', grad_fn=<LogSoftmaxBackward>),\n",
       " tensor([[-13.5829,  -2.4700,  -4.2973,  ..., -13.2823, -13.0803, -13.1522]],\n",
       "        device='cuda:0', grad_fn=<LogSoftmaxBackward>),\n",
       " tensor([[-11.6645,  -4.8875,  -3.8585,  ..., -11.0992, -11.0219, -11.2078]],\n",
       "        device='cuda:0', grad_fn=<LogSoftmaxBackward>),\n",
       " tensor([[-15.3360,  -0.2847,  -5.7287,  ..., -14.8771, -14.5379, -14.9106]],\n",
       "        device='cuda:0', grad_fn=<LogSoftmaxBackward>),\n",
       " tensor([[-11.4499,  -2.7077,  -3.8878,  ..., -11.1507, -11.0742, -11.3360]],\n",
       "        device='cuda:0', grad_fn=<LogSoftmaxBackward>),\n",
       " tensor([[-13.8564,  -1.8083,  -4.6996,  ..., -13.4732, -13.1796, -13.6172]],\n",
       "        device='cuda:0', grad_fn=<LogSoftmaxBackward>),\n",
       " tensor([[-11.3839,  -2.8364,  -3.9187,  ..., -11.1130, -11.0516, -11.2610]],\n",
       "        device='cuda:0', grad_fn=<LogSoftmaxBackward>),\n",
       " tensor([[-13.6331,  -3.8237,  -4.6023,  ..., -13.3152, -13.2213, -13.3760]],\n",
       "        device='cuda:0', grad_fn=<LogSoftmaxBackward>),\n",
       " tensor([[-11.3197,  -2.9139,  -3.9962,  ..., -11.0405, -10.9666, -11.1566]],\n",
       "        device='cuda:0', grad_fn=<LogSoftmaxBackward>),\n",
       " tensor([[-13.3813,  -1.4226,  -4.8418,  ..., -12.9624, -12.6625, -13.1166]],\n",
       "        device='cuda:0', grad_fn=<LogSoftmaxBackward>),\n",
       " tensor([[-11.2572,  -4.6706,  -4.1162,  ..., -10.7765, -10.8794, -11.0438]],\n",
       "        device='cuda:0', grad_fn=<LogSoftmaxBackward>),\n",
       " tensor([[-11.2965,  -2.8294,  -4.0526,  ..., -10.9839, -10.8868, -11.0569]],\n",
       "        device='cuda:0', grad_fn=<LogSoftmaxBackward>),\n",
       " tensor([[-12.7688,  -2.5519,  -4.5544,  ..., -12.4332, -12.2173, -12.3903]],\n",
       "        device='cuda:0', grad_fn=<LogSoftmaxBackward>),\n",
       " tensor([[-12.0278,  -2.4747,  -4.3442,  ..., -11.7072, -11.4160, -11.6495]],\n",
       "        device='cuda:0', grad_fn=<LogSoftmaxBackward>),\n",
       " tensor([[-11.2901,  -2.6878,  -4.1849,  ..., -10.9766, -10.8236, -11.0174]],\n",
       "        device='cuda:0', grad_fn=<LogSoftmaxBackward>),\n",
       " tensor([[-13.8633,  -0.9088,  -5.3685,  ..., -13.2854, -12.9860, -13.5694]],\n",
       "        device='cuda:0', grad_fn=<LogSoftmaxBackward>),\n",
       " tensor([[-14.9055,  -0.3842,  -5.9966,  ..., -14.4732, -13.9775, -14.3793]],\n",
       "        device='cuda:0', grad_fn=<LogSoftmaxBackward>),\n",
       " tensor([[-13.0703,  -4.5260,  -5.0460,  ..., -12.6204, -12.5905, -12.5408]],\n",
       "        device='cuda:0', grad_fn=<LogSoftmaxBackward>),\n",
       " tensor([[-11.3438,  -2.5573,  -4.3045,  ..., -11.0471, -10.8313, -11.1250]],\n",
       "        device='cuda:0', grad_fn=<LogSoftmaxBackward>),\n",
       " tensor([[-14.6150,  -0.3808,  -5.9902,  ..., -14.1146, -13.6351, -14.1622]],\n",
       "        device='cuda:0', grad_fn=<LogSoftmaxBackward>),\n",
       " tensor([[-12.7730,  -2.2672,  -4.8423,  ..., -12.1875, -12.0800, -12.1895]],\n",
       "        device='cuda:0', grad_fn=<LogSoftmaxBackward>),\n",
       " tensor([[-11.4147,  -2.2888,  -4.3326,  ..., -11.1162, -10.8495, -11.2172]],\n",
       "        device='cuda:0', grad_fn=<LogSoftmaxBackward>),\n",
       " tensor([[-11.4618,  -2.2003,  -4.3540,  ..., -11.0785, -10.8671, -11.2137]],\n",
       "        device='cuda:0', grad_fn=<LogSoftmaxBackward>),\n",
       " tensor([[-11.5562,  -2.0882,  -4.3746,  ..., -11.2154, -10.9390, -11.3443]],\n",
       "        device='cuda:0', grad_fn=<LogSoftmaxBackward>),\n",
       " tensor([[-11.5840,  -2.1081,  -4.3664,  ..., -11.2661, -10.9567, -11.3966]],\n",
       "        device='cuda:0', grad_fn=<LogSoftmaxBackward>),\n",
       " tensor([[-14.7303,  -0.5032,  -5.8235,  ..., -14.0521, -13.6309, -14.2971]],\n",
       "        device='cuda:0', grad_fn=<LogSoftmaxBackward>),\n",
       " tensor([[-15.4686,  -0.2001,  -6.4394,  ..., -14.9094, -14.4383, -14.9973]],\n",
       "        device='cuda:0', grad_fn=<LogSoftmaxBackward>),\n",
       " tensor([[-15.1574,  -0.2489,  -6.2598,  ..., -14.5495, -14.0450, -14.7544]],\n",
       "        device='cuda:0', grad_fn=<LogSoftmaxBackward>),\n",
       " tensor([[-11.8458,  -2.1662,  -4.3063,  ..., -11.5217, -11.1296, -11.7232]],\n",
       "        device='cuda:0', grad_fn=<LogSoftmaxBackward>),\n",
       " tensor([[-15.9935,  -0.1276,  -6.5563,  ..., -15.5107, -15.1142, -15.5427]],\n",
       "        device='cuda:0', grad_fn=<LogSoftmaxBackward>),\n",
       " tensor([[-11.9110,  -2.1263,  -4.2969,  ..., -11.5240, -11.1183, -11.7743]],\n",
       "        device='cuda:0', grad_fn=<LogSoftmaxBackward>),\n",
       " tensor([[-15.8114,  -0.1378,  -6.5212,  ..., -15.3892, -14.9156, -15.4109]],\n",
       "        device='cuda:0', grad_fn=<LogSoftmaxBackward>),\n",
       " tensor([[-11.8654,  -3.7965,  -4.4198,  ..., -11.3873, -11.1292, -11.7819]],\n",
       "        device='cuda:0', grad_fn=<LogSoftmaxBackward>),\n",
       " tensor([[-11.9000,  -2.4064,  -4.2964,  ..., -11.4823, -11.1249, -11.7787]],\n",
       "        device='cuda:0', grad_fn=<LogSoftmaxBackward>),\n",
       " tensor([[-14.1180,  -1.5448,  -4.7878,  ..., -13.8095, -13.2814, -13.6627]],\n",
       "        device='cuda:0', grad_fn=<LogSoftmaxBackward>),\n",
       " tensor([[-11.9886,  -4.2832,  -4.2865,  ..., -11.5627, -11.1571, -11.6811]],\n",
       "        device='cuda:0', grad_fn=<LogSoftmaxBackward>),\n",
       " tensor([[-11.9843,  -2.1452,  -4.3046,  ..., -11.6046, -11.1410, -11.7723]],\n",
       "        device='cuda:0', grad_fn=<LogSoftmaxBackward>),\n",
       " tensor([[-15.4015,  -0.2248,  -6.3447,  ..., -15.0495, -14.3287, -15.0834]],\n",
       "        device='cuda:0', grad_fn=<LogSoftmaxBackward>),\n",
       " tensor([[-15.4214,  -0.2147,  -6.4031,  ..., -15.0660, -14.3874, -15.1056]],\n",
       "        device='cuda:0', grad_fn=<LogSoftmaxBackward>),\n",
       " tensor([[-11.9548,  -2.2063,  -4.3307,  ..., -11.5993, -11.1272, -11.7412]],\n",
       "        device='cuda:0', grad_fn=<LogSoftmaxBackward>),\n",
       " tensor([[-14.9950,  -0.2516,  -6.1579,  ..., -14.7185, -13.9257, -14.7510]],\n",
       "        device='cuda:0', grad_fn=<LogSoftmaxBackward>),\n",
       " tensor([[-11.9432,  -2.1008,  -4.2836,  ..., -11.6168, -11.1274, -11.7414]],\n",
       "        device='cuda:0', grad_fn=<LogSoftmaxBackward>),\n",
       " tensor([[-14.4279,  -0.2930,  -5.8295,  ..., -14.2128, -13.3518, -14.2316]],\n",
       "        device='cuda:0', grad_fn=<LogSoftmaxBackward>),\n",
       " tensor([[-11.9285,  -3.2745,  -4.2706,  ..., -11.6659, -11.3588, -11.7237]],\n",
       "        device='cuda:0', grad_fn=<LogSoftmaxBackward>),\n",
       " tensor([[-14.3612,  -0.3604,  -5.5976,  ..., -14.0683, -13.1923, -14.1929]],\n",
       "        device='cuda:0', grad_fn=<LogSoftmaxBackward>),\n",
       " tensor([[-14.7450,  -0.2992,  -5.8533,  ..., -14.5826, -13.7163, -14.5581]],\n",
       "        device='cuda:0', grad_fn=<LogSoftmaxBackward>),\n",
       " tensor([[-11.8115,  -2.0645,  -4.2378,  ..., -11.5678, -11.0189, -11.6706]],\n",
       "        device='cuda:0', grad_fn=<LogSoftmaxBackward>),\n",
       " tensor([[-14.7965,  -0.3045,  -5.7743,  ..., -14.6057, -13.7922, -14.6489]],\n",
       "        device='cuda:0', grad_fn=<LogSoftmaxBackward>),\n",
       " tensor([[-11.8674,  -1.9698,  -4.1793,  ..., -11.6194, -11.0758, -11.7241]],\n",
       "        device='cuda:0', grad_fn=<LogSoftmaxBackward>),\n",
       " tensor([[-13.2945,  -2.6285,  -4.6164,  ..., -13.3242, -12.6063, -13.2366]],\n",
       "        device='cuda:0', grad_fn=<LogSoftmaxBackward>),\n",
       " tensor([[-13.7920,  -2.0901,  -4.5052,  ..., -13.5133, -12.8943, -13.6163]],\n",
       "        device='cuda:0', grad_fn=<LogSoftmaxBackward>),\n",
       " tensor([[-15.0979,  -0.2734,  -5.8146,  ..., -14.9103, -14.1277, -14.9841]],\n",
       "        device='cuda:0', grad_fn=<LogSoftmaxBackward>),\n",
       " tensor([[-13.1921,  -0.8638,  -4.7431,  ..., -12.8811, -12.1887, -13.0544]],\n",
       "        device='cuda:0', grad_fn=<LogSoftmaxBackward>),\n",
       " tensor([[-12.6857,  -1.7460,  -4.0721,  ..., -12.6658, -12.0621, -12.6538]],\n",
       "        device='cuda:0', grad_fn=<LogSoftmaxBackward>),\n",
       " tensor([[-15.3839,  -0.2564,  -5.8373,  ..., -15.2129, -14.4666, -15.2959]],\n",
       "        device='cuda:0', grad_fn=<LogSoftmaxBackward>),\n",
       " tensor([[-15.1663,  -0.2621,  -5.6937,  ..., -14.9843, -14.1989, -15.0324]],\n",
       "        device='cuda:0', grad_fn=<LogSoftmaxBackward>),\n",
       " tensor([[-11.5607,  -1.9992,  -4.1821,  ..., -11.3190, -10.9300, -11.5331]],\n",
       "        device='cuda:0', grad_fn=<LogSoftmaxBackward>),\n",
       " tensor([[-16.0783,  -0.1596,  -6.2356,  ..., -15.8237, -15.1140, -15.8747]],\n",
       "        device='cuda:0', grad_fn=<LogSoftmaxBackward>),\n",
       " tensor([[-13.2139,  -1.4318,  -4.1920,  ..., -13.0134, -12.6655, -12.8064]],\n",
       "        device='cuda:0', grad_fn=<LogSoftmaxBackward>),\n",
       " tensor([[-11.5084,  -2.0879,  -4.1618,  ..., -11.2403, -10.8235, -11.4845]],\n",
       "        device='cuda:0', grad_fn=<LogSoftmaxBackward>),\n",
       " tensor([[-11.5646,  -2.0693,  -4.2508,  ..., -11.3512, -10.8969, -11.5275]],\n",
       "        device='cuda:0', grad_fn=<LogSoftmaxBackward>),\n",
       " tensor([[-12.6375,  -1.6691,  -4.3698,  ..., -12.3627, -12.3539, -12.3781]],\n",
       "        device='cuda:0', grad_fn=<LogSoftmaxBackward>),\n",
       " tensor([[-11.8648,  -2.0423,  -4.0218,  ..., -11.6073, -11.1226, -11.7124]],\n",
       "        device='cuda:0', grad_fn=<LogSoftmaxBackward>),\n",
       " tensor([[-14.6433,  -1.5069,  -4.6358,  ..., -14.2681, -13.9533, -14.3585]],\n",
       "        device='cuda:0', grad_fn=<LogSoftmaxBackward>),\n",
       " tensor([[-15.9524,  -0.1903,  -6.0905,  ..., -15.6955, -15.0125, -15.7781]],\n",
       "        device='cuda:0', grad_fn=<LogSoftmaxBackward>),\n",
       " tensor([[-16.3632,  -0.1219,  -6.4619,  ..., -15.9922, -15.5066, -15.9999]],\n",
       "        device='cuda:0', grad_fn=<LogSoftmaxBackward>),\n",
       " tensor([[-16.0728,  -0.1511,  -6.4477,  ..., -15.6867, -15.1318, -15.8504]],\n",
       "        device='cuda:0', grad_fn=<LogSoftmaxBackward>),\n",
       " tensor([[-12.2863,  -1.9732,  -4.3092,  ..., -11.8903, -11.5266, -11.9777]],\n",
       "        device='cuda:0', grad_fn=<LogSoftmaxBackward>),\n",
       " tensor([[-12.4502,  -1.7958,  -4.2081,  ..., -12.0604, -11.6561, -12.0837]],\n",
       "        device='cuda:0', grad_fn=<LogSoftmaxBackward>),\n",
       " tensor([[-11.9287,  -2.9198,  -4.1756,  ..., -11.5979, -11.3263, -11.7454]],\n",
       "        device='cuda:0', grad_fn=<LogSoftmaxBackward>),\n",
       " tensor([[-16.3518,  -0.1783,  -6.5060,  ..., -16.0078, -15.4039, -16.0981]],\n",
       "        device='cuda:0', grad_fn=<LogSoftmaxBackward>),\n",
       " tensor([[-11.7264,  -1.6497,  -4.1033,  ..., -11.3428, -11.3383, -11.3240]],\n",
       "        device='cuda:0', grad_fn=<LogSoftmaxBackward>),\n",
       " tensor([[-13.8462,  -1.8637,  -4.5299,  ..., -13.4362, -13.2685, -13.2262]],\n",
       "        device='cuda:0', grad_fn=<LogSoftmaxBackward>),\n",
       " tensor([[-13.3853,  -1.1542,  -4.8745,  ..., -13.0969, -12.8479, -13.0067]],\n",
       "        device='cuda:0', grad_fn=<LogSoftmaxBackward>),\n",
       " tensor([[-12.7212,  -2.2885,  -4.2887,  ..., -12.4103, -11.9140, -12.2967]],\n",
       "        device='cuda:0', grad_fn=<LogSoftmaxBackward>),\n",
       " tensor([[-12.4260,  -1.8694,  -4.3115,  ..., -12.0275, -11.6384, -12.0586]],\n",
       "        device='cuda:0', grad_fn=<LogSoftmaxBackward>),\n",
       " tensor([[-12.4046,  -1.9396,  -4.2926,  ..., -12.0391, -11.6186, -12.0579]],\n",
       "        device='cuda:0', grad_fn=<LogSoftmaxBackward>),\n",
       " tensor([[-14.3886,  -1.9058,  -5.2294,  ..., -14.1576, -13.9453, -14.2281]],\n",
       "        device='cuda:0', grad_fn=<LogSoftmaxBackward>),\n",
       " tensor([[-12.5328,  -1.8805,  -4.3212,  ..., -12.1614, -11.7309, -12.1405]],\n",
       "        device='cuda:0', grad_fn=<LogSoftmaxBackward>),\n",
       " tensor([[-12.5531,  -1.8542,  -4.3667,  ..., -12.1554, -11.7432, -12.1508]],\n",
       "        device='cuda:0', grad_fn=<LogSoftmaxBackward>),\n",
       " tensor([[-14.1463,  -1.0800,  -5.3249,  ..., -13.6804, -13.5223, -13.5894]],\n",
       "        device='cuda:0', grad_fn=<LogSoftmaxBackward>),\n",
       " tensor([[-16.4722,  -0.4497,  -6.4968,  ..., -15.8468, -15.4673, -16.0733]],\n",
       "        device='cuda:0', grad_fn=<LogSoftmaxBackward>),\n",
       " tensor([[-16.5259,  -0.2666,  -6.7595,  ..., -16.0306, -15.6801, -16.2076]],\n",
       "        device='cuda:0', grad_fn=<LogSoftmaxBackward>),\n",
       " tensor([[-12.4049,  -1.9714,  -4.3451,  ..., -12.0494, -11.6361, -12.0423]],\n",
       "        device='cuda:0', grad_fn=<LogSoftmaxBackward>),\n",
       " tensor([[-11.9997,  -1.5433,  -4.6185,  ..., -11.5429, -11.3207, -11.4771]],\n",
       "        device='cuda:0', grad_fn=<LogSoftmaxBackward>),\n",
       " tensor([[-12.2363,  -2.1504,  -4.3325,  ..., -11.8887, -11.5052, -11.8709]],\n",
       "        device='cuda:0', grad_fn=<LogSoftmaxBackward>),\n",
       " tensor([[-16.5210,  -0.2648,  -6.7916,  ..., -15.9912, -15.6351, -16.0809]],\n",
       "        device='cuda:0', grad_fn=<LogSoftmaxBackward>),\n",
       " tensor([[-11.9695,  -2.3963,  -4.6367,  ..., -11.3392, -11.6046, -11.4390]],\n",
       "        device='cuda:0', grad_fn=<LogSoftmaxBackward>),\n",
       " tensor([[-12.3102,  -1.9827,  -4.3613,  ..., -11.8679, -11.5165, -11.8906]],\n",
       "        device='cuda:0', grad_fn=<LogSoftmaxBackward>),\n",
       " tensor([[-13.5377,  -2.0124,  -5.0048,  ..., -13.0846, -12.8484, -13.1118]],\n",
       "        device='cuda:0', grad_fn=<LogSoftmaxBackward>),\n",
       " tensor([[-12.2077,  -2.0479,  -4.2724,  ..., -11.8119, -11.4255, -11.8068]],\n",
       "        device='cuda:0', grad_fn=<LogSoftmaxBackward>),\n",
       " tensor([[-12.2834,  -2.0876,  -4.3367,  ..., -11.8346, -11.4800, -11.8117]],\n",
       "        device='cuda:0', grad_fn=<LogSoftmaxBackward>),\n",
       " tensor([[-12.3181,  -1.9563,  -4.4026,  ..., -11.7972, -11.5439, -11.7959]],\n",
       "        device='cuda:0', grad_fn=<LogSoftmaxBackward>),\n",
       " tensor([[-14.9787,  -2.2843,  -5.6156,  ..., -14.5240, -14.6120, -14.4943]],\n",
       "        device='cuda:0', grad_fn=<LogSoftmaxBackward>),\n",
       " tensor([[-13.1232,  -1.7628,  -4.9740,  ..., -12.5941, -12.7144, -12.6488]],\n",
       "        device='cuda:0', grad_fn=<LogSoftmaxBackward>),\n",
       " tensor([[-16.4382,  -0.3048,  -6.7221,  ..., -15.9550, -15.6136, -16.0751]],\n",
       "        device='cuda:0', grad_fn=<LogSoftmaxBackward>),\n",
       " tensor([[-16.1950,  -0.4659,  -6.5585,  ..., -15.6387, -15.4489, -15.7584]],\n",
       "        device='cuda:0', grad_fn=<LogSoftmaxBackward>),\n",
       " tensor([[-12.2108,  -2.0180,  -4.4064,  ..., -11.7130, -11.4292, -11.6848]],\n",
       "        device='cuda:0', grad_fn=<LogSoftmaxBackward>),\n",
       " tensor([[-10.9584,  -4.6519,  -4.8001,  ..., -10.4502, -10.3002, -10.5206]],\n",
       "        device='cuda:0', grad_fn=<LogSoftmaxBackward>),\n",
       " tensor([[-14.9387,  -1.7217,  -5.6728,  ..., -14.6713, -14.5848, -14.6703]],\n",
       "        device='cuda:0', grad_fn=<LogSoftmaxBackward>),\n",
       " tensor([[-11.7596,  -2.6290,  -4.5094,  ..., -11.3054, -11.7960, -11.4324]],\n",
       "        device='cuda:0', grad_fn=<LogSoftmaxBackward>),\n",
       " tensor([[-12.2048,  -4.3318,  -4.7022,  ..., -11.7002, -11.6266, -11.7693]],\n",
       "        device='cuda:0', grad_fn=<LogSoftmaxBackward>),\n",
       " tensor([[-12.2928,  -2.1066,  -4.4844,  ..., -11.7030, -11.4774, -11.7942]],\n",
       "        device='cuda:0', grad_fn=<LogSoftmaxBackward>),\n",
       " tensor([[-16.6194,  -0.3012,  -6.8618,  ..., -16.2198, -15.9486, -16.3866]],\n",
       "        device='cuda:0', grad_fn=<LogSoftmaxBackward>),\n",
       " tensor([[-12.1704,  -2.1786,  -4.4099,  ..., -11.6498, -11.3794, -11.7182]],\n",
       "        device='cuda:0', grad_fn=<LogSoftmaxBackward>),\n",
       " tensor([[-13.3042,  -1.6126,  -5.3396,  ..., -12.9122, -13.1719, -13.1513]],\n",
       "        device='cuda:0', grad_fn=<LogSoftmaxBackward>),\n",
       " tensor([[-11.8777,  -2.2964,  -4.2594,  ..., -11.5805, -11.2434, -11.3588]],\n",
       "        device='cuda:0', grad_fn=<LogSoftmaxBackward>),\n",
       " tensor([[-12.2787,  -4.5724,  -4.7893,  ..., -11.7634, -11.7409, -11.8602]],\n",
       "        device='cuda:0', grad_fn=<LogSoftmaxBackward>),\n",
       " tensor([[-12.2062,  -3.9406,  -4.7309,  ..., -11.6499, -11.5736, -11.7410]],\n",
       "        device='cuda:0', grad_fn=<LogSoftmaxBackward>),\n",
       " tensor([[-16.0570,  -0.3038,  -6.8279,  ..., -15.7576, -15.5191, -15.9602]],\n",
       "        device='cuda:0', grad_fn=<LogSoftmaxBackward>),\n",
       " tensor([[-12.4800,  -2.0237,  -4.5373,  ..., -11.8757, -11.6312, -11.9737]],\n",
       "        device='cuda:0', grad_fn=<LogSoftmaxBackward>),\n",
       " tensor([[-12.5169,  -2.0013,  -4.5507,  ..., -11.9229, -11.6894, -12.0163]],\n",
       "        device='cuda:0', grad_fn=<LogSoftmaxBackward>),\n",
       " tensor([[-12.4463,  -4.0547,  -4.7966,  ..., -11.8705, -11.7966, -11.9530]],\n",
       "        device='cuda:0', grad_fn=<LogSoftmaxBackward>),\n",
       " tensor([[-16.4494,  -0.2586,  -7.0328,  ..., -16.0560, -15.8633, -16.2245]],\n",
       "        device='cuda:0', grad_fn=<LogSoftmaxBackward>),\n",
       " tensor([[-12.5516,  -1.8933,  -4.5010,  ..., -11.9503, -11.7133, -12.0435]],\n",
       "        device='cuda:0', grad_fn=<LogSoftmaxBackward>),\n",
       " tensor([[-12.5568,  -1.9171,  -4.4839,  ..., -11.9571, -11.7176, -12.0082]],\n",
       "        device='cuda:0', grad_fn=<LogSoftmaxBackward>),\n",
       " tensor([[-16.5476,  -0.2102,  -7.0376,  ..., -16.1808, -16.0446, -16.2918]],\n",
       "        device='cuda:0', grad_fn=<LogSoftmaxBackward>),\n",
       " tensor([[-12.6107,  -1.8254,  -4.5007,  ..., -12.0191, -11.7785, -12.0934]],\n",
       "        device='cuda:0', grad_fn=<LogSoftmaxBackward>),\n",
       " tensor([[-14.3920,  -1.1459,  -5.6373,  ..., -14.0272, -14.0816, -14.2626]],\n",
       "        device='cuda:0', grad_fn=<LogSoftmaxBackward>),\n",
       " tensor([[-14.6089,  -1.1573,  -5.3947,  ..., -14.0811, -14.3862, -14.1575]],\n",
       "        device='cuda:0', grad_fn=<LogSoftmaxBackward>),\n",
       " tensor([[-14.8139,  -0.7039,  -5.7647,  ..., -14.4369, -14.4589, -14.6098]],\n",
       "        device='cuda:0', grad_fn=<LogSoftmaxBackward>),\n",
       " tensor([[-16.3782,  -0.1384,  -6.9727,  ..., -15.8608, -15.7538, -15.9279]],\n",
       "        device='cuda:0', grad_fn=<LogSoftmaxBackward>),\n",
       " tensor([[-14.4363,  -1.2123,  -5.5334,  ..., -14.0560, -14.1640, -14.0282]],\n",
       "        device='cuda:0', grad_fn=<LogSoftmaxBackward>),\n",
       " tensor([[-16.4162,  -0.1421,  -7.1280,  ..., -15.9452, -15.8684, -16.1866]],\n",
       "        device='cuda:0', grad_fn=<LogSoftmaxBackward>),\n",
       " tensor([[-12.2426,  -1.8593,  -4.5205,  ..., -11.6921, -11.5831, -11.7038]],\n",
       "        device='cuda:0', grad_fn=<LogSoftmaxBackward>),\n",
       " tensor([[-12.0069,  -1.7259,  -5.1462,  ..., -11.6338, -11.9683, -12.0392]],\n",
       "        device='cuda:0', grad_fn=<LogSoftmaxBackward>),\n",
       " tensor([[-13.7283,  -0.9025,  -5.4026,  ..., -13.2976, -13.4402, -13.3035]],\n",
       "        device='cuda:0', grad_fn=<LogSoftmaxBackward>),\n",
       " tensor([[-16.6351,  -0.1793,  -6.8632,  ..., -16.1758, -16.0440, -16.3340]],\n",
       "        device='cuda:0', grad_fn=<LogSoftmaxBackward>),\n",
       " tensor([[-12.0920,  -1.8329,  -4.4970,  ..., -11.5597, -11.4625, -11.5412]],\n",
       "        device='cuda:0', grad_fn=<LogSoftmaxBackward>),\n",
       " tensor([[-16.6759,  -0.1087,  -7.2236,  ..., -16.1836, -16.0187, -16.3215]],\n",
       "        device='cuda:0', grad_fn=<LogSoftmaxBackward>),\n",
       " tensor([[-12.0384,  -1.8722,  -4.4316,  ..., -11.5292, -11.3537, -11.4589]],\n",
       "        device='cuda:0', grad_fn=<LogSoftmaxBackward>),\n",
       " tensor([[-11.9676,  -1.1664,  -4.6707,  ..., -11.6488, -11.7724, -11.5843]],\n",
       "        device='cuda:0', grad_fn=<LogSoftmaxBackward>),\n",
       " tensor([[-13.3403,  -1.6987,  -4.5813,  ..., -12.6605, -13.1033, -12.7778]],\n",
       "        device='cuda:0', grad_fn=<LogSoftmaxBackward>),\n",
       " tensor([[-12.1200,  -1.9689,  -4.3747,  ..., -11.5175, -11.4438, -11.5255]],\n",
       "        device='cuda:0', grad_fn=<LogSoftmaxBackward>),\n",
       " tensor([[-15.5165,  -0.2927,  -6.1480,  ..., -14.6714, -14.5873, -14.8364]],\n",
       "        device='cuda:0', grad_fn=<LogSoftmaxBackward>),\n",
       " tensor([[-12.1831,  -1.2506,  -5.0051,  ..., -11.8138, -12.0149, -12.0123]],\n",
       "        device='cuda:0', grad_fn=<LogSoftmaxBackward>),\n",
       " tensor([[-16.1582,  -0.1352,  -6.8661,  ..., -15.5452, -15.4558, -15.7576]],\n",
       "        device='cuda:0', grad_fn=<LogSoftmaxBackward>),\n",
       " tensor([[-12.2555,  -2.0363,  -4.4807,  ..., -11.6898, -11.5924, -11.6833]],\n",
       "        device='cuda:0', grad_fn=<LogSoftmaxBackward>),\n",
       " tensor([[-12.1526,  -2.5364,  -4.4992,  ..., -11.5449, -11.4735, -11.5416]],\n",
       "        device='cuda:0', grad_fn=<LogSoftmaxBackward>),\n",
       " tensor([[-11.8620,  -1.3861,  -4.4675,  ..., -11.2615, -11.4578, -11.6491]],\n",
       "        device='cuda:0', grad_fn=<LogSoftmaxBackward>),\n",
       " tensor([[-15.2776,  -1.4150,  -5.1721,  ..., -14.6800, -14.8945, -14.7927]],\n",
       "        device='cuda:0', grad_fn=<LogSoftmaxBackward>),\n",
       " tensor([[-12.2333,  -2.6765,  -4.4378,  ..., -11.6163, -11.5636, -11.6713]],\n",
       "        device='cuda:0', grad_fn=<LogSoftmaxBackward>),\n",
       " tensor([[-12.3894,  -2.1941,  -4.4112,  ..., -11.7809, -11.6474, -11.8260]],\n",
       "        device='cuda:0', grad_fn=<LogSoftmaxBackward>),\n",
       " tensor([[-16.0590,  -0.2084,  -6.4862,  ..., -15.5353, -15.4232, -15.7584]],\n",
       "        device='cuda:0', grad_fn=<LogSoftmaxBackward>),\n",
       " tensor([[-16.4359,  -0.2132,  -6.6273,  ..., -15.8484, -15.7516, -16.1223]],\n",
       "        device='cuda:0', grad_fn=<LogSoftmaxBackward>),\n",
       " tensor([[-12.4173,  -2.1789,  -4.2431,  ..., -11.7590, -11.6555, -11.8945]],\n",
       "        device='cuda:0', grad_fn=<LogSoftmaxBackward>),\n",
       " tensor([[-12.9481,  -0.9846,  -4.7892,  ..., -12.6407, -12.6455, -12.8074]],\n",
       "        device='cuda:0', grad_fn=<LogSoftmaxBackward>),\n",
       " tensor([[-12.4342,  -2.2184,  -4.0642,  ..., -11.7211, -11.6917, -11.9320]],\n",
       "        device='cuda:0', grad_fn=<LogSoftmaxBackward>),\n",
       " tensor([[-12.4056,  -2.1230,  -3.9285,  ..., -11.7642, -11.6699, -11.8616]],\n",
       "        device='cuda:0', grad_fn=<LogSoftmaxBackward>),\n",
       " tensor([[-12.7379,  -0.9789,  -4.8388,  ..., -12.4238, -12.4908, -12.5387]],\n",
       "        device='cuda:0', grad_fn=<LogSoftmaxBackward>),\n",
       " tensor([[-16.6573,  -0.2653,  -6.2282,  ..., -16.0378, -15.9727, -16.3778]],\n",
       "        device='cuda:0', grad_fn=<LogSoftmaxBackward>),\n",
       " tensor([[-12.6169,  -2.0129,  -3.8911,  ..., -11.9310, -11.9286, -12.1788]],\n",
       "        device='cuda:0', grad_fn=<LogSoftmaxBackward>),\n",
       " tensor([[-12.3081,  -2.2749,  -3.7487,  ..., -11.6808, -11.5873, -11.8299]],\n",
       "        device='cuda:0', grad_fn=<LogSoftmaxBackward>),\n",
       " tensor([[-12.4374,  -2.2027,  -3.7869,  ..., -11.7762, -11.7774, -12.0613]],\n",
       "        device='cuda:0', grad_fn=<LogSoftmaxBackward>),\n",
       " tensor([[-12.4369,  -2.1236,  -3.8370,  ..., -11.7459, -11.7628, -12.0584]],\n",
       "        device='cuda:0', grad_fn=<LogSoftmaxBackward>),\n",
       " tensor([[-12.4316,  -2.1683,  -3.8426,  ..., -11.7271, -11.7240, -12.0112]],\n",
       "        device='cuda:0', grad_fn=<LogSoftmaxBackward>),\n",
       " tensor([[-16.2118,  -0.4732,  -5.7114,  ..., -15.5053, -15.5305, -15.8907]],\n",
       "        device='cuda:0', grad_fn=<LogSoftmaxBackward>),\n",
       " tensor([[-12.3634,  -2.0581,  -3.8016,  ..., -11.6864, -11.7470, -12.0027]],\n",
       "        device='cuda:0', grad_fn=<LogSoftmaxBackward>),\n",
       " tensor([[-12.2953,  -1.9809,  -3.7989,  ..., -11.6475, -11.6925, -11.9595]],\n",
       "        device='cuda:0', grad_fn=<LogSoftmaxBackward>),\n",
       " tensor([[-14.4392,  -2.0351,  -4.2918,  ..., -13.6352, -13.8847, -13.9056]],\n",
       "        device='cuda:0', grad_fn=<LogSoftmaxBackward>),\n",
       " tensor([[-16.7213,  -0.2430,  -6.1361,  ..., -16.1361, -16.0499, -16.4305]],\n",
       "        device='cuda:0', grad_fn=<LogSoftmaxBackward>),\n",
       " tensor([[-16.6508,  -0.2388,  -6.0676,  ..., -15.9732, -15.9911, -16.2954]],\n",
       "        device='cuda:0', grad_fn=<LogSoftmaxBackward>),\n",
       " tensor([[-16.6353,  -0.2340,  -6.1741,  ..., -16.0324, -16.0082, -16.3366]],\n",
       "        device='cuda:0', grad_fn=<LogSoftmaxBackward>),\n",
       " tensor([[-11.5104,  -2.1688,  -3.6227,  ..., -11.0267, -11.2426, -11.0970]],\n",
       "        device='cuda:0', grad_fn=<LogSoftmaxBackward>),\n",
       " tensor([[-11.9060,  -2.0703,  -3.8989,  ..., -11.2520, -11.3036, -11.6109]],\n",
       "        device='cuda:0', grad_fn=<LogSoftmaxBackward>),\n",
       " tensor([[-11.0768,  -2.5145,  -3.8997,  ..., -10.7655, -10.7165, -10.9172]],\n",
       "        device='cuda:0', grad_fn=<LogSoftmaxBackward>),\n",
       " tensor([[-16.5455,  -0.2159,  -6.2159,  ..., -16.0004, -15.9149, -16.3154]],\n",
       "        device='cuda:0', grad_fn=<LogSoftmaxBackward>),\n",
       " tensor([[-13.8950,  -1.0578,  -4.6368,  ..., -13.4550, -13.5058, -13.5138]],\n",
       "        device='cuda:0', grad_fn=<LogSoftmaxBackward>),\n",
       " tensor([[-11.7273,  -2.0744,  -3.9707,  ..., -11.1597, -11.2096, -11.4664]],\n",
       "        device='cuda:0', grad_fn=<LogSoftmaxBackward>),\n",
       " tensor([[-15.0974,  -0.4035,  -5.4713,  ..., -14.2608, -14.1912, -14.7457]],\n",
       "        device='cuda:0', grad_fn=<LogSoftmaxBackward>),\n",
       " tensor([[-13.2213,  -1.7585,  -4.8296,  ..., -12.7949, -12.9871, -12.9824]],\n",
       "        device='cuda:0', grad_fn=<LogSoftmaxBackward>),\n",
       " tensor([[-11.7810,  -2.9502,  -3.9341,  ..., -11.2109, -11.3491, -11.5342]],\n",
       "        device='cuda:0', grad_fn=<LogSoftmaxBackward>),\n",
       " tensor([[-11.7577,  -2.5495,  -3.9849,  ..., -11.2109, -11.2815, -11.5018]],\n",
       "        device='cuda:0', grad_fn=<LogSoftmaxBackward>),\n",
       " tensor([[-11.8238,  -2.0192,  -3.9727,  ..., -11.2803, -11.3767, -11.5601]],\n",
       "        device='cuda:0', grad_fn=<LogSoftmaxBackward>),\n",
       " tensor([[-11.9212,  -2.1606,  -3.9538,  ..., -11.3953, -11.4517, -11.6558]],\n",
       "        device='cuda:0', grad_fn=<LogSoftmaxBackward>),\n",
       " tensor([[-12.9336,  -1.7286,  -4.2331,  ..., -12.4780, -12.4769, -12.4476]],\n",
       "        device='cuda:0', grad_fn=<LogSoftmaxBackward>),\n",
       " tensor([[-12.1382,  -1.9640,  -3.8898,  ..., -11.5509, -11.6047, -11.7866]],\n",
       "        device='cuda:0', grad_fn=<LogSoftmaxBackward>),\n",
       " tensor([[-12.2260,  -1.9978,  -3.8784,  ..., -11.6752, -11.7258, -11.9256]],\n",
       "        device='cuda:0', grad_fn=<LogSoftmaxBackward>),\n",
       " tensor([[-16.8582,  -0.2258,  -6.2254,  ..., -16.3015, -16.2661, -16.6212]],\n",
       "        device='cuda:0', grad_fn=<LogSoftmaxBackward>),\n",
       " tensor([[-12.4338,  -1.9019,  -3.7880,  ..., -11.8742, -11.8686, -12.0962]],\n",
       "        device='cuda:0', grad_fn=<LogSoftmaxBackward>),\n",
       " tensor([[-14.3142,  -1.8427,  -5.0978,  ..., -13.9394, -14.1416, -14.0146]],\n",
       "        device='cuda:0', grad_fn=<LogSoftmaxBackward>),\n",
       " tensor([[-15.4707,  -4.3724,  -5.2510,  ..., -14.9941, -15.3397, -15.1596]],\n",
       "        device='cuda:0', grad_fn=<LogSoftmaxBackward>),\n",
       " tensor([[-14.5274,  -1.1684,  -4.9200,  ..., -14.1051, -14.2338, -14.1263]],\n",
       "        device='cuda:0', grad_fn=<LogSoftmaxBackward>),\n",
       " tensor([[-16.9606,  -0.4038,  -5.9042,  ..., -16.3818, -16.4237, -16.7450]],\n",
       "        device='cuda:0', grad_fn=<LogSoftmaxBackward>),\n",
       " tensor([[-17.2425,  -0.2140,  -6.3489,  ..., -16.6766, -16.6103, -16.9678]],\n",
       "        device='cuda:0', grad_fn=<LogSoftmaxBackward>),\n",
       " tensor([[-14.7217,  -0.8765,  -4.9787,  ..., -14.2819, -14.2907, -14.3860]],\n",
       "        device='cuda:0', grad_fn=<LogSoftmaxBackward>),\n",
       " tensor([[-12.3814,  -2.1597,  -3.7419,  ..., -11.8114, -11.8433, -11.9889]],\n",
       "        device='cuda:0', grad_fn=<LogSoftmaxBackward>),\n",
       " tensor([[-12.4848,  -1.9946,  -3.6944,  ..., -11.9301, -11.9505, -12.1255]],\n",
       "        device='cuda:0', grad_fn=<LogSoftmaxBackward>),\n",
       " tensor([[-11.3287,  -1.6630,  -3.6919,  ..., -10.8616, -10.7807, -11.2478]],\n",
       "        device='cuda:0', grad_fn=<LogSoftmaxBackward>),\n",
       " tensor([[-12.2857,  -1.9975,  -3.7212,  ..., -11.7409, -11.7817, -11.9309]],\n",
       "        device='cuda:0', grad_fn=<LogSoftmaxBackward>),\n",
       " tensor([[-14.2970,  -1.6752,  -4.5154,  ..., -13.6203, -13.9094, -13.6907]],\n",
       "        device='cuda:0', grad_fn=<LogSoftmaxBackward>),\n",
       " tensor([[-12.3344,  -1.9897,  -3.6475,  ..., -11.7982, -11.8286, -12.0018]],\n",
       "        device='cuda:0', grad_fn=<LogSoftmaxBackward>),\n",
       " tensor([[-12.4174,  -2.0216,  -3.7364,  ..., -11.8744, -11.9132, -12.0707]],\n",
       "        device='cuda:0', grad_fn=<LogSoftmaxBackward>),\n",
       " tensor([[-14.6351,  -1.0526,  -4.6744,  ..., -14.0822, -14.2596, -14.2033]],\n",
       "        device='cuda:0', grad_fn=<LogSoftmaxBackward>),\n",
       " tensor([[-12.4753,  -1.9779,  -3.6955,  ..., -11.9311, -11.9574, -12.1309]],\n",
       "        device='cuda:0', grad_fn=<LogSoftmaxBackward>),\n",
       " tensor([[-12.4568,  -1.9248,  -3.6481,  ..., -11.9246, -11.9287, -12.0587]],\n",
       "        device='cuda:0', grad_fn=<LogSoftmaxBackward>),\n",
       " tensor([[-12.5274,  -1.9932,  -3.6932,  ..., -11.9443, -11.9935, -12.1455]],\n",
       "        device='cuda:0', grad_fn=<LogSoftmaxBackward>),\n",
       " tensor([[-12.3548,  -1.3947,  -4.1202,  ..., -11.8620, -12.0042, -12.0286]],\n",
       "        device='cuda:0', grad_fn=<LogSoftmaxBackward>),\n",
       " tensor([[-12.5460,  -2.0862,  -3.7093,  ..., -11.9870, -12.0090, -12.1915]],\n",
       "        device='cuda:0', grad_fn=<LogSoftmaxBackward>),\n",
       " tensor([[-17.0714,  -0.1169,  -6.6043,  ..., -16.5358, -16.4549, -16.8744]],\n",
       "        device='cuda:0', grad_fn=<LogSoftmaxBackward>),\n",
       " tensor([[-12.5833,  -2.0900,  -3.7020,  ..., -11.9874, -12.0456, -12.1998]],\n",
       "        device='cuda:0', grad_fn=<LogSoftmaxBackward>),\n",
       " tensor([[-12.5292,  -2.0537,  -3.6968,  ..., -11.9871, -12.0001, -12.1853]],\n",
       "        device='cuda:0', grad_fn=<LogSoftmaxBackward>),\n",
       " tensor([[-16.9409,  -0.1158,  -6.8786,  ..., -16.4431, -16.4286, -16.7209]],\n",
       "        device='cuda:0', grad_fn=<LogSoftmaxBackward>),\n",
       " tensor([[-12.6415,  -2.1092,  -3.7763,  ..., -12.0516, -12.0980, -12.2755]],\n",
       "        device='cuda:0', grad_fn=<LogSoftmaxBackward>),\n",
       " tensor([[-12.5800,  -2.0083,  -3.7159,  ..., -12.0245, -12.0258, -12.1730]],\n",
       "        device='cuda:0', grad_fn=<LogSoftmaxBackward>),\n",
       " tensor([[-12.5195,  -2.1491,  -3.8410,  ..., -11.9299, -11.9704, -12.1433]],\n",
       "        device='cuda:0', grad_fn=<LogSoftmaxBackward>),\n",
       " tensor([[-14.6074,  -0.6719,  -5.1422,  ..., -14.3028, -14.3532, -14.3327]],\n",
       "        device='cuda:0', grad_fn=<LogSoftmaxBackward>),\n",
       " tensor([[-12.4461,  -5.1914,  -4.3806,  ..., -11.9071, -12.0814, -12.1625]],\n",
       "        device='cuda:0', grad_fn=<LogSoftmaxBackward>),\n",
       " tensor([[-12.5248,  -2.0409,  -3.7838,  ..., -11.9636, -11.9473, -12.0909]],\n",
       "        device='cuda:0', grad_fn=<LogSoftmaxBackward>),\n",
       " tensor([[-12.4877,  -2.0074,  -3.8796,  ..., -11.9573, -11.9570, -12.1188]],\n",
       "        device='cuda:0', grad_fn=<LogSoftmaxBackward>),\n",
       " tensor([[-12.4152,  -2.0240,  -3.8961,  ..., -11.9119, -11.9011, -12.0677]],\n",
       "        device='cuda:0', grad_fn=<LogSoftmaxBackward>),\n",
       " tensor([[-12.1150,  -0.9693,  -4.7536,  ..., -11.6582, -11.8132, -11.8429]],\n",
       "        device='cuda:0', grad_fn=<LogSoftmaxBackward>),\n",
       " tensor([[-16.5013,  -0.2371,  -6.4351,  ..., -16.0431, -15.9771, -16.2846]],\n",
       "        device='cuda:0', grad_fn=<LogSoftmaxBackward>),\n",
       " tensor([[-12.3609,  -4.6499,  -4.3482,  ..., -11.8441, -12.0486, -12.0231]],\n",
       "        device='cuda:0', grad_fn=<LogSoftmaxBackward>),\n",
       " tensor([[-12.3373,  -1.9843,  -3.9276,  ..., -11.8005, -11.8246, -11.9635]],\n",
       "        device='cuda:0', grad_fn=<LogSoftmaxBackward>),\n",
       " tensor([[-12.3228,  -2.0491,  -3.9769,  ..., -11.8259, -11.8036, -11.9825]],\n",
       "        device='cuda:0', grad_fn=<LogSoftmaxBackward>),\n",
       " tensor([[-17.7392,  -0.0886,  -7.3086,  ..., -17.2340, -17.1497, -17.4276]],\n",
       "        device='cuda:0', grad_fn=<LogSoftmaxBackward>),\n",
       " tensor([[-14.1699,  -0.5982,  -5.6619,  ..., -13.8347, -13.9059, -14.0378]],\n",
       "        device='cuda:0', grad_fn=<LogSoftmaxBackward>),\n",
       " tensor([[-12.2169,  -1.9441,  -3.9988,  ..., -11.7183, -11.7163, -11.8397]],\n",
       "        device='cuda:0', grad_fn=<LogSoftmaxBackward>),\n",
       " tensor([[-17.4212,  -0.0891,  -7.1896,  ..., -16.9134, -16.8469, -17.1646]],\n",
       "        device='cuda:0', grad_fn=<LogSoftmaxBackward>),\n",
       " tensor([[-12.1645,  -1.9605,  -4.0074,  ..., -11.6799, -11.6879, -11.7907]],\n",
       "        device='cuda:0', grad_fn=<LogSoftmaxBackward>),\n",
       " tensor([[-15.0104,  -0.9456,  -4.9869,  ..., -14.3476, -14.5488, -14.4101]],\n",
       "        device='cuda:0', grad_fn=<LogSoftmaxBackward>),\n",
       " tensor([[-12.1737,  -1.9607,  -4.0047,  ..., -11.7567, -11.7161, -11.8605]],\n",
       "        device='cuda:0', grad_fn=<LogSoftmaxBackward>),\n",
       " tensor([[-12.0700,  -1.9977,  -4.0232,  ..., -11.6575, -11.6375, -11.7650]],\n",
       "        device='cuda:0', grad_fn=<LogSoftmaxBackward>),\n",
       " tensor([[-16.7622,  -0.1496,  -6.6285,  ..., -16.1922, -16.1676, -16.4899]],\n",
       "        device='cuda:0', grad_fn=<LogSoftmaxBackward>),\n",
       " tensor([[-12.0962,  -2.0146,  -3.9961,  ..., -11.6525, -11.6369, -11.7769]],\n",
       "        device='cuda:0', grad_fn=<LogSoftmaxBackward>),\n",
       " tensor([[-12.0546,  -2.0604,  -3.9894,  ..., -11.5688, -11.5959, -11.7119]],\n",
       "        device='cuda:0', grad_fn=<LogSoftmaxBackward>),\n",
       " tensor([[-12.6602,  -1.4481,  -4.8334,  ..., -12.2896, -12.1348, -12.2774]],\n",
       "        device='cuda:0', grad_fn=<LogSoftmaxBackward>),\n",
       " tensor([[-12.0887,  -2.0709,  -4.0450,  ..., -11.6580, -11.6639, -11.7646]],\n",
       "        device='cuda:0', grad_fn=<LogSoftmaxBackward>),\n",
       " tensor([[-12.2167,  -2.0397,  -4.0285,  ..., -11.7520, -11.7726, -11.8620]],\n",
       "        device='cuda:0', grad_fn=<LogSoftmaxBackward>),\n",
       " tensor([[-11.9345,  -3.1152,  -4.2618,  ..., -11.5127, -11.5614, -11.6581]],\n",
       "        device='cuda:0', grad_fn=<LogSoftmaxBackward>),\n",
       " tensor([[-14.4991,  -0.7356,  -5.5185,  ..., -13.9357, -14.0952, -14.2684]],\n",
       "        device='cuda:0', grad_fn=<LogSoftmaxBackward>),\n",
       " tensor([[-16.3212,  -1.2291,  -6.3015,  ..., -15.8453, -15.7742, -16.1413]],\n",
       "        device='cuda:0', grad_fn=<LogSoftmaxBackward>),\n",
       " tensor([[-11.7396,  -3.6297,  -4.2282,  ..., -11.3107, -11.4676, -11.3956]],\n",
       "        device='cuda:0', grad_fn=<LogSoftmaxBackward>),\n",
       " tensor([[-13.0812,  -2.0375,  -4.8989,  ..., -12.7791, -12.7415, -12.6818]],\n",
       "        device='cuda:0', grad_fn=<LogSoftmaxBackward>),\n",
       " tensor([[-16.6322,  -0.1511,  -6.4715,  ..., -15.8462, -15.8645, -16.2265]],\n",
       "        device='cuda:0', grad_fn=<LogSoftmaxBackward>),\n",
       " tensor([[-11.7658,  -2.0490,  -4.0962,  ..., -11.3618, -11.3232, -11.4372]],\n",
       "        device='cuda:0', grad_fn=<LogSoftmaxBackward>),\n",
       " tensor([[-13.4370,  -2.0390,  -4.8943,  ..., -12.9872, -12.8787, -13.1007]],\n",
       "        device='cuda:0', grad_fn=<LogSoftmaxBackward>),\n",
       " tensor([[-13.7797,  -2.0912,  -5.1247,  ..., -13.4829, -13.4498, -13.3571]],\n",
       "        device='cuda:0', grad_fn=<LogSoftmaxBackward>),\n",
       " tensor([[-11.5399,  -2.1645,  -4.0958,  ..., -11.1569, -11.1328, -11.2152]],\n",
       "        device='cuda:0', grad_fn=<LogSoftmaxBackward>),\n",
       " tensor([[-17.5462,  -0.1385,  -6.8847,  ..., -16.8540, -16.8212, -17.1460]],\n",
       "        device='cuda:0', grad_fn=<LogSoftmaxBackward>),\n",
       " tensor([[-12.0915,  -2.2672,  -4.2062,  ..., -11.2533, -11.7255, -11.6275]],\n",
       "        device='cuda:0', grad_fn=<LogSoftmaxBackward>),\n",
       " tensor([[-11.8014,  -2.0123,  -3.9945,  ..., -11.3858, -11.3610, -11.4882]],\n",
       "        device='cuda:0', grad_fn=<LogSoftmaxBackward>),\n",
       " tensor([[-15.9267,  -1.6563,  -5.1873,  ..., -15.3753, -15.4186, -15.4017]],\n",
       "        device='cuda:0', grad_fn=<LogSoftmaxBackward>),\n",
       " tensor([[-11.8412,  -2.0503,  -3.8629,  ..., -11.4094, -11.4008, -11.4927]],\n",
       "        device='cuda:0', grad_fn=<LogSoftmaxBackward>),\n",
       " tensor([[-15.6809,  -1.4019,  -5.3405,  ..., -15.1793, -15.3289, -15.4213]],\n",
       "        device='cuda:0', grad_fn=<LogSoftmaxBackward>),\n",
       " tensor([[-17.4161,  -0.2858,  -6.0674,  ..., -16.7523, -16.6935, -17.1698]],\n",
       "        device='cuda:0', grad_fn=<LogSoftmaxBackward>),\n",
       " tensor([[-17.0106,  -0.2939,  -5.9104,  ..., -16.3132, -16.3082, -16.6588]],\n",
       "        device='cuda:0', grad_fn=<LogSoftmaxBackward>),\n",
       " tensor([[-17.5220,  -0.3457,  -6.0361,  ..., -16.8425, -16.8537, -17.2702]],\n",
       "        device='cuda:0', grad_fn=<LogSoftmaxBackward>),\n",
       " tensor([[-15.8063,  -0.4179,  -5.1856,  ..., -14.9718, -14.8900, -15.4815]],\n",
       "        device='cuda:0', grad_fn=<LogSoftmaxBackward>),\n",
       " tensor([[-16.1778,  -0.4311,  -5.2684,  ..., -15.2981, -15.2877, -15.7713]],\n",
       "        device='cuda:0', grad_fn=<LogSoftmaxBackward>),\n",
       " tensor([[-15.9384,  -0.4114,  -5.1050,  ..., -15.1273, -15.0845, -15.5828]],\n",
       "        device='cuda:0', grad_fn=<LogSoftmaxBackward>),\n",
       " tensor([[-16.1944,  -0.3976,  -5.2719,  ..., -15.4033, -15.3255, -15.8421]],\n",
       "        device='cuda:0', grad_fn=<LogSoftmaxBackward>),\n",
       " tensor([[-15.7297,  -1.4796,  -4.7918,  ..., -15.2930, -15.3811, -15.3135]],\n",
       "        device='cuda:0', grad_fn=<LogSoftmaxBackward>),\n",
       " tensor([[-16.3035,  -0.5055,  -5.0657,  ..., -15.4933, -15.3825, -15.8800]],\n",
       "        device='cuda:0', grad_fn=<LogSoftmaxBackward>),\n",
       " tensor([[-11.8262,  -3.3278,  -3.3724,  ..., -11.4537, -11.4741, -11.5891]],\n",
       "        device='cuda:0', grad_fn=<LogSoftmaxBackward>),\n",
       " tensor([[-16.8665,  -2.7330,  -5.3283,  ..., -16.3477, -16.4770, -16.4130]],\n",
       "        device='cuda:0', grad_fn=<LogSoftmaxBackward>),\n",
       " tensor([[-16.5331,  -1.0288,  -4.8267,  ..., -15.9462, -16.0509, -16.1072]],\n",
       "        device='cuda:0', grad_fn=<LogSoftmaxBackward>),\n",
       " tensor([[-12.2705,  -2.1348,  -3.4212,  ..., -11.6956, -11.7393, -11.8973]],\n",
       "        device='cuda:0', grad_fn=<LogSoftmaxBackward>),\n",
       " tensor([[-15.4248,  -1.9235,  -4.6742,  ..., -14.5162, -15.0134, -14.9859]],\n",
       "        device='cuda:0', grad_fn=<LogSoftmaxBackward>),\n",
       " tensor([[-18.1291,  -0.3195,  -6.1042,  ..., -17.3867, -17.3261, -17.7349]],\n",
       "        device='cuda:0', grad_fn=<LogSoftmaxBackward>),\n",
       " tensor([[-12.4243,  -2.0997,  -3.3863,  ..., -11.8180, -11.8494, -12.0296]],\n",
       "        device='cuda:0', grad_fn=<LogSoftmaxBackward>),\n",
       " tensor([[-12.3192,  -2.0649,  -3.4233,  ..., -11.7309, -11.7628, -11.9163]],\n",
       "        device='cuda:0', grad_fn=<LogSoftmaxBackward>),\n",
       " tensor([[-18.5347,  -0.2534,  -6.1787,  ..., -17.8974, -17.8292, -18.2805]],\n",
       "        device='cuda:0', grad_fn=<LogSoftmaxBackward>),\n",
       " tensor([[-18.2910,  -0.2300,  -6.3342,  ..., -17.5313, -17.6091, -18.0471]],\n",
       "        device='cuda:0', grad_fn=<LogSoftmaxBackward>),\n",
       " tensor([[-17.9158,  -0.4415,  -5.6383,  ..., -17.2527, -17.2160, -17.5945]],\n",
       "        device='cuda:0', grad_fn=<LogSoftmaxBackward>),\n",
       " tensor([[-12.5158,  -1.8569,  -3.3051,  ..., -12.0006, -12.2373, -12.1462]],\n",
       "        device='cuda:0', grad_fn=<LogSoftmaxBackward>),\n",
       " tensor([[-17.9398,  -0.1524,  -6.1415,  ..., -17.1640, -17.2495, -17.7424]],\n",
       "        device='cuda:0', grad_fn=<LogSoftmaxBackward>),\n",
       " tensor([[-11.9592,  -5.8471,  -3.9959,  ..., -11.2707, -11.5380, -11.7488]],\n",
       "        device='cuda:0', grad_fn=<LogSoftmaxBackward>),\n",
       " tensor([[-16.5730,  -0.6819,  -5.4647,  ..., -15.7556, -16.0555, -16.1448]],\n",
       "        device='cuda:0', grad_fn=<LogSoftmaxBackward>),\n",
       " tensor([[-11.8031,  -2.2597,  -3.4519,  ..., -11.1310, -11.4069, -11.5047]],\n",
       "        device='cuda:0', grad_fn=<LogSoftmaxBackward>),\n",
       " tensor([[-18.0247,  -0.1392,  -6.4328,  ..., -17.2105, -17.3315, -17.8036]],\n",
       "        device='cuda:0', grad_fn=<LogSoftmaxBackward>),\n",
       " tensor([[-11.8329,  -2.1043,  -3.5212,  ..., -11.1655, -11.3884, -11.5203]],\n",
       "        device='cuda:0', grad_fn=<LogSoftmaxBackward>),\n",
       " tensor([[-11.9184,  -2.1301,  -3.4968,  ..., -11.2664, -11.4948, -11.6118]],\n",
       "        device='cuda:0', grad_fn=<LogSoftmaxBackward>),\n",
       " tensor([[-12.8520,  -1.7561,  -3.6763,  ..., -11.9953, -12.5696, -12.5344]],\n",
       "        device='cuda:0', grad_fn=<LogSoftmaxBackward>),\n",
       " tensor([[-12.0188,  -2.2195,  -3.5912,  ..., -11.4062, -11.5693, -11.6820]],\n",
       "        device='cuda:0', grad_fn=<LogSoftmaxBackward>),\n",
       " tensor([[-18.2981,  -0.1509,  -6.3258,  ..., -17.4844, -17.6015, -18.0454]],\n",
       "        device='cuda:0', grad_fn=<LogSoftmaxBackward>),\n",
       " tensor([[-16.8351,  -0.9145,  -5.2353,  ..., -16.0030, -16.1644, -16.4265]],\n",
       "        device='cuda:0', grad_fn=<LogSoftmaxBackward>),\n",
       " tensor([[-15.4913,  -1.7992,  -4.8747,  ..., -14.6063, -15.1317, -15.0668]],\n",
       "        device='cuda:0', grad_fn=<LogSoftmaxBackward>),\n",
       " tensor([[-13.6559,  -1.0311,  -5.0680,  ..., -13.1578, -13.3957, -13.3380]],\n",
       "        device='cuda:0', grad_fn=<LogSoftmaxBackward>),\n",
       " tensor([[-17.1580,  -0.2159,  -5.8508,  ..., -16.2894, -16.4131, -16.7885]],\n",
       "        device='cuda:0', grad_fn=<LogSoftmaxBackward>),\n",
       " tensor([[-18.5370,  -0.2032,  -6.5269,  ..., -17.8514, -17.8171, -18.3203]],\n",
       "        device='cuda:0', grad_fn=<LogSoftmaxBackward>),\n",
       " tensor([[-12.3995,  -2.1327,  -3.7220,  ..., -11.7798, -11.8964, -11.9925]],\n",
       "        device='cuda:0', grad_fn=<LogSoftmaxBackward>),\n",
       " tensor([[-12.4913,  -2.0468,  -3.6691,  ..., -11.8565, -11.9932, -12.0770]],\n",
       "        device='cuda:0', grad_fn=<LogSoftmaxBackward>),\n",
       " tensor([[-18.6781,  -0.1620,  -6.5774,  ..., -17.8725, -17.9389, -18.3734]],\n",
       "        device='cuda:0', grad_fn=<LogSoftmaxBackward>),\n",
       " tensor([[-12.5899,  -2.1588,  -3.6145,  ..., -11.9270, -12.0847, -12.1679]],\n",
       "        device='cuda:0', grad_fn=<LogSoftmaxBackward>),\n",
       " tensor([[-12.6770,  -2.0177,  -3.6419,  ..., -12.0549, -12.1668, -12.2645]],\n",
       "        device='cuda:0', grad_fn=<LogSoftmaxBackward>),\n",
       " tensor([[-12.5635,  -3.9519,  -3.7871,  ..., -11.8841, -12.2122, -12.2109]],\n",
       "        device='cuda:0', grad_fn=<LogSoftmaxBackward>),\n",
       " tensor([[-18.2933,  -0.1163,  -6.5587,  ..., -17.5926, -17.5546, -17.9556]],\n",
       "        device='cuda:0', grad_fn=<LogSoftmaxBackward>),\n",
       " tensor([[-18.0661,  -0.1504,  -6.2241,  ..., -17.3050, -17.3759, -17.7213]],\n",
       "        device='cuda:0', grad_fn=<LogSoftmaxBackward>),\n",
       " tensor([[-17.6186,  -0.1970,  -6.0266,  ..., -16.9301, -17.0032, -17.3510]],\n",
       "        device='cuda:0', grad_fn=<LogSoftmaxBackward>),\n",
       " tensor([[-17.0830,  -0.4234,  -5.6199,  ..., -16.4120, -16.4990, -16.7517]],\n",
       "        device='cuda:0', grad_fn=<LogSoftmaxBackward>),\n",
       " tensor([[-16.8429,  -0.1808,  -5.6739,  ..., -15.9723, -15.9358, -16.4847]],\n",
       "        device='cuda:0', grad_fn=<LogSoftmaxBackward>),\n",
       " tensor([[-18.1790,  -0.1856,  -6.2358,  ..., -17.5216, -17.5099, -17.9234]],\n",
       "        device='cuda:0', grad_fn=<LogSoftmaxBackward>),\n",
       " tensor([[-12.2825,  -2.2423,  -3.5907,  ..., -11.5055, -11.8596, -11.8429]],\n",
       "        device='cuda:0', grad_fn=<LogSoftmaxBackward>),\n",
       " tensor([[-16.6631,  -0.2244,  -5.6579,  ..., -15.8428, -15.7867, -16.2871]],\n",
       "        device='cuda:0', grad_fn=<LogSoftmaxBackward>),\n",
       " tensor([[-18.0079,  -0.1733,  -6.1548,  ..., -17.3632, -17.2660, -17.6543]],\n",
       "        device='cuda:0', grad_fn=<LogSoftmaxBackward>),\n",
       " tensor([[-16.6388,  -0.2585,  -5.5758,  ..., -15.7932, -15.7022, -16.2801]],\n",
       "        device='cuda:0', grad_fn=<LogSoftmaxBackward>),\n",
       " tensor([[-15.9496,  -3.0611,  -4.9192,  ..., -15.3444, -15.6058, -15.5690]],\n",
       "        device='cuda:0', grad_fn=<LogSoftmaxBackward>),\n",
       " tensor([[-17.7844,  -0.1915,  -6.1435,  ..., -17.0419, -17.0314, -17.3627]],\n",
       "        device='cuda:0', grad_fn=<LogSoftmaxBackward>),\n",
       " tensor([[-15.9269,  -0.2786,  -5.2824,  ..., -15.0992, -15.0846, -15.5480]],\n",
       "        device='cuda:0', grad_fn=<LogSoftmaxBackward>),\n",
       " tensor([[-18.0564,  -0.2608,  -6.0011,  ..., -17.3449, -17.3643, -17.7325]],\n",
       "        device='cuda:0', grad_fn=<LogSoftmaxBackward>),\n",
       " tensor([[-14.4553,  -2.6436,  -4.7722,  ..., -14.0084, -14.1827, -14.1436]],\n",
       "        device='cuda:0', grad_fn=<LogSoftmaxBackward>),\n",
       " tensor([[-15.1683,  -1.6275,  -3.7744,  ..., -14.3973, -14.6813, -14.7754]],\n",
       "        device='cuda:0', grad_fn=<LogSoftmaxBackward>),\n",
       " tensor([[-15.6630,  -3.3430,  -5.4978,  ..., -15.2442, -15.4084, -15.4241]],\n",
       "        device='cuda:0', grad_fn=<LogSoftmaxBackward>),\n",
       " tensor([[-15.6398,  -2.0501,  -5.0668,  ..., -15.0561, -15.2201, -15.1726]],\n",
       "        device='cuda:0', grad_fn=<LogSoftmaxBackward>),\n",
       " tensor([[-16.3001,  -0.4141,  -5.0728,  ..., -15.3831, -15.3883, -15.9301]],\n",
       "        device='cuda:0', grad_fn=<LogSoftmaxBackward>),\n",
       " tensor([[-16.0820,  -0.4276,  -4.9123,  ..., -15.1863, -15.1855, -15.7053]],\n",
       "        device='cuda:0', grad_fn=<LogSoftmaxBackward>),\n",
       " tensor([[-16.1407,  -0.5485,  -4.9579,  ..., -15.2946, -15.3150, -15.8486]],\n",
       "        device='cuda:0', grad_fn=<LogSoftmaxBackward>),\n",
       " tensor([[-17.8415,  -0.2549,  -5.8930,  ..., -17.1527, -17.0377, -17.4204]],\n",
       "        device='cuda:0', grad_fn=<LogSoftmaxBackward>),\n",
       " tensor([[-12.7159,  -5.0389,  -3.7468,  ..., -12.0431, -12.4555, -12.3939]],\n",
       "        device='cuda:0', grad_fn=<LogSoftmaxBackward>),\n",
       " tensor([[-16.2710,  -0.5697,  -4.8964,  ..., -15.4025, -15.3229, -15.8374]],\n",
       "        device='cuda:0', grad_fn=<LogSoftmaxBackward>),\n",
       " tensor([[-17.4165,  -0.3317,  -5.6918,  ..., -16.8686, -16.7917, -17.2465]],\n",
       "        device='cuda:0', grad_fn=<LogSoftmaxBackward>),\n",
       " tensor([[-12.5367,  -2.4246,  -3.3091,  ..., -11.7973, -12.1050, -12.1167]],\n",
       "        device='cuda:0', grad_fn=<LogSoftmaxBackward>),\n",
       " tensor([[-17.0059,  -1.4519,  -5.3464,  ..., -16.4968, -16.5322, -16.6932]],\n",
       "        device='cuda:0', grad_fn=<LogSoftmaxBackward>),\n",
       " tensor([[-15.3807,  -0.6680,  -4.3531,  ..., -14.5849, -14.4568, -14.9886]],\n",
       "        device='cuda:0', grad_fn=<LogSoftmaxBackward>),\n",
       " tensor([[-12.4621,  -2.5415,  -3.3195,  ..., -11.7200, -11.9979, -12.0158]],\n",
       "        device='cuda:0', grad_fn=<LogSoftmaxBackward>),\n",
       " tensor([[-16.2052,  -5.8556,  -5.1765,  ..., -15.9356, -15.9793, -15.5550]],\n",
       "        device='cuda:0', grad_fn=<LogSoftmaxBackward>),\n",
       " tensor([[-15.7386,  -0.5223,  -4.5940,  ..., -14.9685, -14.8240, -15.3430]],\n",
       "        device='cuda:0', grad_fn=<LogSoftmaxBackward>),\n",
       " tensor([[-15.9398,  -0.5632,  -4.5385,  ..., -15.0876, -15.0026, -15.4830]],\n",
       "        device='cuda:0', grad_fn=<LogSoftmaxBackward>),\n",
       " tensor([[-16.0971,  -0.2625,  -5.0319,  ..., -15.3752, -15.2886, -15.6256]],\n",
       "        device='cuda:0', grad_fn=<LogSoftmaxBackward>),\n",
       " tensor([[-16.0357,  -0.2650,  -4.9492,  ..., -15.2722, -15.1910, -15.6203]],\n",
       "        device='cuda:0', grad_fn=<LogSoftmaxBackward>),\n",
       " tensor([[-11.9936,  -2.2432,  -3.4360,  ..., -11.1565, -11.6246, -11.5695]],\n",
       "        device='cuda:0', grad_fn=<LogSoftmaxBackward>),\n",
       " tensor([[-12.4334,  -2.1544,  -3.3719,  ..., -11.6516, -11.9556, -11.9697]],\n",
       "        device='cuda:0', grad_fn=<LogSoftmaxBackward>),\n",
       " tensor([[-12.4218,  -2.1435,  -3.3841,  ..., -11.6346, -11.9298, -11.9367]],\n",
       "        device='cuda:0', grad_fn=<LogSoftmaxBackward>),\n",
       " tensor([[-12.4797,  -2.0770,  -3.3636,  ..., -11.6726, -11.9914, -11.9927]],\n",
       "        device='cuda:0', grad_fn=<LogSoftmaxBackward>),\n",
       " tensor([[-12.3353,  -2.1686,  -3.4175,  ..., -11.5508, -11.8534, -11.8552]],\n",
       "        device='cuda:0', grad_fn=<LogSoftmaxBackward>),\n",
       " tensor([[-17.2733,  -0.2601,  -5.5417,  ..., -16.6571, -16.4982, -17.0048]],\n",
       "        device='cuda:0', grad_fn=<LogSoftmaxBackward>),\n",
       " tensor([[-12.2305,  -2.1579,  -3.4536,  ..., -11.4411, -11.7687, -11.7446]],\n",
       "        device='cuda:0', grad_fn=<LogSoftmaxBackward>),\n",
       " tensor([[-11.4541,  -1.9172,  -4.0363,  ..., -10.9541, -11.4270, -11.2450]],\n",
       "        device='cuda:0', grad_fn=<LogSoftmaxBackward>),\n",
       " tensor([[-12.1366,  -2.2383,  -3.5289,  ..., -11.4005, -11.6698, -11.6687]],\n",
       "        device='cuda:0', grad_fn=<LogSoftmaxBackward>),\n",
       " tensor([[-12.2736,  -2.0535,  -3.4520,  ..., -11.5212, -11.7804, -11.8193]],\n",
       "        device='cuda:0', grad_fn=<LogSoftmaxBackward>),\n",
       " tensor([[-16.3425,  -1.5148,  -4.2735,  ..., -15.5116, -15.9416, -15.7557]],\n",
       "        device='cuda:0', grad_fn=<LogSoftmaxBackward>),\n",
       " tensor([[-14.8760,  -0.9261,  -3.7767,  ..., -14.2214, -14.1331, -14.3688]],\n",
       "        device='cuda:0', grad_fn=<LogSoftmaxBackward>),\n",
       " tensor([[-17.2862,  -0.2850,  -5.7040,  ..., -16.6007, -16.6016, -16.9438]],\n",
       "        device='cuda:0', grad_fn=<LogSoftmaxBackward>),\n",
       " tensor([[-16.4243,  -1.5369,  -3.9304,  ..., -15.6205, -15.7982, -15.7821]],\n",
       "        device='cuda:0', grad_fn=<LogSoftmaxBackward>),\n",
       " tensor([[-11.5481,  -2.6387,  -3.0962,  ..., -10.9899, -11.0202, -11.4021]],\n",
       "        device='cuda:0', grad_fn=<LogSoftmaxBackward>),\n",
       " tensor([[-17.7064,  -0.1455,  -6.0655,  ..., -17.0413, -16.9955, -17.3019]],\n",
       "        device='cuda:0', grad_fn=<LogSoftmaxBackward>),\n",
       " tensor([[-12.2685,  -2.3149,  -3.3918,  ..., -11.5321, -11.7387, -11.8306]],\n",
       "        device='cuda:0', grad_fn=<LogSoftmaxBackward>),\n",
       " tensor([[-12.3386,  -2.0315,  -3.4041,  ..., -11.5834, -11.7978, -11.8782]],\n",
       "        device='cuda:0', grad_fn=<LogSoftmaxBackward>),\n",
       " tensor([[-12.3638,  -2.1217,  -3.4357,  ..., -11.6372, -11.8146, -11.8853]],\n",
       "        device='cuda:0', grad_fn=<LogSoftmaxBackward>),\n",
       " tensor([[-16.8345,  -0.2437,  -5.8920,  ..., -16.1724, -16.1972, -16.5176]],\n",
       "        device='cuda:0', grad_fn=<LogSoftmaxBackward>),\n",
       " tensor([[-16.2546,  -1.0566,  -5.5095,  ..., -15.7285, -15.6813, -16.0239]],\n",
       "        device='cuda:0', grad_fn=<LogSoftmaxBackward>),\n",
       " tensor([[-14.2575,  -2.0089,  -3.5105,  ..., -13.5336, -13.8216, -14.1682]],\n",
       "        device='cuda:0', grad_fn=<LogSoftmaxBackward>),\n",
       " tensor([[-14.0755,  -1.9572,  -4.5747,  ..., -13.7937, -13.7602, -13.5628]],\n",
       "        device='cuda:0', grad_fn=<LogSoftmaxBackward>),\n",
       " tensor([[-12.5191,  -2.1474,  -3.4924,  ..., -11.7935, -11.9790, -12.0158]],\n",
       "        device='cuda:0', grad_fn=<LogSoftmaxBackward>),\n",
       " tensor([[-12.5018,  -2.1297,  -3.5325,  ..., -11.7721, -11.9577, -11.9692]],\n",
       "        device='cuda:0', grad_fn=<LogSoftmaxBackward>),\n",
       " tensor([[-12.5145,  -2.2077,  -3.5733,  ..., -11.7999, -11.9771, -12.0070]],\n",
       "        device='cuda:0', grad_fn=<LogSoftmaxBackward>),\n",
       " tensor([[-12.6076,  -2.1312,  -3.5748,  ..., -11.8694, -12.0483, -12.0839]],\n",
       "        device='cuda:0', grad_fn=<LogSoftmaxBackward>),\n",
       " tensor([[-17.0010,  -0.1317,  -6.2975,  ..., -16.3891, -16.2623, -16.6013]],\n",
       "        device='cuda:0', grad_fn=<LogSoftmaxBackward>),\n",
       " tensor([[-17.3687,  -0.1818,  -6.5246,  ..., -16.8121, -16.6567, -17.0060]],\n",
       "        device='cuda:0', grad_fn=<LogSoftmaxBackward>),\n",
       " tensor([[-16.8462,  -0.1702,  -6.5113,  ..., -16.1877, -16.1245, -16.4274]],\n",
       "        device='cuda:0', grad_fn=<LogSoftmaxBackward>),\n",
       " tensor([[-15.9449,  -1.7466,  -5.1520,  ..., -15.2786, -15.5276, -15.3803]],\n",
       "        device='cuda:0', grad_fn=<LogSoftmaxBackward>),\n",
       " tensor([[-17.7086,  -0.1338,  -6.9285,  ..., -17.0072, -17.0088, -17.2487]],\n",
       "        device='cuda:0', grad_fn=<LogSoftmaxBackward>),\n",
       " tensor([[-15.5640,  -1.1381,  -5.4573,  ..., -14.9468, -15.0744, -15.2729]],\n",
       "        device='cuda:0', grad_fn=<LogSoftmaxBackward>),\n",
       " tensor([[-12.5370,  -2.1584,  -3.7130,  ..., -11.8132, -12.0054, -12.0606]],\n",
       "        device='cuda:0', grad_fn=<LogSoftmaxBackward>),\n",
       " tensor([[-17.7334,  -0.1879,  -6.5103,  ..., -17.0323, -17.0207, -17.3032]],\n",
       "        device='cuda:0', grad_fn=<LogSoftmaxBackward>),\n",
       " tensor([[-13.6423,  -2.3797,  -4.7418,  ..., -13.1182, -13.3848, -13.1358]],\n",
       "        device='cuda:0', grad_fn=<LogSoftmaxBackward>),\n",
       " tensor([[-16.0325,  -0.3735,  -5.1431,  ..., -15.1561, -15.1544, -15.4979]],\n",
       "        device='cuda:0', grad_fn=<LogSoftmaxBackward>),\n",
       " tensor([[-17.9620,  -0.1230,  -6.9393,  ..., -17.1672, -17.1680, -17.3504]],\n",
       "        device='cuda:0', grad_fn=<LogSoftmaxBackward>),\n",
       " tensor([[-16.9571,  -0.1833,  -6.1286,  ..., -16.0152, -16.0405, -16.3271]],\n",
       "        device='cuda:0', grad_fn=<LogSoftmaxBackward>),\n",
       " tensor([[-16.3699,  -0.1259,  -6.3427,  ..., -15.5268, -15.4894, -15.7627]],\n",
       "        device='cuda:0', grad_fn=<LogSoftmaxBackward>),\n",
       " tensor([[-13.7285,  -1.5285,  -3.8602,  ..., -12.9440, -13.0447, -13.2491]],\n",
       "        device='cuda:0', grad_fn=<LogSoftmaxBackward>),\n",
       " tensor([[-12.9083,  -6.2125,  -4.5611,  ..., -11.9776, -12.6377, -12.5323]],\n",
       "        device='cuda:0', grad_fn=<LogSoftmaxBackward>),\n",
       " tensor([[-16.2036,  -0.5025,  -5.3764,  ..., -15.2228, -15.1845, -15.6599]],\n",
       "        device='cuda:0', grad_fn=<LogSoftmaxBackward>),\n",
       " tensor([[-18.0711,  -0.1526,  -6.9417,  ..., -17.1910, -17.2287, -17.4097]],\n",
       "        device='cuda:0', grad_fn=<LogSoftmaxBackward>),\n",
       " tensor([[-18.2117,  -0.1615,  -7.0212,  ..., -17.2777, -17.3520, -17.6769]],\n",
       "        device='cuda:0', grad_fn=<LogSoftmaxBackward>),\n",
       " tensor([[-17.6478,  -1.2222,  -5.9395,  ..., -16.9072, -16.9686, -17.0961]],\n",
       "        device='cuda:0', grad_fn=<LogSoftmaxBackward>),\n",
       " tensor([[-17.7038,  -0.1429,  -6.8120,  ..., -16.9409, -16.9085, -17.2511]],\n",
       "        device='cuda:0', grad_fn=<LogSoftmaxBackward>),\n",
       " tensor([[-16.8745,  -0.1592,  -6.5808,  ..., -15.9799, -16.0447, -16.2459]],\n",
       "        device='cuda:0', grad_fn=<LogSoftmaxBackward>),\n",
       " tensor([[-14.7388,  -1.8728,  -5.1191,  ..., -14.1521, -14.2876, -14.0627]],\n",
       "        device='cuda:0', grad_fn=<LogSoftmaxBackward>),\n",
       " tensor([[-13.8104,  -1.5945,  -4.6613,  ..., -13.3052, -13.3225, -13.3505]],\n",
       "        device='cuda:0', grad_fn=<LogSoftmaxBackward>),\n",
       " tensor([[-17.4200,  -0.1201,  -6.9099,  ..., -16.6779, -16.7091, -16.8182]],\n",
       "        device='cuda:0', grad_fn=<LogSoftmaxBackward>),\n",
       " tensor([[-15.0048,  -0.9171,  -5.7835,  ..., -14.3585, -14.7580, -14.4711]],\n",
       "        device='cuda:0', grad_fn=<LogSoftmaxBackward>),\n",
       " tensor([[-15.4718,  -0.7240,  -4.8964,  ..., -14.4598, -14.5064, -15.0212]],\n",
       "        device='cuda:0', grad_fn=<LogSoftmaxBackward>),\n",
       " tensor([[-15.6801,  -0.7272,  -5.0850,  ..., -14.7358, -14.7734, -15.2782]],\n",
       "        device='cuda:0', grad_fn=<LogSoftmaxBackward>),\n",
       " tensor([[-12.7606,  -2.6721,  -4.2001,  ..., -11.8630, -12.3889, -12.4136]],\n",
       "        device='cuda:0', grad_fn=<LogSoftmaxBackward>),\n",
       " tensor([[-12.9393,  -2.2914,  -5.4310,  ..., -12.5604, -12.9322, -12.7185]],\n",
       "        device='cuda:0', grad_fn=<LogSoftmaxBackward>),\n",
       " tensor([[-17.4948,  -0.1412,  -6.6650,  ..., -16.6981, -16.7371, -17.1228]],\n",
       "        device='cuda:0', grad_fn=<LogSoftmaxBackward>),\n",
       " tensor([[-17.0755,  -0.1812,  -6.8264,  ..., -16.3385, -16.4111, -16.7113]],\n",
       "        device='cuda:0', grad_fn=<LogSoftmaxBackward>),\n",
       " tensor([[-15.6251,  -0.6178,  -5.1452,  ..., -14.6168, -14.6414, -15.1891]],\n",
       "        device='cuda:0', grad_fn=<LogSoftmaxBackward>),\n",
       " tensor([[-15.9952,  -1.1704,  -6.0024,  ..., -15.2245, -15.3722, -15.5264]],\n",
       "        device='cuda:0', grad_fn=<LogSoftmaxBackward>),\n",
       " tensor([[-15.3575,  -2.1103,  -4.9696,  ..., -14.7914, -14.7062, -14.6193]],\n",
       "        device='cuda:0', grad_fn=<LogSoftmaxBackward>),\n",
       " tensor([[-15.3929,  -0.6687,  -4.7709,  ..., -14.4826, -14.3779, -14.9231]],\n",
       "        device='cuda:0', grad_fn=<LogSoftmaxBackward>),\n",
       " tensor([[-15.1817,  -1.5924,  -5.3546,  ..., -14.7443, -14.9272, -14.6280]],\n",
       "        device='cuda:0', grad_fn=<LogSoftmaxBackward>),\n",
       " tensor([[-18.9273,  -0.0945,  -7.2308,  ..., -18.0704, -18.0875, -18.4523]],\n",
       "        device='cuda:0', grad_fn=<LogSoftmaxBackward>),\n",
       " tensor([[-14.1815,  -3.4873,  -6.1474,  ..., -13.6009, -13.7331, -13.7130]],\n",
       "        device='cuda:0', grad_fn=<LogSoftmaxBackward>),\n",
       " tensor([[-16.0516,  -0.5651,  -5.3116,  ..., -15.0179, -15.0178, -15.6162]],\n",
       "        device='cuda:0', grad_fn=<LogSoftmaxBackward>),\n",
       " tensor([[-12.7512,  -2.4139,  -4.1137,  ..., -11.8987, -12.3580, -12.2644]],\n",
       "        device='cuda:0', grad_fn=<LogSoftmaxBackward>),\n",
       " tensor([[-18.7885,  -0.0762,  -7.3914,  ..., -17.8708, -17.9561, -18.0533]],\n",
       "        device='cuda:0', grad_fn=<LogSoftmaxBackward>),\n",
       " tensor([[-18.0799,  -0.2602,  -6.8930,  ..., -17.2952, -17.2285, -17.5661]],\n",
       "        device='cuda:0', grad_fn=<LogSoftmaxBackward>),\n",
       " tensor([[-16.2432,  -0.2693,  -5.4867,  ..., -15.2780, -15.2237, -15.5802]],\n",
       "        device='cuda:0', grad_fn=<LogSoftmaxBackward>),\n",
       " tensor([[-17.1362,  -0.6897,  -6.0958,  ..., -16.4149, -16.4214, -16.3690]],\n",
       "        device='cuda:0', grad_fn=<LogSoftmaxBackward>),\n",
       " tensor([[-15.7381,  -0.3681,  -5.1657,  ..., -14.7102, -14.6495, -15.0177]],\n",
       "        device='cuda:0', grad_fn=<LogSoftmaxBackward>),\n",
       " tensor([[-15.2064,  -0.4589,  -4.8738,  ..., -14.2422, -14.1611, -14.5576]],\n",
       "        device='cuda:0', grad_fn=<LogSoftmaxBackward>),\n",
       " tensor([[-18.3211,  -0.1114,  -6.5264,  ..., -17.3767, -17.4453, -17.5322]],\n",
       "        device='cuda:0', grad_fn=<LogSoftmaxBackward>),\n",
       " tensor([[-15.3479,  -0.6018,  -4.9697,  ..., -14.2415, -14.2737, -14.6361]],\n",
       "        device='cuda:0', grad_fn=<LogSoftmaxBackward>),\n",
       " tensor([[-15.4824,  -2.3845,  -5.4389,  ..., -14.6970, -15.0842, -14.9196]],\n",
       "        device='cuda:0', grad_fn=<LogSoftmaxBackward>),\n",
       " tensor([[-12.8474,  -2.3406,  -3.1852,  ..., -12.1402, -12.4084, -12.0771]],\n",
       "        device='cuda:0', grad_fn=<LogSoftmaxBackward>),\n",
       " tensor([[-13.8501,  -2.4853,  -3.4218,  ..., -12.7872, -12.9596, -13.1747]],\n",
       "        device='cuda:0', grad_fn=<LogSoftmaxBackward>),\n",
       " tensor([[-12.3003,  -5.6139,  -4.4144,  ..., -11.4524, -11.9322, -11.6802]],\n",
       "        device='cuda:0', grad_fn=<LogSoftmaxBackward>),\n",
       " tensor([[-14.8909,  -0.7091,  -4.6797,  ..., -13.7859, -13.7943, -14.2520]],\n",
       "        device='cuda:0', grad_fn=<LogSoftmaxBackward>),\n",
       " tensor([[-17.9032,  -0.0933,  -6.8361,  ..., -16.9405, -17.0637, -17.1319]],\n",
       "        device='cuda:0', grad_fn=<LogSoftmaxBackward>),\n",
       " tensor([[-12.2530,  -2.6891,  -3.8303,  ..., -11.4208, -11.7398, -11.5687]],\n",
       "        device='cuda:0', grad_fn=<LogSoftmaxBackward>),\n",
       " tensor([[-14.1618,  -1.5180,  -4.8134,  ..., -13.6429, -13.7239, -13.4127]],\n",
       "        device='cuda:0', grad_fn=<LogSoftmaxBackward>),\n",
       " tensor([[-15.2368,  -1.4415,  -5.0975,  ..., -14.7330, -14.7589, -14.6181]],\n",
       "        device='cuda:0', grad_fn=<LogSoftmaxBackward>),\n",
       " tensor([[-14.4489,  -1.6243,  -4.5503,  ..., -14.1219, -14.2620, -13.8617]],\n",
       "        device='cuda:0', grad_fn=<LogSoftmaxBackward>),\n",
       " tensor([[-15.2180,  -0.7137,  -4.2623,  ..., -14.0636, -14.0749, -14.4343]],\n",
       "        device='cuda:0', grad_fn=<LogSoftmaxBackward>),\n",
       " tensor([[-16.2715,  -0.8594,  -5.7721,  ..., -15.2103, -15.4987, -15.4628]],\n",
       "        device='cuda:0', grad_fn=<LogSoftmaxBackward>),\n",
       " tensor([[-17.6769,  -0.1710,  -6.4519,  ..., -16.7738, -16.7591, -16.8719]],\n",
       "        device='cuda:0', grad_fn=<LogSoftmaxBackward>),\n",
       " tensor([[-15.4275,  -1.4800,  -4.6667,  ..., -14.7299, -14.9250, -14.6455]],\n",
       "        device='cuda:0', grad_fn=<LogSoftmaxBackward>),\n",
       " tensor([[-15.4271,  -0.6023,  -4.5675,  ..., -14.3706, -14.2276, -14.5748]],\n",
       "        device='cuda:0', grad_fn=<LogSoftmaxBackward>),\n",
       " tensor([[-15.8416,  -0.5859,  -4.6302,  ..., -14.7401, -14.6427, -15.0131]],\n",
       "        device='cuda:0', grad_fn=<LogSoftmaxBackward>),\n",
       " tensor([[-12.8626,  -2.3902,  -3.6453,  ..., -12.0399, -12.2921, -12.2520]],\n",
       "        device='cuda:0', grad_fn=<LogSoftmaxBackward>),\n",
       " tensor([[-13.0161,  -2.3963,  -3.6297,  ..., -12.1662, -12.4101, -12.3936]],\n",
       "        device='cuda:0', grad_fn=<LogSoftmaxBackward>),\n",
       " tensor([[-12.9603,  -2.3252,  -3.6580,  ..., -12.1083, -12.3657, -12.3555]],\n",
       "        device='cuda:0', grad_fn=<LogSoftmaxBackward>),\n",
       " tensor([[-12.9469,  -2.2937,  -3.5837,  ..., -12.1409, -12.3575, -12.3379]],\n",
       "        device='cuda:0', grad_fn=<LogSoftmaxBackward>),\n",
       " tensor([[-16.4170,  -0.4793,  -5.1469,  ..., -15.3361, -15.2037, -15.6310]],\n",
       "        device='cuda:0', grad_fn=<LogSoftmaxBackward>),\n",
       " tensor([[-12.9671,  -2.7241,  -3.7080,  ..., -12.1419, -12.4240, -12.3875]],\n",
       "        device='cuda:0', grad_fn=<LogSoftmaxBackward>),\n",
       " tensor([[-16.8546,  -0.3426,  -5.0031,  ..., -15.8735, -15.7385, -16.0683]],\n",
       "        device='cuda:0', grad_fn=<LogSoftmaxBackward>),\n",
       " tensor([[-18.8757,  -0.0993,  -6.9719,  ..., -17.9996, -17.9101, -18.0955]],\n",
       "        device='cuda:0', grad_fn=<LogSoftmaxBackward>),\n",
       " tensor([[-12.9318,  -2.1919,  -3.5096,  ..., -12.2145, -12.4585, -12.4311]],\n",
       "        device='cuda:0', grad_fn=<LogSoftmaxBackward>),\n",
       " tensor([[-16.7490,  -0.3939,  -5.0318,  ..., -15.7970, -15.5853, -15.9776]],\n",
       "        device='cuda:0', grad_fn=<LogSoftmaxBackward>),\n",
       " tensor([[-17.1810,  -0.4679,  -6.2740,  ..., -16.3396, -16.3838, -16.4677]],\n",
       "        device='cuda:0', grad_fn=<LogSoftmaxBackward>),\n",
       " tensor([[-16.7544,  -0.3998,  -4.8037,  ..., -15.7571, -15.5658, -15.9601]],\n",
       "        device='cuda:0', grad_fn=<LogSoftmaxBackward>),\n",
       " tensor([[-17.9477,  -0.1337,  -5.9037,  ..., -17.1101, -17.1103, -17.4037]],\n",
       "        device='cuda:0', grad_fn=<LogSoftmaxBackward>),\n",
       " tensor([[-16.1032,  -0.6889,  -4.6704,  ..., -15.2843, -14.8697, -15.3205]],\n",
       "        device='cuda:0', grad_fn=<LogSoftmaxBackward>),\n",
       " tensor([[-12.9557,  -2.1894,  -3.7519,  ..., -12.1975, -12.4053, -12.3746]],\n",
       "        device='cuda:0', grad_fn=<LogSoftmaxBackward>),\n",
       " tensor([[-16.0666,  -0.5374,  -4.5813,  ..., -15.1423, -14.9203, -15.2579]],\n",
       "        device='cuda:0', grad_fn=<LogSoftmaxBackward>),\n",
       " tensor([[-15.7226,  -0.4727,  -4.5688,  ..., -14.8428, -14.7108, -15.0333]],\n",
       "        device='cuda:0', grad_fn=<LogSoftmaxBackward>),\n",
       " tensor([[-16.0283,  -0.6949,  -4.3165,  ..., -15.0761, -14.8710, -15.2447]],\n",
       "        device='cuda:0', grad_fn=<LogSoftmaxBackward>),\n",
       " tensor([[-13.7324,  -1.3343,  -5.4465,  ..., -13.1212, -13.4440, -13.1366]],\n",
       "        device='cuda:0', grad_fn=<LogSoftmaxBackward>),\n",
       " tensor([[-17.7575,  -0.1353,  -6.0133,  ..., -17.0496, -16.9056, -17.0832]],\n",
       "        device='cuda:0', grad_fn=<LogSoftmaxBackward>),\n",
       " tensor([[-17.7954,  -0.1839,  -5.9990,  ..., -16.9815, -16.8406, -17.1546]],\n",
       "        device='cuda:0', grad_fn=<LogSoftmaxBackward>),\n",
       " tensor([[-17.2086,  -0.1981,  -5.5881,  ..., -16.5563, -16.2384, -16.6316]],\n",
       "        device='cuda:0', grad_fn=<LogSoftmaxBackward>),\n",
       " tensor([[-12.9431,  -2.7550,  -3.9999,  ..., -12.2062, -12.3438, -12.3741]],\n",
       "        device='cuda:0', grad_fn=<LogSoftmaxBackward>),\n",
       " tensor([[-12.9312,  -2.8169,  -4.1622,  ..., -12.1984, -12.3254, -12.3927]],\n",
       "        device='cuda:0', grad_fn=<LogSoftmaxBackward>),\n",
       " tensor([[-15.2047,  -0.8247,  -4.3008,  ..., -14.3394, -14.2105, -14.5527]],\n",
       "        device='cuda:0', grad_fn=<LogSoftmaxBackward>),\n",
       " tensor([[-17.3333,  -0.5694,  -5.5849,  ..., -16.7200, -16.4902, -16.6489]],\n",
       "        device='cuda:0', grad_fn=<LogSoftmaxBackward>),\n",
       " tensor([[-15.8848,  -0.9113,  -4.2017,  ..., -14.8628, -14.7625, -15.0670]],\n",
       "        device='cuda:0', grad_fn=<LogSoftmaxBackward>),\n",
       " tensor([[-15.9356,  -0.9405,  -4.3326,  ..., -14.9401, -14.7322, -15.1088]],\n",
       "        device='cuda:0', grad_fn=<LogSoftmaxBackward>),\n",
       " tensor([[-15.9507,  -0.8572,  -4.6211,  ..., -15.0277, -14.7349, -15.1829]],\n",
       "        device='cuda:0', grad_fn=<LogSoftmaxBackward>),\n",
       " tensor([[-15.6538,  -0.7671,  -4.2166,  ..., -14.6800, -14.4133, -14.8539]],\n",
       "        device='cuda:0', grad_fn=<LogSoftmaxBackward>),\n",
       " tensor([[-15.8479,  -0.8581,  -4.3181,  ..., -14.7659, -14.6280, -15.0381]],\n",
       "        device='cuda:0', grad_fn=<LogSoftmaxBackward>),\n",
       " tensor([[-15.9243,  -0.7549,  -4.5155,  ..., -14.9783, -14.7771, -15.1775]],\n",
       "        device='cuda:0', grad_fn=<LogSoftmaxBackward>),\n",
       " tensor([[-18.0233,  -0.2083,  -6.6653,  ..., -17.4243, -17.1149, -17.4590]],\n",
       "        device='cuda:0', grad_fn=<LogSoftmaxBackward>),\n",
       " tensor([[-16.6299,  -1.3109,  -5.6159,  ..., -15.8627, -15.6554, -15.9784]],\n",
       "        device='cuda:0', grad_fn=<LogSoftmaxBackward>),\n",
       " tensor([[-15.7305,  -0.7618,  -4.1968,  ..., -14.7618, -14.5735, -14.9265]],\n",
       "        device='cuda:0', grad_fn=<LogSoftmaxBackward>),\n",
       " tensor([[-11.7485,  -1.3083,  -4.3419,  ..., -11.4434, -11.3098, -11.2800]],\n",
       "        device='cuda:0', grad_fn=<LogSoftmaxBackward>),\n",
       " tensor([[-13.8647,  -2.2938,  -4.8225,  ..., -13.1194, -13.5845, -13.3887]],\n",
       "        device='cuda:0', grad_fn=<LogSoftmaxBackward>),\n",
       " tensor([[-18.1884,  -0.2223,  -6.5763,  ..., -17.6612, -17.2650, -17.6470]],\n",
       "        device='cuda:0', grad_fn=<LogSoftmaxBackward>),\n",
       " tensor([[-16.2246,  -0.7211,  -4.5156,  ..., -15.2232, -15.0323, -15.4249]],\n",
       "        device='cuda:0', grad_fn=<LogSoftmaxBackward>),\n",
       " tensor([[-17.6957,  -1.4790,  -5.1510,  ..., -17.0504, -16.9637, -17.0667]],\n",
       "        device='cuda:0', grad_fn=<LogSoftmaxBackward>),\n",
       " tensor([[-18.5288,  -0.1939,  -6.6201,  ..., -17.9128, -17.4558, -18.0370]],\n",
       "        device='cuda:0', grad_fn=<LogSoftmaxBackward>),\n",
       " tensor([[-13.0825,  -6.0135,  -5.0460,  ..., -12.3805, -12.5188, -12.6376]],\n",
       "        device='cuda:0', grad_fn=<LogSoftmaxBackward>),\n",
       " tensor([[-15.6467,  -0.7157,  -4.5222,  ..., -14.7337, -14.4190, -14.8215]],\n",
       "        device='cuda:0', grad_fn=<LogSoftmaxBackward>),\n",
       " tensor([[-18.0439,  -0.1640,  -6.3103,  ..., -17.3561, -17.1092, -17.5676]],\n",
       "        device='cuda:0', grad_fn=<LogSoftmaxBackward>),\n",
       " tensor([[-17.0038,  -0.9072,  -6.2345,  ..., -16.0466, -16.1067, -16.4352]],\n",
       "        device='cuda:0', grad_fn=<LogSoftmaxBackward>),\n",
       " tensor([[-15.9196,  -0.6439,  -4.5352,  ..., -14.8611, -14.6581, -15.0993]],\n",
       "        device='cuda:0', grad_fn=<LogSoftmaxBackward>),\n",
       " tensor([[-18.1859,  -0.1998,  -6.7317,  ..., -17.5972, -17.2593, -17.6510]],\n",
       "        device='cuda:0', grad_fn=<LogSoftmaxBackward>),\n",
       " tensor([[-18.3787,  -0.1901,  -6.9476,  ..., -17.7061, -17.3543, -17.8058]],\n",
       "        device='cuda:0', grad_fn=<LogSoftmaxBackward>),\n",
       " tensor([[-17.6328,  -0.1979,  -6.5169,  ..., -16.9945, -16.7200, -17.1609]],\n",
       "        device='cuda:0', grad_fn=<LogSoftmaxBackward>),\n",
       " tensor([[-12.7594,  -2.2804,  -4.2665,  ..., -12.1363, -12.1525, -12.2428]],\n",
       "        device='cuda:0', grad_fn=<LogSoftmaxBackward>),\n",
       " tensor([[-15.6921,  -0.8357,  -4.4995,  ..., -14.6540, -14.3723, -14.8343]],\n",
       "        device='cuda:0', grad_fn=<LogSoftmaxBackward>),\n",
       " tensor([[-13.0943,  -1.2941,  -5.7215,  ..., -12.9010, -13.0496, -12.8731]],\n",
       "        device='cuda:0', grad_fn=<LogSoftmaxBackward>),\n",
       " tensor([[-18.1279,  -0.1322,  -6.8988,  ..., -17.5907, -17.3771, -17.6152]],\n",
       "        device='cuda:0', grad_fn=<LogSoftmaxBackward>),\n",
       " tensor([[-12.9295,  -2.7630,  -4.2429,  ..., -12.3340, -12.3208, -12.5087]],\n",
       "        device='cuda:0', grad_fn=<LogSoftmaxBackward>),\n",
       " tensor([[-17.2845,  -0.1781,  -6.5488,  ..., -16.5124, -16.4011, -16.8610]],\n",
       "        device='cuda:0', grad_fn=<LogSoftmaxBackward>),\n",
       " tensor([[-15.8166,  -0.7961,  -4.3640,  ..., -14.7502, -14.5081, -14.9322]],\n",
       "        device='cuda:0', grad_fn=<LogSoftmaxBackward>),\n",
       " tensor([[-18.6138,  -0.1674,  -7.0673,  ..., -17.9941, -17.7846, -18.0174]],\n",
       "        device='cuda:0', grad_fn=<LogSoftmaxBackward>),\n",
       " tensor([[-18.4331,  -0.1980,  -6.3015,  ..., -17.8349, -17.5635, -17.7569]],\n",
       "        device='cuda:0', grad_fn=<LogSoftmaxBackward>),\n",
       " tensor([[-17.9201,  -0.2102,  -6.2942,  ..., -17.3381, -17.0402, -17.4304]],\n",
       "        device='cuda:0', grad_fn=<LogSoftmaxBackward>),\n",
       " tensor([[-15.6871,  -0.8126,  -4.0732,  ..., -14.6360, -14.4144, -14.8630]],\n",
       "        device='cuda:0', grad_fn=<LogSoftmaxBackward>),\n",
       " tensor([[-13.7321,  -2.4008,  -2.2187,  ..., -13.3731, -13.0846, -13.2424]],\n",
       "        device='cuda:0', grad_fn=<LogSoftmaxBackward>),\n",
       " tensor([[-18.0315,  -0.2675,  -5.9363,  ..., -17.4937, -17.1585, -17.5015]],\n",
       "        device='cuda:0', grad_fn=<LogSoftmaxBackward>),\n",
       " tensor([[-16.6456,  -0.2154,  -5.8468,  ..., -16.1913, -16.0033, -16.0304]],\n",
       "        device='cuda:0', grad_fn=<LogSoftmaxBackward>),\n",
       " tensor([[-17.2736,  -0.2176,  -5.8395,  ..., -16.7244, -16.3604, -16.8370]],\n",
       "        device='cuda:0', grad_fn=<LogSoftmaxBackward>),\n",
       " tensor([[-15.4204,  -0.8805,  -4.1805,  ..., -14.4891, -14.2291, -14.6467]],\n",
       "        device='cuda:0', grad_fn=<LogSoftmaxBackward>),\n",
       " tensor([[-16.1707,  -1.7145,  -4.7410,  ..., -15.6161, -15.1711, -15.3289]],\n",
       "        device='cuda:0', grad_fn=<LogSoftmaxBackward>),\n",
       " tensor([[-17.2383,  -0.2044,  -6.0358,  ..., -16.6890, -16.6399, -16.6886]],\n",
       "        device='cuda:0', grad_fn=<LogSoftmaxBackward>),\n",
       " tensor([[-16.7898,  -1.1870,  -5.9731,  ..., -15.8262, -16.0362, -16.1148]],\n",
       "        device='cuda:0', grad_fn=<LogSoftmaxBackward>),\n",
       " tensor([[-12.1431,  -2.6784,  -3.0106,  ..., -12.0532, -11.5416, -11.6639]],\n",
       "        device='cuda:0', grad_fn=<LogSoftmaxBackward>),\n",
       " tensor([[-15.5274,  -0.8336,  -4.2029,  ..., -14.5853, -14.3146, -14.6920]],\n",
       "        device='cuda:0', grad_fn=<LogSoftmaxBackward>),\n",
       " tensor([[-15.9870,  -0.1440,  -6.0897,  ..., -15.2597, -15.3726, -15.5420]],\n",
       "        device='cuda:0', grad_fn=<LogSoftmaxBackward>),\n",
       " tensor([[-15.6054,  -0.6728,  -4.1748,  ..., -14.6714, -14.3265, -14.8103]],\n",
       "        device='cuda:0', grad_fn=<LogSoftmaxBackward>),\n",
       " tensor([[-17.3103,  -1.1956,  -5.4106,  ..., -16.5302, -16.5542, -16.5060]],\n",
       "        device='cuda:0', grad_fn=<LogSoftmaxBackward>),\n",
       " tensor([[-17.1799,  -0.2278,  -5.9497,  ..., -16.6053, -16.4932, -16.5608]],\n",
       "        device='cuda:0', grad_fn=<LogSoftmaxBackward>),\n",
       " tensor([[-15.6316,  -0.7383,  -4.2928,  ..., -14.6646, -14.3984, -14.8096]],\n",
       "        device='cuda:0', grad_fn=<LogSoftmaxBackward>),\n",
       " tensor([[-15.6632,  -0.6772,  -4.0107,  ..., -14.6928, -14.3581, -14.8100]],\n",
       "        device='cuda:0', grad_fn=<LogSoftmaxBackward>),\n",
       " tensor([[-15.7268,  -0.6818,  -4.1137,  ..., -14.7566, -14.4827, -14.9431]],\n",
       "        device='cuda:0', grad_fn=<LogSoftmaxBackward>),\n",
       " tensor([[-16.0443,  -0.7619,  -3.8381,  ..., -14.9701, -14.8194, -15.1572]],\n",
       "        device='cuda:0', grad_fn=<LogSoftmaxBackward>),\n",
       " tensor([[-15.7694,  -0.7769,  -3.8589,  ..., -14.8022, -14.5627, -15.0208]],\n",
       "        device='cuda:0', grad_fn=<LogSoftmaxBackward>),\n",
       " tensor([[-15.7543,  -1.0089,  -5.2288,  ..., -15.2388, -15.0229, -15.2496]],\n",
       "        device='cuda:0', grad_fn=<LogSoftmaxBackward>),\n",
       " tensor([[-17.4463,  -0.2498,  -6.2476,  ..., -16.9472, -16.8166, -16.8646]],\n",
       "        device='cuda:0', grad_fn=<LogSoftmaxBackward>),\n",
       " tensor([[-16.9250,  -0.1817,  -6.0427,  ..., -16.5051, -16.2902, -16.2995]],\n",
       "        device='cuda:0', grad_fn=<LogSoftmaxBackward>),\n",
       " tensor([[-15.8554,  -0.7364,  -4.9934,  ..., -14.9650, -15.0280, -15.4297]],\n",
       "        device='cuda:0', grad_fn=<LogSoftmaxBackward>),\n",
       " tensor([[-15.3627,  -0.7861,  -3.6391,  ..., -14.3919, -14.2249, -14.6499]],\n",
       "        device='cuda:0', grad_fn=<LogSoftmaxBackward>),\n",
       " tensor([[-12.9426,  -2.6949,  -3.6760,  ..., -12.3058, -12.2591, -12.6580]],\n",
       "        device='cuda:0', grad_fn=<LogSoftmaxBackward>),\n",
       " tensor([[-12.9410,  -1.5961,  -5.4250,  ..., -12.6017, -12.8892, -12.8243]],\n",
       "        device='cuda:0', grad_fn=<LogSoftmaxBackward>),\n",
       " tensor([[-17.1348,  -0.2029,  -6.0152,  ..., -16.6233, -16.4526, -16.7116]],\n",
       "        device='cuda:0', grad_fn=<LogSoftmaxBackward>),\n",
       " tensor([[-17.1140,  -0.1640,  -5.9310,  ..., -16.6897, -16.4142, -16.5788]],\n",
       "        device='cuda:0', grad_fn=<LogSoftmaxBackward>),\n",
       " tensor([[-17.5264,  -0.1472,  -6.4357,  ..., -17.0045, -16.8726, -16.9889]],\n",
       "        device='cuda:0', grad_fn=<LogSoftmaxBackward>),\n",
       " tensor([[-12.7607,  -2.9212,  -3.6315,  ..., -12.1471, -12.1441, -12.5038]],\n",
       "        device='cuda:0', grad_fn=<LogSoftmaxBackward>),\n",
       " tensor([[-12.7521,  -2.7694,  -3.5857,  ..., -12.1495, -12.1416, -12.5302]],\n",
       "        device='cuda:0', grad_fn=<LogSoftmaxBackward>),\n",
       " tensor([[-15.0195,  -0.7008,  -3.2589,  ..., -14.0693, -13.8979, -14.1982]],\n",
       "        device='cuda:0', grad_fn=<LogSoftmaxBackward>),\n",
       " tensor([[-15.0120,  -0.7496,  -3.4240,  ..., -13.9985, -13.8976, -14.2288]],\n",
       "        device='cuda:0', grad_fn=<LogSoftmaxBackward>),\n",
       " tensor([[-14.3848,  -0.6564,  -3.6011,  ..., -13.5968, -13.4332, -13.8009]],\n",
       "        device='cuda:0', grad_fn=<LogSoftmaxBackward>),\n",
       " tensor([[-13.1339,  -2.3632,  -3.4477,  ..., -12.4065, -12.3589, -12.7707]],\n",
       "        device='cuda:0', grad_fn=<LogSoftmaxBackward>),\n",
       " tensor([[-13.4146,  -4.5808,  -3.7274,  ..., -12.7577, -12.7397, -13.1285]],\n",
       "        device='cuda:0', grad_fn=<LogSoftmaxBackward>),\n",
       " tensor([[-13.1110,  -2.3901,  -3.2683,  ..., -12.3724, -12.3572, -12.7387]],\n",
       "        device='cuda:0', grad_fn=<LogSoftmaxBackward>),\n",
       " tensor([[-14.9590,  -0.7555,  -3.4923,  ..., -13.9343, -13.7980, -14.1613]],\n",
       "        device='cuda:0', grad_fn=<LogSoftmaxBackward>),\n",
       " tensor([[-16.3545,  -0.2185,  -5.0606,  ..., -15.5162, -15.4622, -15.8891]],\n",
       "        device='cuda:0', grad_fn=<LogSoftmaxBackward>),\n",
       " tensor([[-14.7335,  -0.7716,  -3.1156,  ..., -13.7189, -13.5686, -13.8775]],\n",
       "        device='cuda:0', grad_fn=<LogSoftmaxBackward>),\n",
       " tensor([[-14.6937,  -0.6858,  -3.4925,  ..., -13.7267, -13.5865, -13.9288]],\n",
       "        device='cuda:0', grad_fn=<LogSoftmaxBackward>),\n",
       " tensor([[-16.7968,  -0.1238,  -5.7095,  ..., -16.3888, -16.1185, -16.3486]],\n",
       "        device='cuda:0', grad_fn=<LogSoftmaxBackward>),\n",
       " tensor([[-15.0859,  -0.6535,  -3.2727,  ..., -14.0290, -13.9801, -14.2664]],\n",
       "        device='cuda:0', grad_fn=<LogSoftmaxBackward>),\n",
       " tensor([[-16.1150,  -0.2111,  -4.9020,  ..., -15.5283, -15.1188, -15.5487]],\n",
       "        device='cuda:0', grad_fn=<LogSoftmaxBackward>),\n",
       " tensor([[-17.5525,  -0.1070,  -6.1377,  ..., -16.9723, -16.8026, -17.0331]],\n",
       "        device='cuda:0', grad_fn=<LogSoftmaxBackward>),\n",
       " tensor([[-15.3592,  -2.3942,  -5.4938,  ..., -14.9044, -15.0251, -14.7202]],\n",
       "        device='cuda:0', grad_fn=<LogSoftmaxBackward>),\n",
       " tensor([[-15.2609,  -0.5857,  -3.1883,  ..., -14.1869, -14.0737, -14.3672]],\n",
       "        device='cuda:0', grad_fn=<LogSoftmaxBackward>),\n",
       " tensor([[-15.0899,  -0.5059,  -3.2077,  ..., -13.9700, -13.9555, -14.1868]],\n",
       "        device='cuda:0', grad_fn=<LogSoftmaxBackward>),\n",
       " tensor([[-16.8811,  -0.2764,  -5.1346,  ..., -16.1203, -15.8307, -16.0974]],\n",
       "        device='cuda:0', grad_fn=<LogSoftmaxBackward>),\n",
       " tensor([[-16.2727,  -1.7808,  -4.5978,  ..., -15.6211, -15.3979, -15.5276]],\n",
       "        device='cuda:0', grad_fn=<LogSoftmaxBackward>),\n",
       " tensor([[-17.6213,  -0.1204,  -5.6474,  ..., -16.9770, -16.6626, -17.0066]],\n",
       "        device='cuda:0', grad_fn=<LogSoftmaxBackward>),\n",
       " tensor([[-17.8068,  -0.1073,  -5.9857,  ..., -17.1678, -16.9140, -17.2586]],\n",
       "        device='cuda:0', grad_fn=<LogSoftmaxBackward>),\n",
       " tensor([[-17.5609,  -0.1324,  -5.9809,  ..., -16.8777, -16.5433, -16.8529]],\n",
       "        device='cuda:0', grad_fn=<LogSoftmaxBackward>),\n",
       " tensor([[-17.1521,  -0.7892,  -5.9691,  ..., -16.3613, -16.2826, -16.5854]],\n",
       "        device='cuda:0', grad_fn=<LogSoftmaxBackward>),\n",
       " tensor([[-18.4778,  -0.1621,  -6.2404,  ..., -17.7353, -17.4530, -17.6911]],\n",
       "        device='cuda:0', grad_fn=<LogSoftmaxBackward>),\n",
       " tensor([[-12.5572,  -2.6321,  -2.7095,  ..., -11.7007, -11.8844, -12.1482]],\n",
       "        device='cuda:0', grad_fn=<LogSoftmaxBackward>),\n",
       " tensor([[-10.9177,  -2.2968,  -3.1271,  ..., -10.3873, -10.3381, -10.4526]],\n",
       "        device='cuda:0', grad_fn=<LogSoftmaxBackward>),\n",
       " tensor([[-12.6029,  -2.3260,  -2.7430,  ..., -11.7946, -11.9014, -12.1910]],\n",
       "        device='cuda:0', grad_fn=<LogSoftmaxBackward>),\n",
       " tensor([[-12.6507,  -2.1378,  -2.7222,  ..., -11.8462, -11.9331, -12.2643]],\n",
       "        device='cuda:0', grad_fn=<LogSoftmaxBackward>),\n",
       " tensor([[-12.8200,  -2.3289,  -2.8042,  ..., -11.9821, -12.0899, -12.3642]],\n",
       "        device='cuda:0', grad_fn=<LogSoftmaxBackward>),\n",
       " tensor([[-18.3734,  -0.1350,  -6.3576,  ..., -17.6214, -17.3501, -17.6214]],\n",
       "        device='cuda:0', grad_fn=<LogSoftmaxBackward>),\n",
       " tensor([[-12.9193,  -2.0633,  -2.9570,  ..., -12.0628, -12.1452, -12.4579]],\n",
       "        device='cuda:0', grad_fn=<LogSoftmaxBackward>),\n",
       " tensor([[-14.0879,  -1.7115,  -4.2982,  ..., -13.5733, -13.4982, -13.7269]],\n",
       "        device='cuda:0', grad_fn=<LogSoftmaxBackward>),\n",
       " tensor([[-15.5958,  -0.3530,  -3.8978,  ..., -14.4477, -14.4043, -14.7248]],\n",
       "        device='cuda:0', grad_fn=<LogSoftmaxBackward>),\n",
       " tensor([[-16.0131,  -0.4100,  -3.5777,  ..., -14.8403, -14.7727, -15.0893]],\n",
       "        device='cuda:0', grad_fn=<LogSoftmaxBackward>),\n",
       " tensor([[-13.2655,  -4.7782,  -3.5304,  ..., -12.5187, -12.6517, -12.8916]],\n",
       "        device='cuda:0', grad_fn=<LogSoftmaxBackward>),\n",
       " tensor([[-15.0574,  -1.9092,  -5.7788,  ..., -14.7324, -14.7917, -14.6097]],\n",
       "        device='cuda:0', grad_fn=<LogSoftmaxBackward>),\n",
       " tensor([[-15.2755,  -5.1043,  -5.7585,  ..., -15.1056, -14.8826, -14.6104]],\n",
       "        device='cuda:0', grad_fn=<LogSoftmaxBackward>),\n",
       " tensor([[-15.5070,  -0.4995,  -3.6460,  ..., -14.4279, -14.3132, -14.5500]],\n",
       "        device='cuda:0', grad_fn=<LogSoftmaxBackward>),\n",
       " tensor([[-12.0102,  -6.0459,  -4.7421,  ..., -11.7017, -11.9145, -11.5606]],\n",
       "        device='cuda:0', grad_fn=<LogSoftmaxBackward>),\n",
       " tensor([[-18.5084,  -0.1163,  -6.8270,  ..., -17.7185, -17.4199, -17.6433]],\n",
       "        device='cuda:0', grad_fn=<LogSoftmaxBackward>),\n",
       " tensor([[-15.7932,  -0.5112,  -3.7999,  ..., -14.6732, -14.5686, -14.9436]],\n",
       "        device='cuda:0', grad_fn=<LogSoftmaxBackward>),\n",
       " tensor([[-14.1516,  -1.7867,  -5.1959,  ..., -13.6402, -13.7479, -13.3857]],\n",
       "        device='cuda:0', grad_fn=<LogSoftmaxBackward>),\n",
       " tensor([[-15.6895,  -0.4463,  -3.9322,  ..., -14.6455, -14.6146, -14.8984]],\n",
       "        device='cuda:0', grad_fn=<LogSoftmaxBackward>),\n",
       " tensor([[-17.7502,  -0.1422,  -6.7046,  ..., -17.0517, -16.7631, -17.0109]],\n",
       "        device='cuda:0', grad_fn=<LogSoftmaxBackward>),\n",
       " tensor([[-14.0769,  -2.8979,  -5.1685,  ..., -13.9116, -13.8354, -13.4988]],\n",
       "        device='cuda:0', grad_fn=<LogSoftmaxBackward>),\n",
       " tensor([[-14.2619,  -1.4972,  -5.3430,  ..., -13.8290, -13.8688, -13.5234]],\n",
       "        device='cuda:0', grad_fn=<LogSoftmaxBackward>),\n",
       " tensor([[-17.9165,  -0.0943,  -7.3736,  ..., -17.2493, -17.0488, -17.1202]],\n",
       "        device='cuda:0', grad_fn=<LogSoftmaxBackward>),\n",
       " tensor([[-17.3723,  -0.2576,  -6.7066,  ..., -16.6445, -16.5006, -16.5065]],\n",
       "        device='cuda:0', grad_fn=<LogSoftmaxBackward>),\n",
       " tensor([[-17.3349,  -0.0884,  -7.0265,  ..., -16.8446, -16.5409, -16.6753]],\n",
       "        device='cuda:0', grad_fn=<LogSoftmaxBackward>),\n",
       " tensor([[-14.0847,  -1.0844,  -2.9965,  ..., -13.6336, -13.5911, -13.4766]],\n",
       "        device='cuda:0', grad_fn=<LogSoftmaxBackward>),\n",
       " tensor([[-18.2407,  -0.0967,  -7.3806,  ..., -17.5039, -17.2458, -17.4572]],\n",
       "        device='cuda:0', grad_fn=<LogSoftmaxBackward>),\n",
       " tensor([[-15.2848,  -2.1626,  -5.7987,  ..., -14.7135, -14.4811, -14.6084]],\n",
       "        device='cuda:0', grad_fn=<LogSoftmaxBackward>),\n",
       " tensor([[-17.5839,  -0.1205,  -6.3081,  ..., -16.9265, -16.6542, -16.7725]],\n",
       "        device='cuda:0', grad_fn=<LogSoftmaxBackward>),\n",
       " tensor([[-12.9291,  -1.3257,  -4.1495,  ..., -12.3652, -12.3937, -12.2935]],\n",
       "        device='cuda:0', grad_fn=<LogSoftmaxBackward>),\n",
       " tensor([[-15.0571,  -0.5597,  -4.0503,  ..., -14.1993, -13.9137, -14.4053]],\n",
       "        device='cuda:0', grad_fn=<LogSoftmaxBackward>),\n",
       " tensor([[-13.0179,  -2.1774,  -4.0674,  ..., -12.6954, -12.4329, -12.3386]],\n",
       "        device='cuda:0', grad_fn=<LogSoftmaxBackward>),\n",
       " tensor([[-14.8140,  -0.6017,  -3.9631,  ..., -14.0196, -13.8454, -14.2100]],\n",
       "        device='cuda:0', grad_fn=<LogSoftmaxBackward>),\n",
       " tensor([[-15.8810,  -0.4581,  -4.0125,  ..., -14.8669, -14.7668, -15.1138]],\n",
       "        device='cuda:0', grad_fn=<LogSoftmaxBackward>),\n",
       " tensor([[-18.1183,  -1.3686,  -5.5729,  ..., -17.5130, -17.2971, -17.2486]],\n",
       "        device='cuda:0', grad_fn=<LogSoftmaxBackward>),\n",
       " tensor([[-12.6782,  -2.3254,  -3.6738,  ..., -12.1012, -12.0481, -12.4761]],\n",
       "        device='cuda:0', grad_fn=<LogSoftmaxBackward>),\n",
       " tensor([[-17.1117,  -0.1497,  -6.4636,  ..., -16.6362, -16.2856, -16.5667]],\n",
       "        device='cuda:0', grad_fn=<LogSoftmaxBackward>),\n",
       " tensor([[-14.6838,  -1.5414,  -5.6022,  ..., -14.1754, -14.2041, -14.0917]],\n",
       "        device='cuda:0', grad_fn=<LogSoftmaxBackward>),\n",
       " tensor([[-16.1693,  -1.5788,  -4.8024,  ..., -15.7738, -15.3600, -15.2363]],\n",
       "        device='cuda:0', grad_fn=<LogSoftmaxBackward>),\n",
       " tensor([[-16.0128,  -0.5134,  -3.8347,  ..., -15.0224, -14.8288, -15.2483]],\n",
       "        device='cuda:0', grad_fn=<LogSoftmaxBackward>),\n",
       " tensor([[-13.7548,  -2.8208,  -3.7888,  ..., -13.2057, -13.0970, -13.0152]],\n",
       "        device='cuda:0', grad_fn=<LogSoftmaxBackward>),\n",
       " tensor([[-13.5161,  -1.0018,  -4.1024,  ..., -13.2497, -13.0078, -13.1744]],\n",
       "        device='cuda:0', grad_fn=<LogSoftmaxBackward>),\n",
       " tensor([[-12.7708,  -2.2053,  -3.4078,  ..., -12.1901, -12.1179, -12.5241]],\n",
       "        device='cuda:0', grad_fn=<LogSoftmaxBackward>),\n",
       " tensor([[-15.7858,  -0.4868,  -3.6766,  ..., -14.8162, -14.6564, -15.0339]],\n",
       "        device='cuda:0', grad_fn=<LogSoftmaxBackward>),\n",
       " tensor([[-15.8295,  -0.6055,  -3.4635,  ..., -14.8857, -14.6905, -15.0926]],\n",
       "        device='cuda:0', grad_fn=<LogSoftmaxBackward>),\n",
       " tensor([[-12.7753,  -2.1912,  -3.3371,  ..., -12.1956, -12.1088, -12.5162]],\n",
       "        device='cuda:0', grad_fn=<LogSoftmaxBackward>),\n",
       " tensor([[-18.0917,  -0.1024,  -6.4225,  ..., -17.3482, -17.1673, -17.4096]],\n",
       "        device='cuda:0', grad_fn=<LogSoftmaxBackward>),\n",
       " tensor([[-12.7762,  -2.1193,  -3.2133,  ..., -12.1798, -12.1250, -12.5095]],\n",
       "        device='cuda:0', grad_fn=<LogSoftmaxBackward>),\n",
       " tensor([[-17.3110,  -0.5630,  -6.1661,  ..., -16.7663, -16.6845, -16.6626]],\n",
       "        device='cuda:0', grad_fn=<LogSoftmaxBackward>),\n",
       " tensor([[-14.3914,  -1.0878,  -5.0906,  ..., -14.0458, -13.8860, -13.9753]],\n",
       "        device='cuda:0', grad_fn=<LogSoftmaxBackward>),\n",
       " tensor([[-12.9638,  -2.0114,  -3.1139,  ..., -12.4002, -12.2827, -12.6929]],\n",
       "        device='cuda:0', grad_fn=<LogSoftmaxBackward>),\n",
       " tensor([[-18.8973,  -0.0747,  -6.4078,  ..., -18.1187, -17.8489, -18.1453]],\n",
       "        device='cuda:0', grad_fn=<LogSoftmaxBackward>),\n",
       " tensor([[-13.0425,  -2.1112,  -3.0793,  ..., -12.4481, -12.3802, -12.7579]],\n",
       "        device='cuda:0', grad_fn=<LogSoftmaxBackward>),\n",
       " tensor([[-14.0892,  -1.0134,  -4.7517,  ..., -13.5839, -13.5898, -13.4688]],\n",
       "        device='cuda:0', grad_fn=<LogSoftmaxBackward>),\n",
       " tensor([[-13.2466,  -2.2104,  -3.0766,  ..., -12.5988, -12.5452, -12.9399]],\n",
       "        device='cuda:0', grad_fn=<LogSoftmaxBackward>),\n",
       " tensor([[-17.3166,  -0.3875,  -6.3038,  ..., -16.4868, -16.4739, -16.6812]],\n",
       "        device='cuda:0', grad_fn=<LogSoftmaxBackward>),\n",
       " tensor([[-13.2471,  -2.0675,  -3.0928,  ..., -12.5546, -12.5445, -12.8981]],\n",
       "        device='cuda:0', grad_fn=<LogSoftmaxBackward>),\n",
       " tensor([[-13.4017,  -2.1936,  -3.0890,  ..., -12.6928, -12.6023, -13.0217]],\n",
       "        device='cuda:0', grad_fn=<LogSoftmaxBackward>),\n",
       " tensor([[-17.1141,  -0.3115,  -6.1812,  ..., -16.1543, -16.2174, -16.4523]],\n",
       "        device='cuda:0', grad_fn=<LogSoftmaxBackward>),\n",
       " tensor([[-18.1080,  -0.0698,  -6.1609,  ..., -17.4341, -17.2341, -17.4445]],\n",
       "        device='cuda:0', grad_fn=<LogSoftmaxBackward>),\n",
       " tensor([[-17.7748,  -0.1249,  -6.0731,  ..., -17.0799, -16.8455, -17.1694]],\n",
       "        device='cuda:0', grad_fn=<LogSoftmaxBackward>),\n",
       " tensor([[-18.6698,  -0.0715,  -6.7450,  ..., -17.8689, -17.8394, -17.9627]],\n",
       "        device='cuda:0', grad_fn=<LogSoftmaxBackward>),\n",
       " tensor([[-13.5127,  -2.0700,  -3.1009,  ..., -12.7546, -12.6865, -13.1259]],\n",
       "        device='cuda:0', grad_fn=<LogSoftmaxBackward>),\n",
       " tensor([[-13.5006,  -2.0612,  -3.1414,  ..., -12.7429, -12.6800, -13.1235]],\n",
       "        device='cuda:0', grad_fn=<LogSoftmaxBackward>),\n",
       " tensor([[-16.3317,  -0.1214,  -5.5076,  ..., -15.5684, -15.4474, -15.6191]],\n",
       "        device='cuda:0', grad_fn=<LogSoftmaxBackward>),\n",
       " tensor([[-13.4708,  -2.1478,  -3.1298,  ..., -12.7065, -12.6645, -13.1179]],\n",
       "        device='cuda:0', grad_fn=<LogSoftmaxBackward>),\n",
       " tensor([[-18.3565,  -0.0877,  -6.6595,  ..., -17.5044, -17.3370, -17.6972]],\n",
       "        device='cuda:0', grad_fn=<LogSoftmaxBackward>),\n",
       " tensor([[-17.0580,  -0.6457,  -5.1845,  ..., -16.2370, -16.2299, -16.1694]],\n",
       "        device='cuda:0', grad_fn=<LogSoftmaxBackward>),\n",
       " tensor([[-18.5616,  -0.0712,  -6.8350,  ..., -17.7958, -17.6269, -17.8032]],\n",
       "        device='cuda:0', grad_fn=<LogSoftmaxBackward>),\n",
       " tensor([[-13.4685,  -2.6000,  -3.1957,  ..., -12.6801, -12.6934, -13.1043]],\n",
       "        device='cuda:0', grad_fn=<LogSoftmaxBackward>),\n",
       " tensor([[-14.4510,  -0.9276,  -5.4819,  ..., -14.0107, -13.9833, -13.8805]],\n",
       "        device='cuda:0', grad_fn=<LogSoftmaxBackward>),\n",
       " tensor([[-14.9303,  -0.6808,  -4.5582,  ..., -14.5689, -14.3214, -14.3953]],\n",
       "        device='cuda:0', grad_fn=<LogSoftmaxBackward>),\n",
       " tensor([[-13.3371,  -2.4536,  -3.2508,  ..., -12.5827, -12.5488, -13.0040]],\n",
       "        device='cuda:0', grad_fn=<LogSoftmaxBackward>),\n",
       " tensor([[-13.3910,  -2.1158,  -3.2185,  ..., -12.6198, -12.5807, -13.0451]],\n",
       "        device='cuda:0', grad_fn=<LogSoftmaxBackward>),\n",
       " tensor([[-16.2708,  -0.7609,  -5.4936,  ..., -15.7778, -15.7691, -15.7421]],\n",
       "        device='cuda:0', grad_fn=<LogSoftmaxBackward>),\n",
       " tensor([[-17.4151,  -0.5643,  -5.0884,  ..., -16.5781, -16.5791, -16.6311]],\n",
       "        device='cuda:0', grad_fn=<LogSoftmaxBackward>),\n",
       " tensor([[-12.1772,  -3.6490,  -4.2039,  ..., -11.7491, -11.6101, -11.5553]],\n",
       "        device='cuda:0', grad_fn=<LogSoftmaxBackward>),\n",
       " tensor([[-13.2722,  -2.4388,  -3.3520,  ..., -12.5499, -12.4924, -12.9617]],\n",
       "        device='cuda:0', grad_fn=<LogSoftmaxBackward>),\n",
       " tensor([[-15.9061,  -0.7001,  -3.7409,  ..., -14.8015, -14.6547, -15.0994]],\n",
       "        device='cuda:0', grad_fn=<LogSoftmaxBackward>),\n",
       " tensor([[-13.4677,  -0.9777,  -5.1545,  ..., -12.8568, -12.4236, -12.8427]],\n",
       "        device='cuda:0', grad_fn=<LogSoftmaxBackward>),\n",
       " tensor([[-17.0899,  -0.1331,  -6.4351,  ..., -16.4359, -16.2676, -16.4387]],\n",
       "        device='cuda:0', grad_fn=<LogSoftmaxBackward>),\n",
       " tensor([[-15.8549,  -0.6484,  -3.6072,  ..., -14.8032, -14.5526, -15.0344]],\n",
       "        device='cuda:0', grad_fn=<LogSoftmaxBackward>),\n",
       " tensor([[-16.1700,  -0.7464,  -3.5878,  ..., -15.0226, -14.8028, -15.2104]],\n",
       "        device='cuda:0', grad_fn=<LogSoftmaxBackward>),\n",
       " tensor([[-14.1464,  -0.5158,  -6.0773,  ..., -13.4387, -13.3981, -13.3980]],\n",
       "        device='cuda:0', grad_fn=<LogSoftmaxBackward>),\n",
       " tensor([[-16.3712,  -0.7252,  -4.0133,  ..., -15.2083, -15.0733, -15.5067]],\n",
       "        device='cuda:0', grad_fn=<LogSoftmaxBackward>),\n",
       " tensor([[-13.3322,  -2.0859,  -3.3783,  ..., -12.5617, -12.5982, -13.0000]],\n",
       "        device='cuda:0', grad_fn=<LogSoftmaxBackward>),\n",
       " tensor([[-16.4241,  -0.7501,  -4.0764,  ..., -15.3494, -15.0509, -15.5436]],\n",
       "        device='cuda:0', grad_fn=<LogSoftmaxBackward>),\n",
       " tensor([[-18.1737,  -0.1234,  -6.9582,  ..., -17.3295, -17.3871, -17.3836]],\n",
       "        device='cuda:0', grad_fn=<LogSoftmaxBackward>),\n",
       " tensor([[-17.8544,  -0.2342,  -6.6036,  ..., -17.2045, -16.9919, -17.2003]],\n",
       "        device='cuda:0', grad_fn=<LogSoftmaxBackward>),\n",
       " tensor([[-16.4504,  -0.7020,  -5.5340,  ..., -15.7939, -15.5035, -15.8085]],\n",
       "        device='cuda:0', grad_fn=<LogSoftmaxBackward>),\n",
       " tensor([[-13.7096,  -1.8505,  -5.4565,  ..., -13.2219, -13.1317, -13.0298]],\n",
       "        device='cuda:0', grad_fn=<LogSoftmaxBackward>),\n",
       " tensor([[-16.4573,  -0.7768,  -3.9738,  ..., -15.2784, -15.1415, -15.5569]],\n",
       "        device='cuda:0', grad_fn=<LogSoftmaxBackward>),\n",
       " tensor([[-16.7313,  -0.3535,  -5.5225,  ..., -16.0747, -15.8380, -16.1980]],\n",
       "        device='cuda:0', grad_fn=<LogSoftmaxBackward>),\n",
       " tensor([[-17.0469,  -0.3219,  -4.9196,  ..., -16.0231, -15.8307, -16.4559]],\n",
       "        device='cuda:0', grad_fn=<LogSoftmaxBackward>),\n",
       " tensor([[-15.7689,  -0.8716,  -3.7596,  ..., -14.5484, -14.5163, -15.0000]],\n",
       "        device='cuda:0', grad_fn=<LogSoftmaxBackward>),\n",
       " tensor([[-12.9815,  -3.5069,  -3.7149,  ..., -12.1619, -12.3464, -12.5812]],\n",
       "        device='cuda:0', grad_fn=<LogSoftmaxBackward>),\n",
       " tensor([[-16.8870,  -2.8128,  -5.7941,  ..., -16.2543, -15.9366, -16.3800]],\n",
       "        device='cuda:0', grad_fn=<LogSoftmaxBackward>),\n",
       " tensor([[-16.1377,  -0.8271,  -3.6270,  ..., -14.9676, -14.8326, -15.3400]],\n",
       "        device='cuda:0', grad_fn=<LogSoftmaxBackward>),\n",
       " tensor([[-17.7076,  -1.5330,  -5.8264,  ..., -16.9068, -16.7019, -16.9195]],\n",
       "        device='cuda:0', grad_fn=<LogSoftmaxBackward>),\n",
       " tensor([[-17.7943,  -0.3030,  -4.9426,  ..., -16.8206, -16.5890, -17.2738]],\n",
       "        device='cuda:0', grad_fn=<LogSoftmaxBackward>),\n",
       " tensor([[-15.8915,  -1.8905,  -4.2460,  ..., -15.0719, -15.0451, -15.0197]],\n",
       "        device='cuda:0', grad_fn=<LogSoftmaxBackward>),\n",
       " tensor([[-18.9936,  -0.1110,  -6.6589,  ..., -18.1168, -18.0520, -18.1653]],\n",
       "        device='cuda:0', grad_fn=<LogSoftmaxBackward>),\n",
       " tensor([[-16.1314,  -0.8774,  -3.5433,  ..., -14.9543, -14.8770, -15.2343]],\n",
       "        device='cuda:0', grad_fn=<LogSoftmaxBackward>),\n",
       " tensor([[-18.1848,  -0.1256,  -6.1377,  ..., -17.3770, -17.4007, -17.6117]],\n",
       "        device='cuda:0', grad_fn=<LogSoftmaxBackward>),\n",
       " tensor([[-15.8078,  -0.9991,  -3.5882,  ..., -14.6353, -14.4512, -14.9337]],\n",
       "        device='cuda:0', grad_fn=<LogSoftmaxBackward>),\n",
       " tensor([[-17.9017,  -0.2990,  -4.7848,  ..., -16.6215, -16.5573, -17.0115]],\n",
       "        device='cuda:0', grad_fn=<LogSoftmaxBackward>),\n",
       " tensor([[-13.3876,  -2.1349,  -5.0035,  ..., -12.6684, -13.0234, -12.8185]],\n",
       "        device='cuda:0', grad_fn=<LogSoftmaxBackward>),\n",
       " tensor([[-13.9923,  -1.7250,  -4.6163,  ..., -13.0721, -13.6850, -13.4751]],\n",
       "        device='cuda:0', grad_fn=<LogSoftmaxBackward>),\n",
       " tensor([[-13.6793,  -6.0854,  -4.1044,  ..., -12.7366, -13.0614, -13.3603]],\n",
       "        device='cuda:0', grad_fn=<LogSoftmaxBackward>),\n",
       " tensor([[-19.0397,  -0.0965,  -6.6101,  ..., -18.2468, -18.2266, -18.4006]],\n",
       "        device='cuda:0', grad_fn=<LogSoftmaxBackward>),\n",
       " tensor([[-17.1873,  -0.5357,  -5.9631,  ..., -16.2808, -16.2415, -16.5459]],\n",
       "        device='cuda:0', grad_fn=<LogSoftmaxBackward>),\n",
       " tensor([[-15.4709,  -1.0458,  -3.4531,  ..., -14.2689, -14.2011, -14.6700]],\n",
       "        device='cuda:0', grad_fn=<LogSoftmaxBackward>),\n",
       " tensor([[-15.9133,  -4.0744,  -5.3484,  ..., -15.3831, -15.2195, -15.2506]],\n",
       "        device='cuda:0', grad_fn=<LogSoftmaxBackward>),\n",
       " tensor([[-18.0083,  -0.1046,  -6.4979,  ..., -17.2337, -17.1439, -17.4019]],\n",
       "        device='cuda:0', grad_fn=<LogSoftmaxBackward>),\n",
       " tensor([[-17.6506,  -0.2033,  -6.3073,  ..., -16.9172, -16.8281, -17.0219]],\n",
       "        device='cuda:0', grad_fn=<LogSoftmaxBackward>),\n",
       " tensor([[-15.7950,  -1.0368,  -3.4497,  ..., -14.6135, -14.5793, -14.9531]],\n",
       "        device='cuda:0', grad_fn=<LogSoftmaxBackward>),\n",
       " tensor([[-15.1800,  -1.4269,  -3.0309,  ..., -14.4347, -14.4784, -14.6035]],\n",
       "        device='cuda:0', grad_fn=<LogSoftmaxBackward>),\n",
       " tensor([[-15.7439,  -0.6382,  -5.7878,  ..., -15.0401, -15.0986, -14.9935]],\n",
       "        device='cuda:0', grad_fn=<LogSoftmaxBackward>),\n",
       " tensor([[-15.4701,  -1.1131,  -3.5707,  ..., -14.3269, -14.0847, -14.6072]],\n",
       "        device='cuda:0', grad_fn=<LogSoftmaxBackward>),\n",
       " tensor([[-14.5851,  -0.8189,  -4.2797,  ..., -14.2815, -13.7752, -13.9414]],\n",
       "        device='cuda:0', grad_fn=<LogSoftmaxBackward>),\n",
       " tensor([[-17.1259,  -0.6352,  -6.1487,  ..., -16.4420, -16.3054, -16.4501]],\n",
       "        device='cuda:0', grad_fn=<LogSoftmaxBackward>),\n",
       " tensor([[-15.5457,  -0.9283,  -3.6944,  ..., -14.3503, -14.1605, -14.6067]],\n",
       "        device='cuda:0', grad_fn=<LogSoftmaxBackward>),\n",
       " tensor([[-12.7187,  -2.5063,  -2.8334,  ..., -11.9951, -11.8063, -12.0857]],\n",
       "        device='cuda:0', grad_fn=<LogSoftmaxBackward>),\n",
       " tensor([[-15.2156,  -1.3559,  -3.4664,  ..., -14.0793, -13.9793, -14.4029]],\n",
       "        device='cuda:0', grad_fn=<LogSoftmaxBackward>),\n",
       " tensor([[-14.3920,  -2.7250,  -4.2827,  ..., -13.9159, -13.8152, -13.6978]],\n",
       "        device='cuda:0', grad_fn=<LogSoftmaxBackward>),\n",
       " tensor([[-15.8178,  -1.2698,  -3.6883,  ..., -14.6265, -14.6248, -14.9457]],\n",
       "        device='cuda:0', grad_fn=<LogSoftmaxBackward>),\n",
       " tensor([[-18.2236,  -0.1194,  -6.5218,  ..., -17.2883, -17.2059, -17.3465]],\n",
       "        device='cuda:0', grad_fn=<LogSoftmaxBackward>),\n",
       " tensor([[-17.2798,  -0.1215,  -5.8860,  ..., -16.5017, -16.3302, -16.5835]],\n",
       "        device='cuda:0', grad_fn=<LogSoftmaxBackward>),\n",
       " tensor([[-15.9621,  -1.2486,  -3.6966,  ..., -14.7207, -14.5347, -15.0495]],\n",
       "        device='cuda:0', grad_fn=<LogSoftmaxBackward>),\n",
       " tensor([[-14.6556,  -1.6697,  -3.1197,  ..., -13.3999, -13.1040, -13.6747]],\n",
       "        device='cuda:0', grad_fn=<LogSoftmaxBackward>),\n",
       " tensor([[-16.5727,  -0.5391,  -4.8615,  ..., -15.8238, -15.5535, -15.8216]],\n",
       "        device='cuda:0', grad_fn=<LogSoftmaxBackward>),\n",
       " tensor([[-18.1304,  -0.2368,  -6.0469,  ..., -17.2153, -17.1158, -17.2158]],\n",
       "        device='cuda:0', grad_fn=<LogSoftmaxBackward>),\n",
       " tensor([[-15.7028,  -1.3336,  -3.3268,  ..., -14.4485, -14.2653, -14.6639]],\n",
       "        device='cuda:0', grad_fn=<LogSoftmaxBackward>),\n",
       " tensor([[-15.3266,  -1.1944,  -3.6305,  ..., -14.0747, -13.9466, -14.4111]],\n",
       "        device='cuda:0', grad_fn=<LogSoftmaxBackward>),\n",
       " tensor([[-16.3287,  -0.2327,  -4.7710,  ..., -15.4771, -15.1597, -15.5434]],\n",
       "        device='cuda:0', grad_fn=<LogSoftmaxBackward>),\n",
       " tensor([[-16.1599,  -1.1550,  -5.9012,  ..., -15.5055, -15.5321, -15.6757]],\n",
       "        device='cuda:0', grad_fn=<LogSoftmaxBackward>),\n",
       " tensor([[-15.3931,  -1.2407,  -3.3651,  ..., -14.2395, -14.0039, -14.4147]],\n",
       "        device='cuda:0', grad_fn=<LogSoftmaxBackward>),\n",
       " tensor([[-16.1179,  -0.6459,  -5.0205,  ..., -15.7330, -15.4372, -15.5791]],\n",
       "        device='cuda:0', grad_fn=<LogSoftmaxBackward>),\n",
       " tensor([[-17.4537,  -0.1632,  -5.4796,  ..., -16.4249, -16.4110, -16.6757]],\n",
       "        device='cuda:0', grad_fn=<LogSoftmaxBackward>),\n",
       " tensor([[-18.3990,  -0.1015,  -6.1328,  ..., -17.4815, -17.3609, -17.4260]],\n",
       "        device='cuda:0', grad_fn=<LogSoftmaxBackward>),\n",
       " tensor([[-18.1963,  -0.1348,  -6.7508,  ..., -17.3490, -17.1974, -17.3745]],\n",
       "        device='cuda:0', grad_fn=<LogSoftmaxBackward>),\n",
       " tensor([[-13.1285,  -4.0713,  -3.7318,  ..., -12.4017, -12.6022, -12.7093]],\n",
       "        device='cuda:0', grad_fn=<LogSoftmaxBackward>),\n",
       " tensor([[-14.4828,  -1.4352,  -3.4650,  ..., -13.3583, -13.2008, -13.5891]],\n",
       "        device='cuda:0', grad_fn=<LogSoftmaxBackward>),\n",
       " tensor([[-14.6908,  -1.2380,  -3.5939,  ..., -13.6593, -13.4139, -13.8981]],\n",
       "        device='cuda:0', grad_fn=<LogSoftmaxBackward>),\n",
       " tensor([[-14.2582,  -1.1697,  -4.2561,  ..., -13.7840, -13.6340, -13.4463]],\n",
       "        device='cuda:0', grad_fn=<LogSoftmaxBackward>),\n",
       " tensor([[-16.8351,  -0.1263,  -5.8694,  ..., -16.1119, -15.8527, -16.1462]],\n",
       "        device='cuda:0', grad_fn=<LogSoftmaxBackward>),\n",
       " tensor([[-16.8240,  -2.9121,  -6.3830,  ..., -16.3085, -16.2058, -16.5168]],\n",
       "        device='cuda:0', grad_fn=<LogSoftmaxBackward>),\n",
       " tensor([[-18.1456,  -0.1124,  -6.8852,  ..., -17.1925, -17.0679, -17.2693]],\n",
       "        device='cuda:0', grad_fn=<LogSoftmaxBackward>),\n",
       " tensor([[-15.6831,  -0.6225,  -3.9681,  ..., -14.7967, -14.3933, -14.8720]],\n",
       "        device='cuda:0', grad_fn=<LogSoftmaxBackward>),\n",
       " tensor([[-16.7990,  -0.1975,  -6.1070,  ..., -16.0057, -15.8654, -16.2318]],\n",
       "        device='cuda:0', grad_fn=<LogSoftmaxBackward>),\n",
       " tensor([[-18.0081,  -0.1076,  -6.3869,  ..., -17.2811, -17.0340, -17.3199]],\n",
       "        device='cuda:0', grad_fn=<LogSoftmaxBackward>),\n",
       " tensor([[-16.2208,  -0.1951,  -4.9932,  ..., -15.4754, -15.2698, -15.5732]],\n",
       "        device='cuda:0', grad_fn=<LogSoftmaxBackward>),\n",
       " tensor([[-15.4164,  -0.8780,  -5.4768,  ..., -15.4942, -15.2103, -15.1226]],\n",
       "        device='cuda:0', grad_fn=<LogSoftmaxBackward>),\n",
       " tensor([[-16.9052,  -0.1260,  -6.1357,  ..., -16.3252, -16.0989, -16.2312]],\n",
       "        device='cuda:0', grad_fn=<LogSoftmaxBackward>),\n",
       " tensor([[-15.8256,  -3.6300,  -5.3100,  ..., -15.4626, -15.4044, -15.2826]],\n",
       "        device='cuda:0', grad_fn=<LogSoftmaxBackward>),\n",
       " tensor([[-12.2083,  -3.6997,  -4.2505,  ..., -11.5205, -11.2137, -11.8334]],\n",
       "        device='cuda:0', grad_fn=<LogSoftmaxBackward>),\n",
       " tensor([[-17.5158,  -0.1818,  -5.9906,  ..., -16.6457, -16.4541, -16.8168]],\n",
       "        device='cuda:0', grad_fn=<LogSoftmaxBackward>),\n",
       " tensor([[-15.4212,  -0.8027,  -3.7220,  ..., -14.4826, -14.2596, -14.7043]],\n",
       "        device='cuda:0', grad_fn=<LogSoftmaxBackward>),\n",
       " tensor([[-14.8830,  -1.5858,  -3.4155,  ..., -13.7656, -13.6936, -14.1023]],\n",
       "        device='cuda:0', grad_fn=<LogSoftmaxBackward>),\n",
       " tensor([[-14.7182,  -1.8295,  -5.2530,  ..., -14.1413, -14.0196, -14.0548]],\n",
       "        device='cuda:0', grad_fn=<LogSoftmaxBackward>),\n",
       " tensor([[-17.2452,  -0.1474,  -5.6995,  ..., -16.3533, -16.3321, -16.5139]],\n",
       "        device='cuda:0', grad_fn=<LogSoftmaxBackward>),\n",
       " tensor([[-15.9209,  -1.4404,  -5.6876,  ..., -15.3401, -15.2630, -15.3802]],\n",
       "        device='cuda:0', grad_fn=<LogSoftmaxBackward>),\n",
       " tensor([[-12.1315,  -1.7827,  -4.3841,  ..., -11.5777, -11.7725, -11.6417]],\n",
       "        device='cuda:0', grad_fn=<LogSoftmaxBackward>),\n",
       " tensor([[-17.8178,  -0.2047,  -6.7943,  ..., -16.8325, -16.8370, -17.1977]],\n",
       "        device='cuda:0', grad_fn=<LogSoftmaxBackward>),\n",
       " tensor([[-14.7721,  -1.3519,  -3.3676,  ..., -13.7310, -13.5889, -14.0155]],\n",
       "        device='cuda:0', grad_fn=<LogSoftmaxBackward>),\n",
       " tensor([[-17.6672,  -0.0792,  -6.7188,  ..., -16.8242, -16.8764, -16.9004]],\n",
       "        device='cuda:0', grad_fn=<LogSoftmaxBackward>),\n",
       " tensor([[-14.6913,  -2.8244,  -3.3468,  ..., -14.4438, -14.1340, -14.1956]],\n",
       "        device='cuda:0', grad_fn=<LogSoftmaxBackward>),\n",
       " tensor([[-14.6062,  -1.5654,  -3.0999,  ..., -13.5309, -13.4155, -13.7581]],\n",
       "        device='cuda:0', grad_fn=<LogSoftmaxBackward>),\n",
       " tensor([[-13.5521,  -2.0667,  -3.0298,  ..., -13.3048, -12.9588, -13.1921]],\n",
       "        device='cuda:0', grad_fn=<LogSoftmaxBackward>),\n",
       " tensor([[-17.4411,  -0.0671,  -6.4027,  ..., -16.6948, -16.6161, -16.6968]],\n",
       "        device='cuda:0', grad_fn=<LogSoftmaxBackward>),\n",
       " tensor([[-13.5406,  -6.0774,  -4.2958,  ..., -12.8758, -13.1917, -13.1460]],\n",
       "        device='cuda:0', grad_fn=<LogSoftmaxBackward>),\n",
       " tensor([[-17.0324,  -0.1569,  -5.6467,  ..., -16.1154, -16.1273, -16.3126]],\n",
       "        device='cuda:0', grad_fn=<LogSoftmaxBackward>),\n",
       " tensor([[-14.6044,  -1.3733,  -3.0933,  ..., -13.6815, -13.4910, -13.8918]],\n",
       "        device='cuda:0', grad_fn=<LogSoftmaxBackward>),\n",
       " tensor([[-14.4772,  -1.3062,  -3.6007,  ..., -13.5188, -13.4308, -13.8299]],\n",
       "        device='cuda:0', grad_fn=<LogSoftmaxBackward>),\n",
       " tensor([[-18.1922,  -0.1502,  -7.1134,  ..., -17.2467, -17.2221, -17.5119]],\n",
       "        device='cuda:0', grad_fn=<LogSoftmaxBackward>),\n",
       " tensor([[-14.8207,  -1.1773,  -3.3822,  ..., -13.9137, -13.7193, -14.1376]],\n",
       "        device='cuda:0', grad_fn=<LogSoftmaxBackward>),\n",
       " tensor([[-14.7219,  -1.1384,  -3.4367,  ..., -13.6823, -13.4637, -13.9565]],\n",
       "        device='cuda:0', grad_fn=<LogSoftmaxBackward>),\n",
       " tensor([[-15.7724,  -0.5291,  -4.8407,  ..., -15.0033, -14.7995, -15.2933]],\n",
       "        device='cuda:0', grad_fn=<LogSoftmaxBackward>),\n",
       " tensor([[-15.3900,  -0.9866,  -3.3895,  ..., -14.3311, -14.2136, -14.7270]],\n",
       "        device='cuda:0', grad_fn=<LogSoftmaxBackward>),\n",
       " tensor([[-15.4682,  -0.9870,  -3.2590,  ..., -14.4623, -14.3459, -14.7394]],\n",
       "        device='cuda:0', grad_fn=<LogSoftmaxBackward>),\n",
       " tensor([[-14.8454,  -1.4459,  -3.4430,  ..., -14.4365, -14.0138, -14.1608]],\n",
       "        device='cuda:0', grad_fn=<LogSoftmaxBackward>),\n",
       " tensor([[-15.1730,  -0.9201,  -5.9161,  ..., -14.7455, -14.6930, -14.5205]],\n",
       "        device='cuda:0', grad_fn=<LogSoftmaxBackward>),\n",
       " tensor([[-18.1490,  -0.0588,  -7.0549,  ..., -17.2688, -17.1465, -17.2142]],\n",
       "        device='cuda:0', grad_fn=<LogSoftmaxBackward>),\n",
       " tensor([[-15.0443,  -0.8882,  -3.4573,  ..., -14.1478, -13.8154, -14.3746]],\n",
       "        device='cuda:0', grad_fn=<LogSoftmaxBackward>),\n",
       " tensor([[-14.1405,  -4.4384,  -4.1382,  ..., -13.2174, -13.2094, -13.8083]],\n",
       "        device='cuda:0', grad_fn=<LogSoftmaxBackward>),\n",
       " tensor([[-15.1638,  -0.7672,  -3.6813,  ..., -14.2734, -14.0255, -14.5443]],\n",
       "        device='cuda:0', grad_fn=<LogSoftmaxBackward>),\n",
       " tensor([[-15.6248,  -0.8090,  -3.4454,  ..., -14.6883, -14.3717, -14.9054]],\n",
       "        device='cuda:0', grad_fn=<LogSoftmaxBackward>),\n",
       " tensor([[-13.1855,  -2.4622,  -3.6987,  ..., -12.4650, -12.6025, -12.7797]],\n",
       "        device='cuda:0', grad_fn=<LogSoftmaxBackward>),\n",
       " tensor([[-15.3503,  -0.7649,  -3.4605,  ..., -14.4302, -14.1268, -14.6263]],\n",
       "        device='cuda:0', grad_fn=<LogSoftmaxBackward>),\n",
       " tensor([[-13.8846,  -1.0690,  -4.7371,  ..., -13.3517, -13.1145, -13.2967]],\n",
       "        device='cuda:0', grad_fn=<LogSoftmaxBackward>),\n",
       " tensor([[-13.1154,  -3.1720,  -3.7135,  ..., -12.4733, -12.6044, -12.7515]],\n",
       "        device='cuda:0', grad_fn=<LogSoftmaxBackward>),\n",
       " tensor([[-18.4996,  -0.1306,  -6.8132,  ..., -17.7087, -17.5751, -17.7339]],\n",
       "        device='cuda:0', grad_fn=<LogSoftmaxBackward>),\n",
       " tensor([[-18.8276,  -0.0703,  -7.0123,  ..., -17.9710, -17.8804, -17.9518]],\n",
       "        device='cuda:0', grad_fn=<LogSoftmaxBackward>),\n",
       " tensor([[-14.8312,  -0.9118,  -3.2791,  ..., -13.8445, -13.5172, -14.0126]],\n",
       "        device='cuda:0', grad_fn=<LogSoftmaxBackward>),\n",
       " tensor([[-17.2022,  -0.7285,  -5.4916,  ..., -16.6127, -16.5211, -16.3590]],\n",
       "        device='cuda:0', grad_fn=<LogSoftmaxBackward>),\n",
       " tensor([[-16.6162,  -0.9128,  -6.0393,  ..., -16.1013, -15.7545, -16.0759]],\n",
       "        device='cuda:0', grad_fn=<LogSoftmaxBackward>),\n",
       " tensor([[-18.4413,  -0.0610,  -7.0841,  ..., -17.7204, -17.5171, -17.7432]],\n",
       "        device='cuda:0', grad_fn=<LogSoftmaxBackward>),\n",
       " tensor([[-18.6070,  -0.0822,  -7.1542,  ..., -17.8805, -17.7435, -17.8036]],\n",
       "        device='cuda:0', grad_fn=<LogSoftmaxBackward>),\n",
       " tensor([[-15.1593,  -0.7675,  -3.5634,  ..., -14.3166, -14.0042, -14.6123]],\n",
       "        device='cuda:0', grad_fn=<LogSoftmaxBackward>),\n",
       " tensor([[-13.4687,  -1.3797,  -5.0001,  ..., -12.8543, -12.7790, -12.7924]],\n",
       "        device='cuda:0', grad_fn=<LogSoftmaxBackward>),\n",
       " tensor([[-12.9581,  -5.7685,  -4.5310,  ..., -12.4018, -12.6057, -12.6772]],\n",
       "        device='cuda:0', grad_fn=<LogSoftmaxBackward>),\n",
       " tensor([[-18.0571,  -1.7768,  -4.2758,  ..., -17.4074, -17.2826, -17.3046]],\n",
       "        device='cuda:0', grad_fn=<LogSoftmaxBackward>),\n",
       " tensor([[-13.0272,  -2.6433,  -4.0806,  ..., -12.3663, -12.8624, -12.6631]],\n",
       "        device='cuda:0', grad_fn=<LogSoftmaxBackward>),\n",
       " tensor([[-12.4036,  -2.5769,  -3.8459,  ..., -11.7729, -11.9025, -12.0681]],\n",
       "        device='cuda:0', grad_fn=<LogSoftmaxBackward>),\n",
       " tensor([[-15.5771,  -2.5025,  -5.9880,  ..., -15.4575, -15.5254, -15.5010]],\n",
       "        device='cuda:0', grad_fn=<LogSoftmaxBackward>),\n",
       " tensor([[-15.5484,  -0.7194,  -3.5923,  ..., -14.7057, -14.3136, -14.9058]],\n",
       "        device='cuda:0', grad_fn=<LogSoftmaxBackward>),\n",
       " ...]"
      ]
     },
     "execution_count": 26,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "y_pred"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 27,
   "id": "d493d733-3200-40d2-90f7-8c89ec0e1b8d",
   "metadata": {},
   "outputs": [],
   "source": [
    "def evaluate(encoder, decoder, sentence, max_length=args.max_len):\n",
    "    \n",
    "    with torch.no_grad():\n",
    "        input_tensor = tensorFromSentence(input_lang, sentence)\n",
    "        input_length = input_tensor.size()[0]\n",
    "        encoder_hidden = encoder.initHidden()\n",
    "\n",
    "        encoder_outputs = torch.zeros(max_length, encoder.hidden_size, device=device)\n",
    "\n",
    "        for ei in range(input_length):\n",
    "            encoder_output, encoder_hidden = encoder(input_tensor[ei],\n",
    "                                                     encoder_hidden)\n",
    "            encoder_outputs[ei] += encoder_output[0, 0]\n",
    "\n",
    "        decoder_input = torch.tensor([[SOS_token]], device=device)  # SOS\n",
    "\n",
    "        decoder_hidden = encoder_hidden\n",
    "\n",
    "        decoded_words = []\n",
    "        decoder_attentions = torch.zeros(max_length, max_length)\n",
    "\n",
    "        for di in range(max_length):\n",
    "            decoder_output, decoder_hidden, decoder_attention = decoder(\n",
    "                decoder_input, decoder_hidden, encoder_outputs)\n",
    "            decoder_attentions[di] = decoder_attention.data\n",
    "            topv, topi = decoder_output.data.topk(1)\n",
    "            if topi.item() == EOS_token:\n",
    "                decoded_words.append('<EOS>')\n",
    "                break\n",
    "            else:\n",
    "                decoded_words.append(output_lang.index2word[topi.item()])\n",
    "\n",
    "            decoder_input = topi.squeeze().detach()\n",
    "\n",
    "        return decoded_words, decoder_attentions[:di + 1]"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "7efb3f52-bf51-49d1-b3ed-f9a4dc074455",
   "metadata": {},
   "source": [
    "### matplotlib로 학습 중에 저장된 손실 값 plot_losses 의 배열을 사용하여 도식화"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 28,
   "id": "fab79b5e-7e2c-4f05-9c67-63dbd6f5b611",
   "metadata": {},
   "outputs": [],
   "source": [
    "output_words, attentions = evaluate(\n",
    "    encoder1, attn_decoder1,\n",
    "    sentence=\"알겠어\")\n",
    "# plt.matshow(attentions.numpy())"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 29,
   "id": "eb5ca3e3-6cb2-4782-9196-98a2426d27ce",
   "metadata": {
    "scrolled": true,
    "tags": []
   },
   "outputs": [
    {
     "data": {
      "text/plain": [
       "['알', '겠', '어', '.', '<EOS>']"
      ]
     },
     "execution_count": 29,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "output_words"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "0ed1699a-a5ec-44ce-9375-96f63a3ad7f7",
   "metadata": {},
   "source": [
    "### 학습 세트에 있는 임의의 문장을 평가하고 입력, 목표 및 출력을 출력하여 주관적인 품질 판단"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 30,
   "id": "2a14d61a-e7af-4d7d-9939-7a510910dc26",
   "metadata": {},
   "outputs": [],
   "source": [
    "def evaluateRandomly(encoder, decoder, n=10):\n",
    "    for i in range(n):\n",
    "        pair = random.choice(pairs)\n",
    "        print('form ->', pair[0])\n",
    "        print('corrected form ->', pair[1])\n",
    "        output_words, attentions = evaluate(encoder, decoder, pair[0])\n",
    "        # print('output_words: ',output_words)\n",
    "        output_sentence = ''.join(output_words)\n",
    "        output_sentence = output_sentence.replace('<EOS>','')\n",
    "        # 띄어쓰기 교정기\n",
    "        # spelled_sent = spell_checker.check(output_sentence)\n",
    "        # hanspell_sent = spelled_sent.checked\n",
    "        # print('output ->', hanspell_sent)\n",
    "        \n",
    "        print('output ->', output_sentence)\n",
    "\n",
    "        print('')"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 31,
   "id": "4ae933ba-f793-442d-87f3-74680943a7ef",
   "metadata": {
    "tags": []
   },
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "form -> 아휴 아왜 그걸 불로 더 신난 노래도 많은데\n",
      "corrected form -> 아휴. 아, 왜 그걸 불러, 더 신나는 노래도 많은데.\n",
      "output -> 아휴 아  걸 불   불더 신신 노래도 많은데\n",
      "\n",
      "form -> 맥폭 사라 진짜\n",
      "corrected form -> 맥북 사라 진짜.\n",
      "output -> 맥빈 사람  짜짜\n",
      "\n",
      "form -> 응 살찌겠네 더 아니야 폭식 안 하는 하는 것보다 이게 밥 먹는 게 날 거야\n",
      "corrected form -> 응. 살찌겠네 더. 아냐 폭식 안 하는 하는 것보다 이게 밥 먹는 게 날 거야.\n",
      "output -> 응. 살찌겠네. 더 아니야  식 안 하는 것 는 하는 것  게         거야.\n",
      "\n",
      "form -> 초밥\n",
      "corrected form -> 초밥\n",
      "output -> 초밥밥\n",
      "\n",
      "form -> 자기 과거 얘기 한 것처럼 쫌 진짜 좀 대단한 일이 맛있었는데\n",
      "corrected form -> 자기 과거 얘기에 한 거처럼 좀 진짜 좀 대단한 일이 마 있었는데\n",
      "output -> 자기 과거 얘기 한처 것처럼 쫌       단     있는데데데데\n",
      "\n",
      "form -> 어떤 얘긴지 알 거 같애 잘 잘 깠다는 얘기구나\n",
      "corrected form -> 어떤 얘긴지 알 거 같애. 잘 잘 깠다는 얘기구나\n",
      "output -> 어떤 얘기지 알 거 같애. 잘 잘 다 잘  다 \n",
      "\n",
      "form -> 무슨 골짜이 뭐였는데\n",
      "corrected form -> 무슨 골짜기 뭐였는데\n",
      "output -> 무슨 골짜이 뭐였는데\n",
      "\n",
      "form -> 전화할 때 어쩐지 소리가 안 들리더라고 그래서 그거 때매 십만 원밖에 못 받았어\n",
      "corrected form -> 전화할 때 어쩐지 소리가 안 들리더라고. 그래서 그것 때매 만원 밖에 못 받았어.\n",
      "output -> 전화할 때 어 지 지소 안 들들이라고 그래서 그거 때 때 때  매받았어.\n",
      "\n",
      "form -> 그 애들한테 막 애들 앞에서는 안 하고 막 깨 앞 깨한테 니 나 너 나덕보에 공교 올라간 걸 내가 수비 내려와서 그니까 막 애들 앞에서는\n",
      "corrected form -> 그 애들한테 막 애들 앞에선 안 하고 막 꺠 앞, 꺠한테 니 나, 너 나 덕분에 공격 올라간 거라고, 내가 수비로 내려가서 그러니까 막 애들 앞에서는\n",
      "output -> 그 애들한테 막 애들 앞에서 안 하고 막 깨고 막 나 나 나 나 나 나 나 나 나 내가 수 올올  라                    .\n",
      "\n",
      "form -> 아 근데 진짜 그런 그런 애들 은근 많더라 하근데\n",
      "corrected form -> 아니 진짜 그런 그런 애들 은근 많더라 근데.\n",
      "output -> 아 근데 진짜 그런 애들 은들은 많 은더라라하하하\n",
      "\n"
     ]
    }
   ],
   "source": [
    "evaluateRandomly(encoder1, attn_decoder1)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 32,
   "id": "f82ceb25-4d76-4123-8b08-f76ed82d8094",
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "이제 어쩌면 좋죠?\n"
     ]
    }
   ],
   "source": [
    "from hanspell import spell_checker\n",
    "\n",
    "sent = \"이제어쩌면좋죠?\"\n",
    "spelled_sent = spell_checker.check(sent)\n",
    "hanspell_sent = spelled_sent.checked\n",
    "print(hanspell_sent)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 33,
   "id": "1d1a5d36-ab15-4615-96cb-5549abbb01b7",
   "metadata": {},
   "outputs": [
    {
     "data": {
      "text/plain": [
       "'알겠어'"
      ]
     },
     "execution_count": 33,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "a=['알', '겠', '어', '<EOS>']\n",
    "a=''.join(a)\n",
    "a=a.replace('<EOS>','')\n",
    "a"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 34,
   "id": "6de47120-3e32-4587-9405-fe24e6b90249",
   "metadata": {
    "scrolled": true,
    "tags": []
   },
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "input = 이제 어쩌면 좋죠\n",
      "output = 이제 어쩌면 좋죠.\n"
     ]
    },
    {
     "data": {
      "text/plain": [
       "'이제 어쩌면 좋죠.'"
      ]
     },
     "execution_count": 34,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "def evaluateAndShowAttention(input_sentence):\n",
    "    output_words, attentions = evaluate(\n",
    "        encoder1, attn_decoder1, input_sentence)\n",
    "    \n",
    "    output_words=''.join(output_words)\n",
    "    output_words = output_words.replace('<EOS>','')\n",
    "    print('input =', input_sentence)\n",
    "    print('output =', output_words)\n",
    "    # showAttention(input_sentence, output_words, attentions)\n",
    "    \n",
    "    return output_words\n",
    "    \n",
    "evaluateAndShowAttention(\"이제 어쩌면 좋죠\")\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 35,
   "id": "bf1ca16f-f7d3-4e24-96f1-fdd08be2d345",
   "metadata": {},
   "outputs": [
    {
     "data": {
      "text/html": [
       "<div>\n",
       "<style scoped>\n",
       "    .dataframe tbody tr th:only-of-type {\n",
       "        vertical-align: middle;\n",
       "    }\n",
       "\n",
       "    .dataframe tbody tr th {\n",
       "        vertical-align: top;\n",
       "    }\n",
       "\n",
       "    .dataframe thead th {\n",
       "        text-align: right;\n",
       "    }\n",
       "</style>\n",
       "<table border=\"1\" class=\"dataframe\">\n",
       "  <thead>\n",
       "    <tr style=\"text-align: right;\">\n",
       "      <th></th>\n",
       "      <th>form</th>\n",
       "      <th>corrected_form</th>\n",
       "      <th>t5_form</th>\n",
       "    </tr>\n",
       "  </thead>\n",
       "  <tbody>\n",
       "    <tr>\n",
       "      <th>0</th>\n",
       "      <td>기존점을 쓰고 그것에 관한 관점인가 감상인가 아닌가 이런 걸 구별하는 연습도 해보셔...</td>\n",
       "      <td>기준점을 주고 그것에 관한 관점인가 감상인가 아닌가 이런 걸 구별하는 연습도 해 보...</td>\n",
       "      <td>기존점을 쓰고 그것에 관한 관점인가 감상인가 아닌가 이런 걸 구별하는 연습도 해보셔...</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>1</th>\n",
       "      <td>킬부도 시원을 하면 안 돼요</td>\n",
       "      <td>필부도 시건을 하면 안 돼요.</td>\n",
       "      <td>킬부도 시원을 하면 안 돼요?</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>2</th>\n",
       "      <td>현강의 문이 발짝 열셨습니다 서 일 이 강 얻어셨나요 복수를 잘 하셨나요 저 일강을...</td>\n",
       "      <td>삼강의 문이 활짝 열렸습니다. 자 일 이강 어떠셨나요 복습 잘 하셨나요 자 일강을 ...</td>\n",
       "      <td>현강의 문이 발짝 열셨습니다. 서 일 이 강 얻어셨나요? 복수를 잘 하셨나요? 저 ...</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>3</th>\n",
       "      <td>화자가 누군가에게 말을 걷낸다는건 내 말을 들어주는 사람이 있는 거죠 그래서 이걸 ...</td>\n",
       "      <td>화자가 누군가에게 말을 건넨다는 건 내 말을 들어주는 사람이 있는 거죠 그래서 이걸...</td>\n",
       "      <td>화자가 누군가에게 말을 걷낸다는 건 내 말을 들어주는 사람이 있는 거죠. 그래서 이...</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>4</th>\n",
       "      <td>길게 써있는데요 아까 제목에서 봤던 거가 굉장히 비슷한 내용기죠 새</td>\n",
       "      <td>길게 쓰여있는데요. 아까 제목에서 봤던 것과 굉장히 비슷한 내용이죠 세.</td>\n",
       "      <td>길게 써있는데요. 아까 제목에서 봤던 거가 굉장히 비슷한 내용이죠. 세</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>5</th>\n",
       "      <td>계속 여러 번 나오고 있어요 이거 쉬 안 좋은 문제점이거든요 그것 을 보안하기위에 ...</td>\n",
       "      <td>계속 여러 번 나오고 있어요. 이것이 안 좋은 문제점이거든요. 그것을 보완하기 위해...</td>\n",
       "      <td>계속 여러 번 나오고 있어요. 이거 쉬 안 좋은 문제점이거든요. 그것 을 보완하기 ...</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>6</th>\n",
       "      <td>그다음 기자 기자의 질문 어린 석은 사람을 높이 평가이 이것을 긍정적으로 바라봤죠 ...</td>\n",
       "      <td>그다음 기자 기자의 질문 어리석은 사람을 높이 평가한 이유 이것을 긍정적으로 바라봤...</td>\n",
       "      <td>그다음 기자? 기자의 질문. 어린 석은 사람을 높이 평가. 이것을 긍정적으로 바라봤...</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>7</th>\n",
       "      <td>자 반달리즘에 경향을 어떻게 이해한다고 했죠 그래서 뭔가 기존의 것에서 벗어나려고 ...</td>\n",
       "      <td>자 반달리즘의 경향을 어떻게 이해한다고 했죠 그렇죠 뭔가 기존의 것에서 벗어나려고 ...</td>\n",
       "      <td>자 반달리즘에 경향을 어떻게 이해한다고 했죠? 그래서 뭔가 기존의 것에서 벗어나려고...</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>8</th>\n",
       "      <td>칼처럼 비정한 것이기 때문입니다 지문에 있었던 내용 그대로 가지고 왔거든요 적절하구요</td>\n",
       "      <td>칼처럼 비정한 것이기 때문입니다. 지문에 있었던 내용 그대로 가지고 왔거든요 적절하고요.</td>\n",
       "      <td>칼처럼 비정한 것이기 때문입니다. 지문에 있었던 내용 그대로 가지고 왔거든요. 적절...</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>9</th>\n",
       "      <td>교과서에 나와 있는 지문이 이것보다 훨씬 길고 이 앞에 작품이 하나 더 있어요 그죠</td>\n",
       "      <td>교과서에 나와 있는 지문이 이것보다 훨씬 길고 이 앞에 작품이 하나 더 있어요 그죠.</td>\n",
       "      <td>교과서에 나와 있는 지문이 이것보다 훨씬 길고 이 앞에 작품이 하나 더 있어요. 그죠?</td>\n",
       "    </tr>\n",
       "  </tbody>\n",
       "</table>\n",
       "</div>"
      ],
      "text/plain": [
       "                                                form  \\\n",
       "0  기존점을 쓰고 그것에 관한 관점인가 감상인가 아닌가 이런 걸 구별하는 연습도 해보셔...   \n",
       "1                                    킬부도 시원을 하면 안 돼요   \n",
       "2  현강의 문이 발짝 열셨습니다 서 일 이 강 얻어셨나요 복수를 잘 하셨나요 저 일강을...   \n",
       "3  화자가 누군가에게 말을 걷낸다는건 내 말을 들어주는 사람이 있는 거죠 그래서 이걸 ...   \n",
       "4              길게 써있는데요 아까 제목에서 봤던 거가 굉장히 비슷한 내용기죠 새   \n",
       "5  계속 여러 번 나오고 있어요 이거 쉬 안 좋은 문제점이거든요 그것 을 보안하기위에 ...   \n",
       "6  그다음 기자 기자의 질문 어린 석은 사람을 높이 평가이 이것을 긍정적으로 바라봤죠 ...   \n",
       "7  자 반달리즘에 경향을 어떻게 이해한다고 했죠 그래서 뭔가 기존의 것에서 벗어나려고 ...   \n",
       "8    칼처럼 비정한 것이기 때문입니다 지문에 있었던 내용 그대로 가지고 왔거든요 적절하구요   \n",
       "9     교과서에 나와 있는 지문이 이것보다 훨씬 길고 이 앞에 작품이 하나 더 있어요 그죠   \n",
       "\n",
       "                                      corrected_form  \\\n",
       "0  기준점을 주고 그것에 관한 관점인가 감상인가 아닌가 이런 걸 구별하는 연습도 해 보...   \n",
       "1                                   필부도 시건을 하면 안 돼요.   \n",
       "2  삼강의 문이 활짝 열렸습니다. 자 일 이강 어떠셨나요 복습 잘 하셨나요 자 일강을 ...   \n",
       "3  화자가 누군가에게 말을 건넨다는 건 내 말을 들어주는 사람이 있는 거죠 그래서 이걸...   \n",
       "4           길게 쓰여있는데요. 아까 제목에서 봤던 것과 굉장히 비슷한 내용이죠 세.   \n",
       "5  계속 여러 번 나오고 있어요. 이것이 안 좋은 문제점이거든요. 그것을 보완하기 위해...   \n",
       "6  그다음 기자 기자의 질문 어리석은 사람을 높이 평가한 이유 이것을 긍정적으로 바라봤...   \n",
       "7  자 반달리즘의 경향을 어떻게 이해한다고 했죠 그렇죠 뭔가 기존의 것에서 벗어나려고 ...   \n",
       "8  칼처럼 비정한 것이기 때문입니다. 지문에 있었던 내용 그대로 가지고 왔거든요 적절하고요.   \n",
       "9    교과서에 나와 있는 지문이 이것보다 훨씬 길고 이 앞에 작품이 하나 더 있어요 그죠.   \n",
       "\n",
       "                                             t5_form  \n",
       "0  기존점을 쓰고 그것에 관한 관점인가 감상인가 아닌가 이런 걸 구별하는 연습도 해보셔...  \n",
       "1                                   킬부도 시원을 하면 안 돼요?  \n",
       "2  현강의 문이 발짝 열셨습니다. 서 일 이 강 얻어셨나요? 복수를 잘 하셨나요? 저 ...  \n",
       "3  화자가 누군가에게 말을 걷낸다는 건 내 말을 들어주는 사람이 있는 거죠. 그래서 이...  \n",
       "4            길게 써있는데요. 아까 제목에서 봤던 거가 굉장히 비슷한 내용이죠. 세  \n",
       "5  계속 여러 번 나오고 있어요. 이거 쉬 안 좋은 문제점이거든요. 그것 을 보완하기 ...  \n",
       "6  그다음 기자? 기자의 질문. 어린 석은 사람을 높이 평가. 이것을 긍정적으로 바라봤...  \n",
       "7  자 반달리즘에 경향을 어떻게 이해한다고 했죠? 그래서 뭔가 기존의 것에서 벗어나려고...  \n",
       "8  칼처럼 비정한 것이기 때문입니다. 지문에 있었던 내용 그대로 가지고 왔거든요. 적절...  \n",
       "9   교과서에 나와 있는 지문이 이것보다 훨씬 길고 이 앞에 작품이 하나 더 있어요. 그죠?  "
      ]
     },
     "execution_count": 35,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "dd=pd.read_csv('s-kr/fine-tune/dataset/ktalk_results/gec_ktalk_result.csv',\n",
    "               encoding='cp949', index_col='Unnamed: 0')\n",
    "\n",
    "text=dd.head(10)\n",
    "text"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 36,
   "id": "a5c23291-eed9-41fe-b92f-46f20905789d",
   "metadata": {
    "scrolled": true,
    "tags": []
   },
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "input = 기존점을 쓰고 그것에 관한 관점인가 감상인가 아닌가 이런 걸 구별하는 연습도 해보셔야 합니다\n",
      "output = 기존점을 쓰고 그것에 관 인 점 관 인 인 인                         니\n",
      "input = 킬부도 시원을 하면 안 돼요\n",
      "output = 깔부도 시원을 하면 안 돼.\n",
      "input = 현강의 문이 발짝 열셨습니다 서 일 이 강 얻어셨나요 복수를 잘 하셨나요 저 일강을 통해서요 우리 언어에 성격에 대해서 배웠어요 서 오늘 이 시간에는요 일 이강에서 언어에 성격과 그 문제 적금까지 해결했다면 언어에서 좀 더 좁혀서 한국어 국어에 특질은 어떤 게 있는지 같이 살펴보도록 하겠습니다 저 살펴보기 전에 항상 운리\n",
      "output = 현강의 문이 발컬 열셨습 다 서 일                                                                                                                                                             .\n",
      "input = 화자가 누군가에게 말을 걷낸다는건 내 말을 들어주는 사람이 있는 거죠 그래서 이걸 바꾸 말하면 총\n",
      "output = 화자가 누군가 게 말을    는 는                                  .\n",
      "input = 길게 써있는데요 아까 제목에서 봤던 거가 굉장히 비슷한 내용기죠 새\n",
      "output = 길게 써 는 요 까                           새새\n",
      "input = 계속 여러 번 나오고 있어요 이거 쉬 안 좋은 문제점이거든요 그것 을 보안하기위에 이 정도로 반복하는데도 못 알아들면 안 돼요\n",
      "output = 계속 여러 나오고 있어어                      거                                 안\n",
      "input = 그다음 기자 기자의 질문 어린 석은 사람을 높이 평가이 이것을 긍정적으로 바라봤죠 그 이후로 질문했어요 어린 석은 사람이신인 무직함이 세상을 조\n",
      "output = 그다음 기자 기자          사                                                            .\n",
      "input = 자 반달리즘에 경향을 어떻게 이해한다고 했죠 그래서 뭔가 기존의 것에서 벗어나려고 하는 새로운 것을 추구하려고 하는 경향을 고려한다면 어떻게 돼요 차\n",
      "output = 자 반 리  경  향을 어  별                                하는  하는  하고                        .어.\n",
      "input = 칼처럼 비정한 것이기 때문입니다 지문에 있었던 내용 그대로 가지고 왔거든요 적절하구요\n",
      "output = 칼처럼 비정한 것이기 때문이 다                             요\n",
      "input = 교과서에 나와 있는 지문이 이것보다 훨씬 길고 이 앞에 작품이 하나 더 있어요 그죠\n",
      "output = 교과서에 나와 있는 지문이 이것 다 앞 길 다                    .어\n"
     ]
    },
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "/tmp/ipykernel_23979/2646065310.py:1: SettingWithCopyWarning: \n",
      "A value is trying to be set on a copy of a slice from a DataFrame.\n",
      "Try using .loc[row_indexer,col_indexer] = value instead\n",
      "\n",
      "See the caveats in the documentation: http://pandas.pydata.org/pandas-docs/stable/user_guide/indexing.html#returning-a-view-versus-a-copy\n",
      "  text['lstm_bada']=list(map(lambda x:evaluateAndShowAttention(x), text['form']))\n"
     ]
    },
    {
     "data": {
      "text/html": [
       "<div>\n",
       "<style scoped>\n",
       "    .dataframe tbody tr th:only-of-type {\n",
       "        vertical-align: middle;\n",
       "    }\n",
       "\n",
       "    .dataframe tbody tr th {\n",
       "        vertical-align: top;\n",
       "    }\n",
       "\n",
       "    .dataframe thead th {\n",
       "        text-align: right;\n",
       "    }\n",
       "</style>\n",
       "<table border=\"1\" class=\"dataframe\">\n",
       "  <thead>\n",
       "    <tr style=\"text-align: right;\">\n",
       "      <th></th>\n",
       "      <th>form</th>\n",
       "      <th>corrected_form</th>\n",
       "      <th>t5_form</th>\n",
       "      <th>lstm_bada</th>\n",
       "    </tr>\n",
       "  </thead>\n",
       "  <tbody>\n",
       "    <tr>\n",
       "      <th>0</th>\n",
       "      <td>기존점을 쓰고 그것에 관한 관점인가 감상인가 아닌가 이런 걸 구별하는 연습도 해보셔...</td>\n",
       "      <td>기준점을 주고 그것에 관한 관점인가 감상인가 아닌가 이런 걸 구별하는 연습도 해 보...</td>\n",
       "      <td>기존점을 쓰고 그것에 관한 관점인가 감상인가 아닌가 이런 걸 구별하는 연습도 해보셔...</td>\n",
       "      <td>기존점을 쓰고 그것에 관 인 점 관 인 인 인                     ...</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>1</th>\n",
       "      <td>킬부도 시원을 하면 안 돼요</td>\n",
       "      <td>필부도 시건을 하면 안 돼요.</td>\n",
       "      <td>킬부도 시원을 하면 안 돼요?</td>\n",
       "      <td>깔부도 시원을 하면 안 돼.</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>2</th>\n",
       "      <td>현강의 문이 발짝 열셨습니다 서 일 이 강 얻어셨나요 복수를 잘 하셨나요 저 일강을...</td>\n",
       "      <td>삼강의 문이 활짝 열렸습니다. 자 일 이강 어떠셨나요 복습 잘 하셨나요 자 일강을 ...</td>\n",
       "      <td>현강의 문이 발짝 열셨습니다. 서 일 이 강 얻어셨나요? 복수를 잘 하셨나요? 저 ...</td>\n",
       "      <td>현강의 문이 발컬 열셨습 다 서 일                           ...</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>3</th>\n",
       "      <td>화자가 누군가에게 말을 걷낸다는건 내 말을 들어주는 사람이 있는 거죠 그래서 이걸 ...</td>\n",
       "      <td>화자가 누군가에게 말을 건넨다는 건 내 말을 들어주는 사람이 있는 거죠 그래서 이걸...</td>\n",
       "      <td>화자가 누군가에게 말을 걷낸다는 건 내 말을 들어주는 사람이 있는 거죠. 그래서 이...</td>\n",
       "      <td>화자가 누군가 게 말을    는 는                           ...</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>4</th>\n",
       "      <td>길게 써있는데요 아까 제목에서 봤던 거가 굉장히 비슷한 내용기죠 새</td>\n",
       "      <td>길게 쓰여있는데요. 아까 제목에서 봤던 것과 굉장히 비슷한 내용이죠 세.</td>\n",
       "      <td>길게 써있는데요. 아까 제목에서 봤던 거가 굉장히 비슷한 내용이죠. 세</td>\n",
       "      <td>길게 써 는 요 까                           새새</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>5</th>\n",
       "      <td>계속 여러 번 나오고 있어요 이거 쉬 안 좋은 문제점이거든요 그것 을 보안하기위에 ...</td>\n",
       "      <td>계속 여러 번 나오고 있어요. 이것이 안 좋은 문제점이거든요. 그것을 보완하기 위해...</td>\n",
       "      <td>계속 여러 번 나오고 있어요. 이거 쉬 안 좋은 문제점이거든요. 그것 을 보완하기 ...</td>\n",
       "      <td>계속 여러 나오고 있어어                      거          ...</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>6</th>\n",
       "      <td>그다음 기자 기자의 질문 어린 석은 사람을 높이 평가이 이것을 긍정적으로 바라봤죠 ...</td>\n",
       "      <td>그다음 기자 기자의 질문 어리석은 사람을 높이 평가한 이유 이것을 긍정적으로 바라봤...</td>\n",
       "      <td>그다음 기자? 기자의 질문. 어린 석은 사람을 높이 평가. 이것을 긍정적으로 바라봤...</td>\n",
       "      <td>그다음 기자 기자          사                          ...</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>7</th>\n",
       "      <td>자 반달리즘에 경향을 어떻게 이해한다고 했죠 그래서 뭔가 기존의 것에서 벗어나려고 ...</td>\n",
       "      <td>자 반달리즘의 경향을 어떻게 이해한다고 했죠 그렇죠 뭔가 기존의 것에서 벗어나려고 ...</td>\n",
       "      <td>자 반달리즘에 경향을 어떻게 이해한다고 했죠? 그래서 뭔가 기존의 것에서 벗어나려고...</td>\n",
       "      <td>자 반 리  경  향을 어  별                             ...</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>8</th>\n",
       "      <td>칼처럼 비정한 것이기 때문입니다 지문에 있었던 내용 그대로 가지고 왔거든요 적절하구요</td>\n",
       "      <td>칼처럼 비정한 것이기 때문입니다. 지문에 있었던 내용 그대로 가지고 왔거든요 적절하고요.</td>\n",
       "      <td>칼처럼 비정한 것이기 때문입니다. 지문에 있었던 내용 그대로 가지고 왔거든요. 적절...</td>\n",
       "      <td>칼처럼 비정한 것이기 때문이 다                             요</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>9</th>\n",
       "      <td>교과서에 나와 있는 지문이 이것보다 훨씬 길고 이 앞에 작품이 하나 더 있어요 그죠</td>\n",
       "      <td>교과서에 나와 있는 지문이 이것보다 훨씬 길고 이 앞에 작품이 하나 더 있어요 그죠.</td>\n",
       "      <td>교과서에 나와 있는 지문이 이것보다 훨씬 길고 이 앞에 작품이 하나 더 있어요. 그죠?</td>\n",
       "      <td>교과서에 나와 있는 지문이 이것 다 앞 길 다                    .어</td>\n",
       "    </tr>\n",
       "  </tbody>\n",
       "</table>\n",
       "</div>"
      ],
      "text/plain": [
       "                                                form  \\\n",
       "0  기존점을 쓰고 그것에 관한 관점인가 감상인가 아닌가 이런 걸 구별하는 연습도 해보셔...   \n",
       "1                                    킬부도 시원을 하면 안 돼요   \n",
       "2  현강의 문이 발짝 열셨습니다 서 일 이 강 얻어셨나요 복수를 잘 하셨나요 저 일강을...   \n",
       "3  화자가 누군가에게 말을 걷낸다는건 내 말을 들어주는 사람이 있는 거죠 그래서 이걸 ...   \n",
       "4              길게 써있는데요 아까 제목에서 봤던 거가 굉장히 비슷한 내용기죠 새   \n",
       "5  계속 여러 번 나오고 있어요 이거 쉬 안 좋은 문제점이거든요 그것 을 보안하기위에 ...   \n",
       "6  그다음 기자 기자의 질문 어린 석은 사람을 높이 평가이 이것을 긍정적으로 바라봤죠 ...   \n",
       "7  자 반달리즘에 경향을 어떻게 이해한다고 했죠 그래서 뭔가 기존의 것에서 벗어나려고 ...   \n",
       "8    칼처럼 비정한 것이기 때문입니다 지문에 있었던 내용 그대로 가지고 왔거든요 적절하구요   \n",
       "9     교과서에 나와 있는 지문이 이것보다 훨씬 길고 이 앞에 작품이 하나 더 있어요 그죠   \n",
       "\n",
       "                                      corrected_form  \\\n",
       "0  기준점을 주고 그것에 관한 관점인가 감상인가 아닌가 이런 걸 구별하는 연습도 해 보...   \n",
       "1                                   필부도 시건을 하면 안 돼요.   \n",
       "2  삼강의 문이 활짝 열렸습니다. 자 일 이강 어떠셨나요 복습 잘 하셨나요 자 일강을 ...   \n",
       "3  화자가 누군가에게 말을 건넨다는 건 내 말을 들어주는 사람이 있는 거죠 그래서 이걸...   \n",
       "4           길게 쓰여있는데요. 아까 제목에서 봤던 것과 굉장히 비슷한 내용이죠 세.   \n",
       "5  계속 여러 번 나오고 있어요. 이것이 안 좋은 문제점이거든요. 그것을 보완하기 위해...   \n",
       "6  그다음 기자 기자의 질문 어리석은 사람을 높이 평가한 이유 이것을 긍정적으로 바라봤...   \n",
       "7  자 반달리즘의 경향을 어떻게 이해한다고 했죠 그렇죠 뭔가 기존의 것에서 벗어나려고 ...   \n",
       "8  칼처럼 비정한 것이기 때문입니다. 지문에 있었던 내용 그대로 가지고 왔거든요 적절하고요.   \n",
       "9    교과서에 나와 있는 지문이 이것보다 훨씬 길고 이 앞에 작품이 하나 더 있어요 그죠.   \n",
       "\n",
       "                                             t5_form  \\\n",
       "0  기존점을 쓰고 그것에 관한 관점인가 감상인가 아닌가 이런 걸 구별하는 연습도 해보셔...   \n",
       "1                                   킬부도 시원을 하면 안 돼요?   \n",
       "2  현강의 문이 발짝 열셨습니다. 서 일 이 강 얻어셨나요? 복수를 잘 하셨나요? 저 ...   \n",
       "3  화자가 누군가에게 말을 걷낸다는 건 내 말을 들어주는 사람이 있는 거죠. 그래서 이...   \n",
       "4            길게 써있는데요. 아까 제목에서 봤던 거가 굉장히 비슷한 내용이죠. 세   \n",
       "5  계속 여러 번 나오고 있어요. 이거 쉬 안 좋은 문제점이거든요. 그것 을 보완하기 ...   \n",
       "6  그다음 기자? 기자의 질문. 어린 석은 사람을 높이 평가. 이것을 긍정적으로 바라봤...   \n",
       "7  자 반달리즘에 경향을 어떻게 이해한다고 했죠? 그래서 뭔가 기존의 것에서 벗어나려고...   \n",
       "8  칼처럼 비정한 것이기 때문입니다. 지문에 있었던 내용 그대로 가지고 왔거든요. 적절...   \n",
       "9   교과서에 나와 있는 지문이 이것보다 훨씬 길고 이 앞에 작품이 하나 더 있어요. 그죠?   \n",
       "\n",
       "                                           lstm_bada  \n",
       "0  기존점을 쓰고 그것에 관 인 점 관 인 인 인                     ...  \n",
       "1                                    깔부도 시원을 하면 안 돼.  \n",
       "2  현강의 문이 발컬 열셨습 다 서 일                           ...  \n",
       "3  화자가 누군가 게 말을    는 는                           ...  \n",
       "4            길게 써 는 요 까                           새새  \n",
       "5  계속 여러 나오고 있어어                      거          ...  \n",
       "6  그다음 기자 기자          사                          ...  \n",
       "7  자 반 리  경  향을 어  별                             ...  \n",
       "8    칼처럼 비정한 것이기 때문이 다                             요  \n",
       "9    교과서에 나와 있는 지문이 이것 다 앞 길 다                    .어  "
      ]
     },
     "execution_count": 36,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "text['lstm_bada']=list(map(lambda x:evaluateAndShowAttention(x), text['form']))\n",
    "text"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "6b78335f-c2d9-41ac-b698-4cc913d11fd7",
   "metadata": {},
   "outputs": [],
   "source": []
  },
  {
   "cell_type": "code",
   "execution_count": 37,
   "id": "c0660f46-35f2-49ff-b4c7-d4a29f0e6055",
   "metadata": {},
   "outputs": [
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "100%|█████████████████████████████████████████████████████████████████████████████████████████████████████████████| 10/10 [00:00<00:00, 10667.10it/s]\n",
      "/tmp/ipykernel_23979/8404822.py:41: SettingWithCopyWarning: \n",
      "A value is trying to be set on a copy of a slice from a DataFrame.\n",
      "Try using .loc[row_indexer,col_indexer] = value instead\n",
      "\n",
      "See the caveats in the documentation: http://pandas.pydata.org/pandas-docs/stable/user_guide/indexing.html#returning-a-view-versus-a-copy\n",
      "  text['acc']=lis\n"
     ]
    }
   ],
   "source": [
    "import Levenshtein as Lev\n",
    "\n",
    "def cal_cer(input_text: str, output_text: str) -> float:\n",
    "    input_text = input_text.replace(' ', '')\n",
    "    output_text = output_text.replace(' ', '')\n",
    "    \n",
    "    dist = Lev.distance(output_text, input_text)\n",
    "    length = len(input_text)\n",
    "    \n",
    "    # assert length==0 ,'시발'\n",
    "    return dist / length\n",
    "\n",
    "\n",
    "# label_data=['기준점을 주고 그것에 관한 관점인가 감상인가 아닌가 이런 걸 구별하는 연습도 해 보셔야 합니다.']\n",
    "# results=['기존점을 쓰고 그것에 관한 관점인가 감상인가 아닌가 이런 걸 구별하는 연습도 해보셔야 합니다.']\n",
    "\n",
    "\n",
    "# cer 측정\n",
    "def cal_acc(results:list, label_data:list) -> float:\n",
    "    total_cer = 0.0\n",
    "    for naver_data, answer_data in zip(results, label_data):\n",
    "        cer = cal_cer(answer_data, naver_data)\n",
    "        total_cer += cer\n",
    "\n",
    "    result_cer = total_cer / len(results)\n",
    "    acc=(1-result_cer) * 100\n",
    "    return round(acc,2)\n",
    "\n",
    "# print(\"Test CER: {:.3f}\".format(result_cer))\n",
    "# print(\"Test ACC: {:.3f}\".format((1-result_cer) * 100))\n",
    "# print(cal_acc(results, label_data))\n",
    "\n",
    "lis=[]\n",
    "for i in trange(len(text)):\n",
    "    \n",
    "    label_data=[text['corrected_form'][i]]\n",
    "    results=[text['lstm_bada'][i]]\n",
    "    # print('1 :',label_data,results)\n",
    "    lis.append(cal_acc(results, label_data))\n",
    "    # print('2:',lis)\n",
    "text['acc']=lis"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 38,
   "id": "72022db3-439d-4c6d-ae8c-789d0d2fc03b",
   "metadata": {},
   "outputs": [],
   "source": [
    "text.to_csv(f'./장표정리_list2.tsv',encoding='utf-8',sep='\\t')"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 39,
   "id": "d6ddd580-0654-45be-a301-ced08169ff69",
   "metadata": {},
   "outputs": [
    {
     "data": {
      "text/html": [
       "<div>\n",
       "<style scoped>\n",
       "    .dataframe tbody tr th:only-of-type {\n",
       "        vertical-align: middle;\n",
       "    }\n",
       "\n",
       "    .dataframe tbody tr th {\n",
       "        vertical-align: top;\n",
       "    }\n",
       "\n",
       "    .dataframe thead th {\n",
       "        text-align: right;\n",
       "    }\n",
       "</style>\n",
       "<table border=\"1\" class=\"dataframe\">\n",
       "  <thead>\n",
       "    <tr style=\"text-align: right;\">\n",
       "      <th></th>\n",
       "      <th>form</th>\n",
       "      <th>corrected_form</th>\n",
       "      <th>t5_form</th>\n",
       "      <th>lstm_bada</th>\n",
       "      <th>acc</th>\n",
       "    </tr>\n",
       "  </thead>\n",
       "  <tbody>\n",
       "    <tr>\n",
       "      <th>0</th>\n",
       "      <td>기존점을 쓰고 그것에 관한 관점인가 감상인가 아닌가 이런 걸 구별하는 연습도 해보셔...</td>\n",
       "      <td>기준점을 주고 그것에 관한 관점인가 감상인가 아닌가 이런 걸 구별하는 연습도 해 보...</td>\n",
       "      <td>기존점을 쓰고 그것에 관한 관점인가 감상인가 아닌가 이런 걸 구별하는 연습도 해보셔...</td>\n",
       "      <td>기존점을 쓰고 그것에 관 인 점 관 인 인 인                     ...</td>\n",
       "      <td>27.50</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>1</th>\n",
       "      <td>킬부도 시원을 하면 안 돼요</td>\n",
       "      <td>필부도 시건을 하면 안 돼요.</td>\n",
       "      <td>킬부도 시원을 하면 안 돼요?</td>\n",
       "      <td>깔부도 시원을 하면 안 돼.</td>\n",
       "      <td>75.00</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>2</th>\n",
       "      <td>현강의 문이 발짝 열셨습니다 서 일 이 강 얻어셨나요 복수를 잘 하셨나요 저 일강을...</td>\n",
       "      <td>삼강의 문이 활짝 열렸습니다. 자 일 이강 어떠셨나요 복습 잘 하셨나요 자 일강을 ...</td>\n",
       "      <td>현강의 문이 발짝 열셨습니다. 서 일 이 강 얻어셨나요? 복수를 잘 하셨나요? 저 ...</td>\n",
       "      <td>현강의 문이 발컬 열셨습 다 서 일                           ...</td>\n",
       "      <td>7.46</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>3</th>\n",
       "      <td>화자가 누군가에게 말을 걷낸다는건 내 말을 들어주는 사람이 있는 거죠 그래서 이걸 ...</td>\n",
       "      <td>화자가 누군가에게 말을 건넨다는 건 내 말을 들어주는 사람이 있는 거죠 그래서 이걸...</td>\n",
       "      <td>화자가 누군가에게 말을 걷낸다는 건 내 말을 들어주는 사람이 있는 거죠. 그래서 이...</td>\n",
       "      <td>화자가 누군가 게 말을    는 는                           ...</td>\n",
       "      <td>28.57</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>4</th>\n",
       "      <td>길게 써있는데요 아까 제목에서 봤던 거가 굉장히 비슷한 내용기죠 새</td>\n",
       "      <td>길게 쓰여있는데요. 아까 제목에서 봤던 것과 굉장히 비슷한 내용이죠 세.</td>\n",
       "      <td>길게 써있는데요. 아까 제목에서 봤던 거가 굉장히 비슷한 내용이죠. 세</td>\n",
       "      <td>길게 써 는 요 까                           새새</td>\n",
       "      <td>16.13</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>5</th>\n",
       "      <td>계속 여러 번 나오고 있어요 이거 쉬 안 좋은 문제점이거든요 그것 을 보안하기위에 ...</td>\n",
       "      <td>계속 여러 번 나오고 있어요. 이것이 안 좋은 문제점이거든요. 그것을 보완하기 위해...</td>\n",
       "      <td>계속 여러 번 나오고 있어요. 이거 쉬 안 좋은 문제점이거든요. 그것 을 보완하기 ...</td>\n",
       "      <td>계속 여러 나오고 있어어                      거          ...</td>\n",
       "      <td>20.00</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>6</th>\n",
       "      <td>그다음 기자 기자의 질문 어린 석은 사람을 높이 평가이 이것을 긍정적으로 바라봤죠 ...</td>\n",
       "      <td>그다음 기자 기자의 질문 어리석은 사람을 높이 평가한 이유 이것을 긍정적으로 바라봤...</td>\n",
       "      <td>그다음 기자? 기자의 질문. 어린 석은 사람을 높이 평가. 이것을 긍정적으로 바라봤...</td>\n",
       "      <td>그다음 기자 기자          사                          ...</td>\n",
       "      <td>14.29</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>7</th>\n",
       "      <td>자 반달리즘에 경향을 어떻게 이해한다고 했죠 그래서 뭔가 기존의 것에서 벗어나려고 ...</td>\n",
       "      <td>자 반달리즘의 경향을 어떻게 이해한다고 했죠 그렇죠 뭔가 기존의 것에서 벗어나려고 ...</td>\n",
       "      <td>자 반달리즘에 경향을 어떻게 이해한다고 했죠? 그래서 뭔가 기존의 것에서 벗어나려고...</td>\n",
       "      <td>자 반 리  경  향을 어  별                             ...</td>\n",
       "      <td>21.21</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>8</th>\n",
       "      <td>칼처럼 비정한 것이기 때문입니다 지문에 있었던 내용 그대로 가지고 왔거든요 적절하구요</td>\n",
       "      <td>칼처럼 비정한 것이기 때문입니다. 지문에 있었던 내용 그대로 가지고 왔거든요 적절하고요.</td>\n",
       "      <td>칼처럼 비정한 것이기 때문입니다. 지문에 있었던 내용 그대로 가지고 왔거든요. 적절...</td>\n",
       "      <td>칼처럼 비정한 것이기 때문이 다                             요</td>\n",
       "      <td>33.33</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>9</th>\n",
       "      <td>교과서에 나와 있는 지문이 이것보다 훨씬 길고 이 앞에 작품이 하나 더 있어요 그죠</td>\n",
       "      <td>교과서에 나와 있는 지문이 이것보다 훨씬 길고 이 앞에 작품이 하나 더 있어요 그죠.</td>\n",
       "      <td>교과서에 나와 있는 지문이 이것보다 훨씬 길고 이 앞에 작품이 하나 더 있어요. 그죠?</td>\n",
       "      <td>교과서에 나와 있는 지문이 이것 다 앞 길 다                    .어</td>\n",
       "      <td>47.06</td>\n",
       "    </tr>\n",
       "  </tbody>\n",
       "</table>\n",
       "</div>"
      ],
      "text/plain": [
       "                                                form  \\\n",
       "0  기존점을 쓰고 그것에 관한 관점인가 감상인가 아닌가 이런 걸 구별하는 연습도 해보셔...   \n",
       "1                                    킬부도 시원을 하면 안 돼요   \n",
       "2  현강의 문이 발짝 열셨습니다 서 일 이 강 얻어셨나요 복수를 잘 하셨나요 저 일강을...   \n",
       "3  화자가 누군가에게 말을 걷낸다는건 내 말을 들어주는 사람이 있는 거죠 그래서 이걸 ...   \n",
       "4              길게 써있는데요 아까 제목에서 봤던 거가 굉장히 비슷한 내용기죠 새   \n",
       "5  계속 여러 번 나오고 있어요 이거 쉬 안 좋은 문제점이거든요 그것 을 보안하기위에 ...   \n",
       "6  그다음 기자 기자의 질문 어린 석은 사람을 높이 평가이 이것을 긍정적으로 바라봤죠 ...   \n",
       "7  자 반달리즘에 경향을 어떻게 이해한다고 했죠 그래서 뭔가 기존의 것에서 벗어나려고 ...   \n",
       "8    칼처럼 비정한 것이기 때문입니다 지문에 있었던 내용 그대로 가지고 왔거든요 적절하구요   \n",
       "9     교과서에 나와 있는 지문이 이것보다 훨씬 길고 이 앞에 작품이 하나 더 있어요 그죠   \n",
       "\n",
       "                                      corrected_form  \\\n",
       "0  기준점을 주고 그것에 관한 관점인가 감상인가 아닌가 이런 걸 구별하는 연습도 해 보...   \n",
       "1                                   필부도 시건을 하면 안 돼요.   \n",
       "2  삼강의 문이 활짝 열렸습니다. 자 일 이강 어떠셨나요 복습 잘 하셨나요 자 일강을 ...   \n",
       "3  화자가 누군가에게 말을 건넨다는 건 내 말을 들어주는 사람이 있는 거죠 그래서 이걸...   \n",
       "4           길게 쓰여있는데요. 아까 제목에서 봤던 것과 굉장히 비슷한 내용이죠 세.   \n",
       "5  계속 여러 번 나오고 있어요. 이것이 안 좋은 문제점이거든요. 그것을 보완하기 위해...   \n",
       "6  그다음 기자 기자의 질문 어리석은 사람을 높이 평가한 이유 이것을 긍정적으로 바라봤...   \n",
       "7  자 반달리즘의 경향을 어떻게 이해한다고 했죠 그렇죠 뭔가 기존의 것에서 벗어나려고 ...   \n",
       "8  칼처럼 비정한 것이기 때문입니다. 지문에 있었던 내용 그대로 가지고 왔거든요 적절하고요.   \n",
       "9    교과서에 나와 있는 지문이 이것보다 훨씬 길고 이 앞에 작품이 하나 더 있어요 그죠.   \n",
       "\n",
       "                                             t5_form  \\\n",
       "0  기존점을 쓰고 그것에 관한 관점인가 감상인가 아닌가 이런 걸 구별하는 연습도 해보셔...   \n",
       "1                                   킬부도 시원을 하면 안 돼요?   \n",
       "2  현강의 문이 발짝 열셨습니다. 서 일 이 강 얻어셨나요? 복수를 잘 하셨나요? 저 ...   \n",
       "3  화자가 누군가에게 말을 걷낸다는 건 내 말을 들어주는 사람이 있는 거죠. 그래서 이...   \n",
       "4            길게 써있는데요. 아까 제목에서 봤던 거가 굉장히 비슷한 내용이죠. 세   \n",
       "5  계속 여러 번 나오고 있어요. 이거 쉬 안 좋은 문제점이거든요. 그것 을 보완하기 ...   \n",
       "6  그다음 기자? 기자의 질문. 어린 석은 사람을 높이 평가. 이것을 긍정적으로 바라봤...   \n",
       "7  자 반달리즘에 경향을 어떻게 이해한다고 했죠? 그래서 뭔가 기존의 것에서 벗어나려고...   \n",
       "8  칼처럼 비정한 것이기 때문입니다. 지문에 있었던 내용 그대로 가지고 왔거든요. 적절...   \n",
       "9   교과서에 나와 있는 지문이 이것보다 훨씬 길고 이 앞에 작품이 하나 더 있어요. 그죠?   \n",
       "\n",
       "                                           lstm_bada    acc  \n",
       "0  기존점을 쓰고 그것에 관 인 점 관 인 인 인                     ...  27.50  \n",
       "1                                    깔부도 시원을 하면 안 돼.  75.00  \n",
       "2  현강의 문이 발컬 열셨습 다 서 일                           ...   7.46  \n",
       "3  화자가 누군가 게 말을    는 는                           ...  28.57  \n",
       "4            길게 써 는 요 까                           새새  16.13  \n",
       "5  계속 여러 나오고 있어어                      거          ...  20.00  \n",
       "6  그다음 기자 기자          사                          ...  14.29  \n",
       "7  자 반 리  경  향을 어  별                             ...  21.21  \n",
       "8    칼처럼 비정한 것이기 때문이 다                             요  33.33  \n",
       "9    교과서에 나와 있는 지문이 이것 다 앞 길 다                    .어  47.06  "
      ]
     },
     "execution_count": 39,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "text"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 42,
   "id": "dd9ff877-6adc-402e-980d-7f6240dfcc3f",
   "metadata": {
    "scrolled": true,
    "tags": []
   },
   "outputs": [
    {
     "data": {
      "text/html": [
       "<div>\n",
       "<style scoped>\n",
       "    .dataframe tbody tr th:only-of-type {\n",
       "        vertical-align: middle;\n",
       "    }\n",
       "\n",
       "    .dataframe tbody tr th {\n",
       "        vertical-align: top;\n",
       "    }\n",
       "\n",
       "    .dataframe thead th {\n",
       "        text-align: right;\n",
       "    }\n",
       "</style>\n",
       "<table border=\"1\" class=\"dataframe\">\n",
       "  <thead>\n",
       "    <tr style=\"text-align: right;\">\n",
       "      <th></th>\n",
       "      <th>form</th>\n",
       "      <th>corrected_form</th>\n",
       "      <th>t5_form</th>\n",
       "      <th>t5_5gram_form</th>\n",
       "      <th>acc</th>\n",
       "    </tr>\n",
       "  </thead>\n",
       "  <tbody>\n",
       "    <tr>\n",
       "      <th>0</th>\n",
       "      <td>기존점을 쓰고 그것에 관한 관점인가 감상인가 아닌가 이런 걸 구별하는 연습도 해보셔...</td>\n",
       "      <td>기준점을 주고 그것에 관한 관점인가 감상인가 아닌가 이런 걸 구별하는 연습도 해 보...</td>\n",
       "      <td>기존점을 쓰고 그것에 관한 관점인가 감상인가 아닌가 이런 걸 구별하는 연습도 해보셔...</td>\n",
       "      <td>기존점을 쓰고 그것에 관한 관점인가 감상인가 아닌가? 이런 걸 구별하는 연습도 해보...</td>\n",
       "      <td>92.50</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>1</th>\n",
       "      <td>킬부도 시원을 하면 안 돼요</td>\n",
       "      <td>필부도 시건을 하면 안 돼요.</td>\n",
       "      <td>킬부도 시원을 하면 안 돼요?</td>\n",
       "      <td>킬부도 시원을 하면 안 돼요.</td>\n",
       "      <td>83.33</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>2</th>\n",
       "      <td>현강의 문이 발짝 열셨습니다 서 일 이 강 얻어셨나요 복수를 잘 하셨나요 저 일강을...</td>\n",
       "      <td>삼강의 문이 활짝 열렸습니다. 자 일 이강 어떠셨나요 복습 잘 하셨나요 자 일강을 ...</td>\n",
       "      <td>현강의 문이 발짝 열셨습니다. 서 일 이 강 얻어셨나요? 복수를 잘 하셨나요? 저 ...</td>\n",
       "      <td>현강의 문이 발짝 열십니다. 서 일 이 강 얻어셨나요 복수를 잘 하셨나요? 저 일강...</td>\n",
       "      <td>83.58</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>3</th>\n",
       "      <td>화자가 누군가에게 말을 걷낸다는건 내 말을 들어주는 사람이 있는 거죠 그래서 이걸 ...</td>\n",
       "      <td>화자가 누군가에게 말을 건넨다는 건 내 말을 들어주는 사람이 있는 거죠 그래서 이걸...</td>\n",
       "      <td>화자가 누군가에게 말을 걷낸다는 건 내 말을 들어주는 사람이 있는 거죠. 그래서 이...</td>\n",
       "      <td>화자가 누군가에게 말을 걷낸다는 건 내 말을 들어주는 사람이 있는 거죠. 그래서 이...</td>\n",
       "      <td>85.71</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>4</th>\n",
       "      <td>길게 써있는데요 아까 제목에서 봤던 거가 굉장히 비슷한 내용기죠 새</td>\n",
       "      <td>길게 쓰여있는데요. 아까 제목에서 봤던 것과 굉장히 비슷한 내용이죠 세.</td>\n",
       "      <td>길게 써있는데요. 아까 제목에서 봤던 거가 굉장히 비슷한 내용이죠. 세</td>\n",
       "      <td>길게 써있는데요. 아까 제목에서 봤던 거가 굉장히 비슷한 내용기죠. 새</td>\n",
       "      <td>77.42</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>5</th>\n",
       "      <td>계속 여러 번 나오고 있어요 이거 쉬 안 좋은 문제점이거든요 그것 을 보안하기위에 ...</td>\n",
       "      <td>계속 여러 번 나오고 있어요. 이것이 안 좋은 문제점이거든요. 그것을 보완하기 위해...</td>\n",
       "      <td>계속 여러 번 나오고 있어요. 이거 쉬 안 좋은 문제점이거든요. 그것 을 보완하기 ...</td>\n",
       "      <td>계속 여러 번 나오고 있어요. 이거 쉬 안 좋은 문제점이거든요 그것 을 보안하기위에...</td>\n",
       "      <td>89.09</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>6</th>\n",
       "      <td>그다음 기자 기자의 질문 어린 석은 사람을 높이 평가이 이것을 긍정적으로 바라봤죠 ...</td>\n",
       "      <td>그다음 기자 기자의 질문 어리석은 사람을 높이 평가한 이유 이것을 긍정적으로 바라봤...</td>\n",
       "      <td>그다음 기자? 기자의 질문. 어린 석은 사람을 높이 평가. 이것을 긍정적으로 바라봤...</td>\n",
       "      <td>그다음 기사 기사 질문 어린 석은 사람을 높이 평가이 이것을 긍정적으로 바라봤죠. ...</td>\n",
       "      <td>77.78</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>7</th>\n",
       "      <td>자 반달리즘에 경향을 어떻게 이해한다고 했죠 그래서 뭔가 기존의 것에서 벗어나려고 ...</td>\n",
       "      <td>자 반달리즘의 경향을 어떻게 이해한다고 했죠 그렇죠 뭔가 기존의 것에서 벗어나려고 ...</td>\n",
       "      <td>자 반달리즘에 경향을 어떻게 이해한다고 했죠? 그래서 뭔가 기존의 것에서 벗어나려고...</td>\n",
       "      <td>자 반달리즘에 경향을 어떻게 이해한다고 했죠. 그래서 뭔가 기존의 것에서 벗어나려고...</td>\n",
       "      <td>81.82</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>8</th>\n",
       "      <td>칼처럼 비정한 것이기 때문입니다 지문에 있었던 내용 그대로 가지고 왔거든요 적절하구요</td>\n",
       "      <td>칼처럼 비정한 것이기 때문입니다. 지문에 있었던 내용 그대로 가지고 왔거든요 적절하고요.</td>\n",
       "      <td>칼처럼 비정한 것이기 때문입니다. 지문에 있었던 내용 그대로 가지고 왔거든요. 적절...</td>\n",
       "      <td>칼처럼 비정한 것이기 아닙니다. 지문에 있었던 내용 그대로 가지고 왔거든요 적절하구요.</td>\n",
       "      <td>89.74</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>9</th>\n",
       "      <td>교과서에 나와 있는 지문이 이것보다 훨씬 길고 이 앞에 작품이 하나 더 있어요 그죠</td>\n",
       "      <td>교과서에 나와 있는 지문이 이것보다 훨씬 길고 이 앞에 작품이 하나 더 있어요 그죠.</td>\n",
       "      <td>교과서에 나와 있는 지문이 이것보다 훨씬 길고 이 앞에 작품이 하나 더 있어요. 그죠?</td>\n",
       "      <td>교과서에 나와 있는 지문이 이것보다 훨씬 길고 이 앞에 작품이 하나 더 있어요. 그죠</td>\n",
       "      <td>94.12</td>\n",
       "    </tr>\n",
       "  </tbody>\n",
       "</table>\n",
       "</div>"
      ],
      "text/plain": [
       "                                                form  \\\n",
       "0  기존점을 쓰고 그것에 관한 관점인가 감상인가 아닌가 이런 걸 구별하는 연습도 해보셔...   \n",
       "1                                    킬부도 시원을 하면 안 돼요   \n",
       "2  현강의 문이 발짝 열셨습니다 서 일 이 강 얻어셨나요 복수를 잘 하셨나요 저 일강을...   \n",
       "3  화자가 누군가에게 말을 걷낸다는건 내 말을 들어주는 사람이 있는 거죠 그래서 이걸 ...   \n",
       "4              길게 써있는데요 아까 제목에서 봤던 거가 굉장히 비슷한 내용기죠 새   \n",
       "5  계속 여러 번 나오고 있어요 이거 쉬 안 좋은 문제점이거든요 그것 을 보안하기위에 ...   \n",
       "6  그다음 기자 기자의 질문 어린 석은 사람을 높이 평가이 이것을 긍정적으로 바라봤죠 ...   \n",
       "7  자 반달리즘에 경향을 어떻게 이해한다고 했죠 그래서 뭔가 기존의 것에서 벗어나려고 ...   \n",
       "8    칼처럼 비정한 것이기 때문입니다 지문에 있었던 내용 그대로 가지고 왔거든요 적절하구요   \n",
       "9     교과서에 나와 있는 지문이 이것보다 훨씬 길고 이 앞에 작품이 하나 더 있어요 그죠   \n",
       "\n",
       "                                      corrected_form  \\\n",
       "0  기준점을 주고 그것에 관한 관점인가 감상인가 아닌가 이런 걸 구별하는 연습도 해 보...   \n",
       "1                                   필부도 시건을 하면 안 돼요.   \n",
       "2  삼강의 문이 활짝 열렸습니다. 자 일 이강 어떠셨나요 복습 잘 하셨나요 자 일강을 ...   \n",
       "3  화자가 누군가에게 말을 건넨다는 건 내 말을 들어주는 사람이 있는 거죠 그래서 이걸...   \n",
       "4           길게 쓰여있는데요. 아까 제목에서 봤던 것과 굉장히 비슷한 내용이죠 세.   \n",
       "5  계속 여러 번 나오고 있어요. 이것이 안 좋은 문제점이거든요. 그것을 보완하기 위해...   \n",
       "6  그다음 기자 기자의 질문 어리석은 사람을 높이 평가한 이유 이것을 긍정적으로 바라봤...   \n",
       "7  자 반달리즘의 경향을 어떻게 이해한다고 했죠 그렇죠 뭔가 기존의 것에서 벗어나려고 ...   \n",
       "8  칼처럼 비정한 것이기 때문입니다. 지문에 있었던 내용 그대로 가지고 왔거든요 적절하고요.   \n",
       "9    교과서에 나와 있는 지문이 이것보다 훨씬 길고 이 앞에 작품이 하나 더 있어요 그죠.   \n",
       "\n",
       "                                             t5_form  \\\n",
       "0  기존점을 쓰고 그것에 관한 관점인가 감상인가 아닌가 이런 걸 구별하는 연습도 해보셔...   \n",
       "1                                   킬부도 시원을 하면 안 돼요?   \n",
       "2  현강의 문이 발짝 열셨습니다. 서 일 이 강 얻어셨나요? 복수를 잘 하셨나요? 저 ...   \n",
       "3  화자가 누군가에게 말을 걷낸다는 건 내 말을 들어주는 사람이 있는 거죠. 그래서 이...   \n",
       "4            길게 써있는데요. 아까 제목에서 봤던 거가 굉장히 비슷한 내용이죠. 세   \n",
       "5  계속 여러 번 나오고 있어요. 이거 쉬 안 좋은 문제점이거든요. 그것 을 보완하기 ...   \n",
       "6  그다음 기자? 기자의 질문. 어린 석은 사람을 높이 평가. 이것을 긍정적으로 바라봤...   \n",
       "7  자 반달리즘에 경향을 어떻게 이해한다고 했죠? 그래서 뭔가 기존의 것에서 벗어나려고...   \n",
       "8  칼처럼 비정한 것이기 때문입니다. 지문에 있었던 내용 그대로 가지고 왔거든요. 적절...   \n",
       "9   교과서에 나와 있는 지문이 이것보다 훨씬 길고 이 앞에 작품이 하나 더 있어요. 그죠?   \n",
       "\n",
       "                                       t5_5gram_form    acc  \n",
       "0  기존점을 쓰고 그것에 관한 관점인가 감상인가 아닌가? 이런 걸 구별하는 연습도 해보...  92.50  \n",
       "1                                   킬부도 시원을 하면 안 돼요.  83.33  \n",
       "2  현강의 문이 발짝 열십니다. 서 일 이 강 얻어셨나요 복수를 잘 하셨나요? 저 일강...  83.58  \n",
       "3  화자가 누군가에게 말을 걷낸다는 건 내 말을 들어주는 사람이 있는 거죠. 그래서 이...  85.71  \n",
       "4            길게 써있는데요. 아까 제목에서 봤던 거가 굉장히 비슷한 내용기죠. 새  77.42  \n",
       "5  계속 여러 번 나오고 있어요. 이거 쉬 안 좋은 문제점이거든요 그것 을 보안하기위에...  89.09  \n",
       "6  그다음 기사 기사 질문 어린 석은 사람을 높이 평가이 이것을 긍정적으로 바라봤죠. ...  77.78  \n",
       "7  자 반달리즘에 경향을 어떻게 이해한다고 했죠. 그래서 뭔가 기존의 것에서 벗어나려고...  81.82  \n",
       "8   칼처럼 비정한 것이기 아닙니다. 지문에 있었던 내용 그대로 가지고 왔거든요 적절하구요.  89.74  \n",
       "9    교과서에 나와 있는 지문이 이것보다 훨씬 길고 이 앞에 작품이 하나 더 있어요. 그죠  94.12  "
      ]
     },
     "execution_count": 42,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "text2=pd.read_csv('./save/5gram_result.tsv',encoding='utf-8',sep='\\t', index_col='Unnamed: 0')\n",
    "text2"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 52,
   "id": "87b82851-07b3-4690-89a5-9beee1dfec43",
   "metadata": {},
   "outputs": [],
   "source": [
    "# 표 합쳐서 장표 구성하면됨. 남은 시간엔 통일부 ㄱ\n",
    "\n",
    "# text2.columns\n",
    "resu=pd.merge(text2[['form', 'corrected_form','t5_5gram_form', 'acc']],text[['form', 'corrected_form', 'lstm_bada','acc']],on='form')\n",
    "resu.to_csv('./장표완료.csv',sep=',',encoding='cp949')"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "c34ebbb1-67cc-4c9b-aa58-1a1df887e158",
   "metadata": {},
   "outputs": [],
   "source": []
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "3a935c77-aed6-454c-a51e-d6cdce6c18db",
   "metadata": {},
   "outputs": [],
   "source": []
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "350cb9c2-d32c-47aa-8ce7-85d50d3599cb",
   "metadata": {},
   "outputs": [],
   "source": []
  }
 ],
 "metadata": {
  "kernelspec": {
   "display_name": "Python 3 (ipykernel)",
   "language": "python",
   "name": "python3"
  },
  "language_info": {
   "codemirror_mode": {
    "name": "ipython",
    "version": 3
   },
   "file_extension": ".py",
   "mimetype": "text/x-python",
   "name": "python",
   "nbconvert_exporter": "python",
   "pygments_lexer": "ipython3",
   "version": "3.8.3"
  }
 },
 "nbformat": 4,
 "nbformat_minor": 5
}
